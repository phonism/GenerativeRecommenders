{"config":{"lang":["zh","en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"GenerativeRecommenders","text":"<p>\u57fa\u4e8e PyTorch \u7684\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u6846\u67b6\u3002</p>"},{"location":"#_1","title":"\u7279\u6027","text":"<ul> <li>\ud83d\ude80 \u73b0\u4ee3\u67b6\u6784: \u57fa\u4e8e Transformer \u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u751f\u6210\u5f0f\u63a8\u8350</li> <li>\ud83d\udcca \u4e30\u5bcc\u6570\u636e\u96c6: \u652f\u6301 P5 Amazon \u7b49\u4e3b\u6d41\u63a8\u8350\u6570\u636e\u96c6</li> <li>\ud83d\udd27 \u6613\u4e8e\u6269\u5c55: \u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u548c\u6a21\u578b</li> <li>\ud83c\udfaf \u7aef\u5230\u7aef: \u4ece\u6570\u636e\u9884\u5904\u7406\u5230\u6a21\u578b\u8bad\u7ec3\u7684\u5b8c\u6574\u6d41\u7a0b</li> <li>\ud83d\udcc8 \u9ad8\u6027\u80fd: \u4f18\u5316\u7684\u8bad\u7ec3\u6d41\u7a0b\u548c\u63a8\u7406\u6027\u80fd</li> </ul>"},{"location":"#_2","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"#_3","title":"\u5b89\u88c5","text":"<pre><code>git clone https://github.com/phonism/GenerativeRecommenders.git\ncd GenerativeRecommenders\npip install -e .\n</code></pre>"},{"location":"#_4","title":"\u7b80\u5355\u793a\u4f8b","text":"<pre><code>from generative_recommenders.data import P5AmazonItemDataset\nfrom generative_recommenders.models import RqVae\n\n# \u52a0\u8f7d\u6570\u636e\u96c6\ndataset = P5AmazonItemDataset(\n    root=\"data/amazon\",\n    split=\"beauty\"\n)\n\n# \u8bad\u7ec3 RQVAE \u6a21\u578b\nmodel = RqVae(\n    vocab_size=len(dataset),\n    embedding_dim=256\n)\n\n# \u5f00\u59cb\u8bad\u7ec3...\n</code></pre>"},{"location":"#_5","title":"\u67b6\u6784\u6982\u89c8","text":"<p>GenerativeRecommenders \u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u578b\uff1a</p> <ol> <li>RQVAE (Residual Quantized VAE): \u5b66\u4e60\u7269\u54c1\u7684\u5411\u91cf\u91cf\u5316\u8868\u793a</li> <li>TIGER (Transformer-based Generative Retrieval): \u57fa\u4e8e\u7528\u6237\u5386\u53f2\u5e8f\u5217\u751f\u6210\u63a8\u8350</li> </ol>"},{"location":"#_6","title":"\u6838\u5fc3\u7ec4\u4ef6","text":"<ul> <li>\u6570\u636e\u5904\u7406: \u652f\u6301\u591a\u79cd\u63a8\u8350\u6570\u636e\u96c6\u683c\u5f0f</li> <li>\u6a21\u578b\u67b6\u6784: RQVAE + TIGER \u53cc\u9636\u6bb5\u8bad\u7ec3</li> <li>\u8bad\u7ec3\u6846\u67b6: \u57fa\u4e8e PyTorch Lightning \u7684\u73b0\u4ee3\u5316\u8bad\u7ec3</li> <li>\u914d\u7f6e\u7ba1\u7406: \u7075\u6d3b\u7684 Gin \u914d\u7f6e\u7cfb\u7edf</li> </ul>"},{"location":"#_7","title":"\u8d21\u732e","text":"<p>\u6211\u4eec\u6b22\u8fce\u793e\u533a\u8d21\u732e\uff01\u8bf7\u67e5\u770b\u8d21\u732e\u6307\u5357\u4e86\u89e3\u8be6\u60c5\u3002</p>"},{"location":"#_8","title":"\u8bb8\u53ef\u8bc1","text":"<p>\u672c\u9879\u76ee\u91c7\u7528 MIT \u8bb8\u53ef\u8bc1 - \u67e5\u770b LICENSE \u6587\u4ef6\u4e86\u89e3\u8be6\u60c5\u3002</p>"},{"location":"en/","title":"Index","text":"<ul> <li>{'name': '\u4e2d\u6587', 'build': True}</li> </ul>"},{"location":"en/#generativerecommenders","title":"GenerativeRecommenders","text":"<p>A PyTorch-based generative recommender systems research framework.</p>"},{"location":"en/#overview","title":"Overview","text":"<p>GenerativeRecommenders is a modular framework for recommender systems research, implementing state-of-the-art generative recommendation algorithms. It provides clean code architecture, flexible configuration systems, and extensible data processing pipelines.</p>"},{"location":"en/#key-features","title":"Key Features","text":"<ul> <li>\u2728 Modular Design: Clean component separation for easy understanding and extension</li> <li>\ud83d\udd27 Configuration-Driven: Flexible configuration system based on Gin-Config</li> <li>\ud83d\udcca Multiple Models: Latest generative recommendation models like RQVAE and TIGER</li> <li>\ud83c\udfaf Dataset Support: Popular recommendation datasets like P5 Amazon</li> <li>\ud83d\ude80 Distributed Training: Multi-GPU training support with Accelerate</li> <li>\ud83d\udcc8 Experiment Tracking: Weights &amp; Biases integration for experiment management</li> <li>\ud83d\udd0d Cache Optimization: Smart data preprocessing caching mechanisms</li> </ul>"},{"location":"en/#supported-models","title":"Supported Models","text":""},{"location":"en/#rqvae-residual-quantized-variational-autoencoder","title":"RQVAE (Residual Quantized Variational Autoencoder)","text":"<ul> <li>Vector quantized variational autoencoder for recommendations</li> <li>Multiple quantization strategies: Gumbel-Softmax, STE, Rotation Trick, Sinkhorn</li> <li>Used for learning semantic item representations</li> </ul>"},{"location":"en/#tiger-recommender-systems-with-generative-retrieval","title":"TIGER (Recommender Systems with Generative Retrieval)","text":"<ul> <li>Transformer-based generative retrieval model</li> <li>Sequential modeling using semantic IDs</li> <li>Trie-constrained generation process</li> </ul>"},{"location":"en/#quick-start","title":"Quick Start","text":""},{"location":"en/#installation","title":"Installation","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"en/#train-rqvae","title":"Train RQVAE","text":"<pre><code>python generative_recommenders/trainers/rqvae_trainer.py config/rqvae/p5_amazon.gin\n</code></pre>"},{"location":"en/#train-tiger","title":"Train TIGER","text":"<pre><code>python generative_recommenders/trainers/tiger_trainer.py config/tiger/p5_amazon.gin\n</code></pre>"},{"location":"en/#project-structure","title":"Project Structure","text":"<pre><code>GenerativeRecommenders/\n\u251c\u2500\u2500 generative_recommenders/          # Core code\n\u2502   \u251c\u2500\u2500 data/                        # Data processing modules\n\u2502   \u2502   \u251c\u2500\u2500 configs.py               # Configuration classes\n\u2502   \u2502   \u251c\u2500\u2500 base_dataset.py          # Abstract dataset classes\n\u2502   \u2502   \u251c\u2500\u2500 p5_amazon.py             # P5 Amazon dataset\n\u2502   \u2502   \u251c\u2500\u2500 processors/              # Data processors\n\u2502   \u2502   \u2514\u2500\u2500 dataset_factory.py       # Dataset factory\n\u2502   \u251c\u2500\u2500 models/                      # Model implementations\n\u2502   \u2502   \u251c\u2500\u2500 rqvae.py                 # RQVAE model\n\u2502   \u2502   \u2514\u2500\u2500 tiger.py                 # TIGER model\n\u2502   \u251c\u2500\u2500 modules/                     # Base modules\n\u2502   \u2502   \u251c\u2500\u2500 embedding.py             # Embedding layers\n\u2502   \u2502   \u251c\u2500\u2500 encoder.py               # Encoders\n\u2502   \u2502   \u251c\u2500\u2500 loss.py                  # Loss functions\n\u2502   \u2502   \u2514\u2500\u2500 metrics.py               # Evaluation metrics\n\u2502   \u2514\u2500\u2500 trainers/                    # Training scripts\n\u2502       \u251c\u2500\u2500 rqvae_trainer.py         # RQVAE trainer\n\u2502       \u2514\u2500\u2500 tiger_trainer.py         # TIGER trainer\n\u251c\u2500\u2500 config/                          # Configuration files\n\u2502   \u251c\u2500\u2500 rqvae/                       # RQVAE configs\n\u2502   \u2514\u2500\u2500 tiger/                       # TIGER configs\n\u2514\u2500\u2500 docs/                           # Documentation\n</code></pre>"},{"location":"en/#key-improvements","title":"Key Improvements","text":"<p>Compared to the original implementation, our refactored version provides:</p> <ol> <li>Cleaner Code Structure: Modular design with clear responsibilities</li> <li>Configuration Management: Support for flexible parameter configuration and experiment management</li> <li>Enhanced Generalizability: Easy to extend to new datasets and models</li> <li>Performance Optimization: Caching mechanisms and improved memory efficiency</li> <li>Better Documentation: Complete API documentation and usage examples</li> </ol>"},{"location":"en/#benchmark-results","title":"Benchmark Results","text":"Dataset Model Metric Result P5 Amazon-Beauty TIGER Recall@10 0.42"},{"location":"en/#contributing","title":"Contributing","text":"<p>We welcome Issues and Pull Requests! Please refer to our Contributing Guide.</p>"},{"location":"en/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"en/#citation","title":"Citation","text":"<p>If you use this framework in your research, please cite the relevant papers:</p> <pre><code>@inproceedings{rqvae2023,\n  title={RQ-VAE Recommender},\n  author={Botta, Edoardo},\n  year={2023}\n}\n\n@article{tiger2023,\n  title={TIGER: Recommender Systems with Generative Retrieval},\n  year={2023}\n}\n</code></pre>"},{"location":"en/contributing/","title":"Contributing Guide","text":"<p>We welcome contributions to GenerativeRecommenders! This guide will help you get started.</p>"},{"location":"en/contributing/#development-setup","title":"Development Setup","text":""},{"location":"en/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>PyTorch &gt;= 1.11.0</li> <li>Git</li> </ul>"},{"location":"en/contributing/#installation","title":"Installation","text":"<ol> <li>Fork the repository on GitHub</li> <li> <p>Clone your fork:    <pre><code>git clone https://github.com/YOUR_USERNAME/GenerativeRecommenders.git\ncd GenerativeRecommenders\n</code></pre></p> </li> <li> <p>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install in development mode:    <pre><code>pip install -e .[dev]\n</code></pre></p> </li> </ol>"},{"location":"en/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"en/contributing/#code-style","title":"Code Style","text":"<p>We follow PEP 8 style guidelines. Please run the following before submitting:</p> <pre><code># Format code\nblack generative_recommenders/ tests/\nisort generative_recommenders/ tests/\n\n# Check style\nflake8 generative_recommenders/ tests/\nmypy generative_recommenders/\n</code></pre>"},{"location":"en/contributing/#testing","title":"Testing","text":"<p>Run tests before submitting your changes:</p> <pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=generative_recommenders\n\n# Run specific test\npytest tests/test_datasets.py::test_p5_amazon_dataset\n</code></pre>"},{"location":"en/contributing/#documentation","title":"Documentation","text":"<p>Update documentation when adding new features:</p> <pre><code># Build documentation locally\ncd docs\nmkdocs serve\n</code></pre>"},{"location":"en/contributing/#contributing-guidelines","title":"Contributing Guidelines","text":""},{"location":"en/contributing/#issues","title":"Issues","text":"<ul> <li>Search existing issues before creating new ones</li> <li>Use clear, descriptive titles</li> <li>Provide steps to reproduce for bugs</li> <li>Include system information (OS, Python version, etc.)</li> </ul>"},{"location":"en/contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li> <p>Create a feature branch from <code>main</code>:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and commit:    <pre><code>git commit -m \"Add: brief description of changes\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a Pull Request on GitHub</p> </li> </ol>"},{"location":"en/contributing/#commit-message-format","title":"Commit Message Format","text":"<p>Use clear, descriptive commit messages:</p> <ul> <li>Add: New features or functionality</li> <li>Fix: Bug fixes</li> <li>Update: Changes to existing functionality</li> <li>Docs: Documentation changes</li> <li>Test: Adding or updating tests</li> <li>Refactor: Code refactoring without functional changes</li> </ul> <p>Examples: <pre><code>Add: TIGER model with transformer architecture\nFix: P5Amazon dataset loading for large categories\nUpdate: configuration system to use dataclasses\nDocs: API reference for dataset factory\nTest: unit tests for text processors\n</code></pre></p>"},{"location":"en/contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"en/contributing/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix issues reported in GitHub Issues</li> <li>Include test cases that reproduce the bug</li> <li>Update documentation if needed</li> </ul>"},{"location":"en/contributing/#new-features","title":"New Features","text":"<p>Before implementing major features: 1. Create an issue to discuss the feature 2. Get feedback from maintainers 3. Follow the existing architecture patterns</p>"},{"location":"en/contributing/#documentation_1","title":"Documentation","text":"<ul> <li>Fix typos and improve clarity</li> <li>Add examples and tutorials</li> <li>Translate documentation (Chinese/English)</li> <li>Improve API documentation</li> </ul>"},{"location":"en/contributing/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Profile code to identify bottlenecks</li> <li>Include benchmarks showing improvements</li> <li>Ensure changes don't break existing functionality</li> </ul>"},{"location":"en/contributing/#code-architecture","title":"Code Architecture","text":""},{"location":"en/contributing/#adding-new-datasets","title":"Adding New Datasets","text":"<p>To add a new dataset, follow these steps:</p> <ol> <li> <p>Create the base dataset class:    <pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\n\nclass MyDataset(BaseRecommenderDataset):\n    def download(self):\n        # Implement download logic\n        pass\n\n    def load_raw_data(self):\n        # Implement data loading\n        pass\n\n    def preprocess_data(self, raw_data):\n        # Implement preprocessing\n        pass\n</code></pre></p> </li> <li> <p>Create wrapper classes:    <pre><code>from generative_recommenders.data.base_dataset import ItemDataset, SequenceDataset\n\nclass MyItemDataset(ItemDataset):\n    def __init__(self, **kwargs):\n        # Initialize with your dataset\n        pass\n\nclass MySequenceDataset(SequenceDataset):\n    def __init__(self, **kwargs):\n        # Initialize with your dataset\n        pass\n</code></pre></p> </li> <li> <p>Add configuration:    <pre><code>from generative_recommenders.data.configs import DatasetConfig\n\n@dataclass\nclass MyDatasetConfig(DatasetConfig):\n    # Add dataset-specific parameters\n    special_param: str = \"default_value\"\n</code></pre></p> </li> <li> <p>Register the dataset:    <pre><code>from generative_recommenders.data.dataset_factory import DatasetFactory\n\nDatasetFactory.register_dataset(\n    name=\"my_dataset\",\n    base_class=MyDataset,\n    item_class=MyItemDataset,\n    sequence_class=MySequenceDataset\n)\n</code></pre></p> </li> <li> <p>Add tests and documentation</p> </li> </ol> <p>For more details, please refer to the API Documentation.</p>"},{"location":"en/contributing/#adding-new-models","title":"Adding New Models","text":"<ol> <li> <p>Inherit from base classes:    <pre><code>import torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        dropout: float = 0.1,\n    ) -&gt; None:\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n\n        # Define layers\n        self.encoder = nn.Linear(input_dim, hidden_dim)\n        self.decoder = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Implement forward pass\n        hidden = self.dropout(torch.relu(self.encoder(x)))\n        output = self.decoder(hidden)\n        return output\n</code></pre></p> </li> <li> <p>Add to Gin configuration system:    <pre><code>import gin\n\n@gin.configurable\nclass MyModel(nn.Module):\n    # Implementation\n</code></pre></p> </li> <li> <p>Create training utilities:    <pre><code>from generative_recommenders.trainers.base_trainer import BaseTrainer\n\nclass MyModelTrainer(BaseTrainer):\n    def __init__(self, model, config):\n        super().__init__(model, config)\n\n    def training_step(self, batch, batch_idx):\n        # Implement training logic\n        pass\n</code></pre></p> </li> <li> <p>Add comprehensive tests</p> </li> <li>Update documentation</li> </ol>"},{"location":"en/contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"en/contributing/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test individual functions and classes</li> <li>Use pytest fixtures for setup</li> <li>Mock external dependencies</li> <li>Aim for &gt;90% code coverage</li> </ul> <pre><code>import pytest\nfrom generative_recommenders.data import P5AmazonDataset\n\ndef test_p5_amazon_dataset_creation():\n    config = P5AmazonConfig(\n        root_dir=\"test_data\",\n        category=\"beauty\"\n    )\n    dataset = P5AmazonDataset(config)\n    assert dataset.category == \"beauty\"\n</code></pre>"},{"location":"en/contributing/#integration-tests","title":"Integration Tests","text":"<ul> <li>Test component interactions</li> <li>Use sample datasets</li> <li>Test end-to-end workflows</li> </ul> <pre><code>def test_full_training_pipeline():\n    # Test complete training workflow\n    pass\n</code></pre>"},{"location":"en/contributing/#performance-tests","title":"Performance Tests","text":"<ul> <li>Benchmark critical operations</li> <li>Test with realistic data sizes</li> <li>Monitor memory usage</li> </ul>"},{"location":"en/contributing/#documentation-standards","title":"Documentation Standards","text":""},{"location":"en/contributing/#docstring-format","title":"Docstring Format","text":"<p>Use Google-style docstrings:</p> <pre><code>def process_data(data: pd.DataFrame, normalize: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Process input data with optional normalization.\n\n    Args:\n        data: Input DataFrame to process\n        normalize: Whether to normalize numerical features\n\n    Returns:\n        Processed DataFrame\n\n    Raises:\n        ValueError: If data is empty\n\n    Example:\n        &gt;&gt;&gt; df = pd.DataFrame({'col1': [1, 2, 3]})\n        &gt;&gt;&gt; result = process_data(df, normalize=True)\n    \"\"\"\n</code></pre>"},{"location":"en/contributing/#api-documentation","title":"API Documentation","text":"<ul> <li>Document all public methods and classes</li> <li>Include usage examples</li> <li>Explain parameters and return values</li> <li>Add type hints</li> </ul>"},{"location":"en/contributing/#tutorials-and-guides","title":"Tutorials and Guides","text":"<ul> <li>Provide step-by-step instructions</li> <li>Include complete working examples</li> <li>Explain the reasoning behind design decisions</li> <li>Keep examples up-to-date with API changes</li> </ul>"},{"location":"en/contributing/#release-process","title":"Release Process","text":""},{"location":"en/contributing/#version-numbering","title":"Version Numbering","text":"<p>We follow Semantic Versioning (SemVer): - MAJOR: Breaking changes - MINOR: New features, backward compatible - PATCH: Bug fixes, backward compatible</p>"},{"location":"en/contributing/#changelog","title":"Changelog","text":"<p>Update CHANGELOG.md with: - New features - Bug fixes - Breaking changes - Deprecations</p>"},{"location":"en/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Join our discussions on GitHub</li> <li>Ask questions in Issues</li> <li>Check existing documentation</li> <li>Review code examples</li> </ul>"},{"location":"en/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help maintain a welcoming community</li> <li>Follow GitHub's Community Guidelines</li> </ul> <p>Thank you for contributing to GenerativeRecommenders!</p>"},{"location":"en/deployment/","title":"Deployment Guide","text":"<p>This guide covers deploying GenerativeRecommenders models in production environments.</p>"},{"location":"en/deployment/#production-deployment","title":"Production Deployment","text":""},{"location":"en/deployment/#model-serving-with-fastapi","title":"Model Serving with FastAPI","text":"<p>Create a REST API server for model inference:</p> <pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport torch\nfrom generative_recommenders.models.tiger import Tiger\nfrom generative_recommenders.models.rqvae import RqVae\n\napp = FastAPI(title=\"GenerativeRecommenders API\", version=\"1.0.0\")\n\nclass RecommendationRequest(BaseModel):\n    user_id: int\n    user_history: List[int]\n    num_recommendations: int = 10\n\nclass RecommendationResponse(BaseModel):\n    user_id: int\n    recommendations: List[int]\n    scores: Optional[List[float]] = None\n\nclass ModelService:\n    def __init__(self, rqvae_path: str, tiger_path: str):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # Load models\n        self.rqvae = RqVae.load_from_checkpoint(rqvae_path)\n        self.rqvae.to(self.device)\n        self.rqvae.eval()\n\n        self.tiger = Tiger.load_from_checkpoint(tiger_path)\n        self.tiger.to(self.device)\n        self.tiger.eval()\n\n    def get_recommendations(self, user_history: List[int], k: int) -&gt; List[int]:\n        \"\"\"Generate recommendations for user\"\"\"\n        with torch.no_grad():\n            # Convert item IDs to semantic IDs\n            semantic_sequence = self.items_to_semantic_ids(user_history)\n\n            # Generate recommendations\n            input_seq = torch.tensor(semantic_sequence).unsqueeze(0).to(self.device)\n            generated = self.tiger.generate(input_seq, max_length=k*3)  # Generate more to account for duplicates\n\n            # Convert back to item IDs and deduplicate\n            recommendations = self.semantic_ids_to_items(generated.squeeze().tolist())\n\n            # Remove items already in user history\n            recommendations = [item for item in recommendations if item not in user_history]\n\n            return recommendations[:k]\n\n# Initialize model service\nmodel_service = ModelService(\n    rqvae_path=\"checkpoints/rqvae.ckpt\",\n    tiger_path=\"checkpoints/tiger.ckpt\"\n)\n\n@app.post(\"/recommend\", response_model=RecommendationResponse)\nasync def recommend(request: RecommendationRequest):\n    \"\"\"Generate recommendations for a user\"\"\"\n    try:\n        recommendations = model_service.get_recommendations(\n            request.user_history,\n            request.num_recommendations\n        )\n\n        return RecommendationResponse(\n            user_id=request.user_id,\n            recommendations=recommendations\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"healthy\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"en/deployment/#docker-deployment","title":"Docker Deployment","text":"<p>Create a Dockerfile:</p> <pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose port\nEXPOSE 8000\n\n# Run the application\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <p>Build and run the container:</p> <pre><code># Build the image\ndocker build -t generative-recommenders:latest .\n\n# Run the container\ndocker run -d -p 8000:8000 \\\n    -v /path/to/checkpoints:/app/checkpoints \\\n    generative-recommenders:latest\n</code></pre>"},{"location":"en/deployment/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>Create Kubernetes manifests:</p> <pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: generative-recommenders\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: generative-recommenders\n  template:\n    metadata:\n      labels:\n        app: generative-recommenders\n    spec:\n      containers:\n      - name: api\n        image: generative-recommenders:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: MODEL_PATH\n          value: \"/models\"\n        volumeMounts:\n        - name: model-storage\n          mountPath: /models\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2000m\"\n      volumes:\n      - name: model-storage\n        persistentVolumeClaim:\n          claimName: model-pvc\n\n---\n# service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: generative-recommenders-service\nspec:\n  selector:\n    app: generative-recommenders\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8000\n  type: LoadBalancer\n</code></pre> <p>Deploy to Kubernetes:</p> <pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre>"},{"location":"en/deployment/#batch-processing","title":"Batch Processing","text":""},{"location":"en/deployment/#apache-spark-integration","title":"Apache Spark Integration","text":"<p>Process large datasets with Spark:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf, explode\nfrom pyspark.sql.types import ArrayType, IntegerType\nimport torch\n\ndef create_spark_session():\n    return SparkSession.builder \\\n        .appName(\"GenerativeRecommenders\") \\\n        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n        .getOrCreate()\n\ndef broadcast_model(spark, model_path):\n    \"\"\"Broadcast model to all workers\"\"\"\n    model = Tiger.load_from_checkpoint(model_path)\n    model.eval()\n    return spark.sparkContext.broadcast(model)\n\ndef batch_recommend_udf(broadcast_model):\n    \"\"\"UDF for batch recommendations\"\"\"\n    @udf(returnType=ArrayType(IntegerType()))\n    def recommend(user_history):\n        model = broadcast_model.value\n        with torch.no_grad():\n            # Convert to tensor\n            input_seq = torch.tensor(user_history).unsqueeze(0)\n\n            # Generate recommendations\n            recommendations = model.generate(input_seq, max_length=20)\n\n            return recommendations.squeeze().tolist()\n\n    return recommend\n\n# Main processing\nspark = create_spark_session()\nmodel_broadcast = broadcast_model(spark, \"checkpoints/tiger.ckpt\")\n\n# Load user data\nuser_data = spark.read.parquet(\"s3://data/user_interactions\")\n\n# Generate recommendations\nrecommend_func = batch_recommend_udf(model_broadcast)\nrecommendations = user_data.withColumn(\n    \"recommendations\", \n    recommend_func(col(\"interaction_history\"))\n)\n\n# Save results\nrecommendations.write.mode(\"overwrite\").parquet(\"s3://output/recommendations\")\n</code></pre>"},{"location":"en/deployment/#apache-airflow-pipeline","title":"Apache Airflow Pipeline","text":"<p>Create a recommendation pipeline:</p> <pre><code>from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'generative_recommenders_pipeline',\n    default_args=default_args,\n    description='Daily recommendation generation',\n    schedule_interval='@daily',\n    catchup=False\n)\n\ndef extract_user_data(**context):\n    \"\"\"Extract user interaction data\"\"\"\n    # Implementation here\n    pass\n\ndef generate_recommendations(**context):\n    \"\"\"Generate recommendations using TIGER model\"\"\"\n    # Implementation here\n    pass\n\ndef upload_recommendations(**context):\n    \"\"\"Upload recommendations to recommendation service\"\"\"\n    # Implementation here\n    pass\n\n# Define tasks\nextract_task = PythonOperator(\n    task_id='extract_user_data',\n    python_callable=extract_user_data,\n    dag=dag\n)\n\nrecommend_task = PythonOperator(\n    task_id='generate_recommendations',\n    python_callable=generate_recommendations,\n    dag=dag\n)\n\nupload_task = PythonOperator(\n    task_id='upload_recommendations',\n    python_callable=upload_recommendations,\n    dag=dag\n)\n\n# Set dependencies\nextract_task &gt;&gt; recommend_task &gt;&gt; upload_task\n</code></pre>"},{"location":"en/deployment/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"en/deployment/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>Add metrics to your FastAPI application:</p> <pre><code>from prometheus_client import Counter, Histogram, generate_latest\nimport time\n\n# Metrics\nREQUEST_COUNT = Counter('recommendations_requests_total', 'Total recommendation requests')\nREQUEST_LATENCY = Histogram('recommendations_request_duration_seconds', 'Request latency')\nERROR_COUNT = Counter('recommendations_errors_total', 'Total errors')\n\n@app.middleware(\"http\")\nasync def add_metrics(request, call_next):\n    start_time = time.time()\n    REQUEST_COUNT.inc()\n\n    try:\n        response = await call_next(request)\n        return response\n    except Exception as e:\n        ERROR_COUNT.inc()\n        raise\n    finally:\n        REQUEST_LATENCY.observe(time.time() - start_time)\n\n@app.get(\"/metrics\")\nasync def metrics():\n    \"\"\"Prometheus metrics endpoint\"\"\"\n    return Response(generate_latest(), media_type=\"text/plain\")\n</code></pre>"},{"location":"en/deployment/#logging-configuration","title":"Logging Configuration","text":"<p>Set up structured logging:</p> <pre><code>import logging\nimport json\nfrom datetime import datetime\n\nclass JSONFormatter(logging.Formatter):\n    def format(self, record):\n        log_entry = {\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"level\": record.levelname,\n            \"message\": record.getMessage(),\n            \"module\": record.module,\n            \"function\": record.funcName,\n            \"line\": record.lineno\n        }\n\n        if hasattr(record, 'user_id'):\n            log_entry['user_id'] = record.user_id\n\n        if hasattr(record, 'request_id'):\n            log_entry['request_id'] = record.request_id\n\n        return json.dumps(log_entry)\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    handlers=[logging.StreamHandler()],\n    format='%(message)s'\n)\n\nlogger = logging.getLogger(__name__)\nfor handler in logger.handlers:\n    handler.setFormatter(JSONFormatter())\n</code></pre>"},{"location":"en/deployment/#performance-optimization","title":"Performance Optimization","text":""},{"location":"en/deployment/#model-quantization","title":"Model Quantization","text":"<p>Reduce model size and inference time:</p> <pre><code>import torch.quantization as quantization\n\ndef quantize_model(model, example_inputs):\n    \"\"\"Quantize model for faster inference\"\"\"\n    # Prepare model for quantization\n    model.qconfig = quantization.get_default_qconfig('fbgemm')\n    model_prepared = quantization.prepare(model, inplace=False)\n\n    # Calibrate with example inputs\n    model_prepared.eval()\n    with torch.no_grad():\n        model_prepared(example_inputs)\n\n    # Convert to quantized model\n    model_quantized = quantization.convert(model_prepared, inplace=False)\n\n    return model_quantized\n\n# Usage\nexample_input = torch.randint(0, 1000, (1, 50))\nquantized_tiger = quantize_model(tiger_model, example_input)\n</code></pre>"},{"location":"en/deployment/#onnx-export","title":"ONNX Export","text":"<p>Export models to ONNX for cross-platform deployment:</p> <pre><code>def export_to_onnx(model, example_input, output_path):\n    \"\"\"Export PyTorch model to ONNX\"\"\"\n    model.eval()\n\n    torch.onnx.export(\n        model,\n        example_input,\n        output_path,\n        export_params=True,\n        opset_version=11,\n        do_constant_folding=True,\n        input_names=['input_ids'],\n        output_names=['output'],\n        dynamic_axes={\n            'input_ids': {0: 'batch_size', 1: 'sequence'},\n            'output': {0: 'batch_size', 1: 'sequence'}\n        }\n    )\n\n# Export models\nexport_to_onnx(tiger_model, example_input, \"models/tiger.onnx\")\n</code></pre>"},{"location":"en/deployment/#tensorrt-optimization","title":"TensorRT Optimization","text":"<p>Optimize models for NVIDIA GPUs:</p> <pre><code>import tensorrt as trt\n\ndef convert_onnx_to_tensorrt(onnx_path, engine_path, max_batch_size=32):\n    \"\"\"Convert ONNX model to TensorRT engine\"\"\"\n    logger = trt.Logger(trt.Logger.WARNING)\n    builder = trt.Builder(logger)\n    network = builder.create_network(1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n    parser = trt.OnnxParser(network, logger)\n\n    # Parse ONNX model\n    with open(onnx_path, 'rb') as model:\n        if not parser.parse(model.read()):\n            for error in range(parser.num_errors):\n                print(parser.get_error(error))\n            return None\n\n    # Build engine\n    config = builder.create_builder_config()\n    config.max_workspace_size = 1 &lt;&lt; 30  # 1GB\n\n    profile = builder.create_optimization_profile()\n    profile.set_shape(\"input_ids\", (1, 1), (max_batch_size, 512), (max_batch_size, 512))\n    config.add_optimization_profile(profile)\n\n    engine = builder.build_engine(network, config)\n\n    # Save engine\n    with open(engine_path, 'wb') as f:\n        f.write(engine.serialize())\n\n    return engine\n</code></pre>"},{"location":"en/deployment/#ab-testing-framework","title":"A/B Testing Framework","text":""},{"location":"en/deployment/#experiment-configuration","title":"Experiment Configuration","text":"<p>Set up A/B testing for model comparisons:</p> <pre><code>import hashlib\nimport random\nfrom typing import Dict, Any\n\nclass ABTestFramework:\n    def __init__(self, experiments: Dict[str, Any]):\n        self.experiments = experiments\n\n    def get_variant(self, user_id: int, experiment_name: str) -&gt; str:\n        \"\"\"Get user's variant for an experiment\"\"\"\n        if experiment_name not in self.experiments:\n            return \"control\"\n\n        experiment = self.experiments[experiment_name]\n\n        # Use consistent hashing for user assignment\n        hash_input = f\"{user_id}_{experiment_name}_{experiment['salt']}\"\n        hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)\n        bucket = hash_value % 100\n\n        cumulative_traffic = 0\n        for variant, traffic in experiment['variants'].items():\n            cumulative_traffic += traffic\n            if bucket &lt; cumulative_traffic:\n                return variant\n\n        return \"control\"\n\n    def is_user_in_experiment(self, user_id: int, experiment_name: str) -&gt; bool:\n        \"\"\"Check if user is in experiment\"\"\"\n        if experiment_name not in self.experiments:\n            return False\n\n        experiment = self.experiments[experiment_name]\n        if not experiment.get('active', False):\n            return False\n\n        # Check eligibility criteria\n        if 'eligibility' in experiment:\n            # Implement eligibility logic\n            pass\n\n        return True\n\n# Example experiment configuration\nexperiments_config = {\n    \"model_comparison\": {\n        \"active\": True,\n        \"salt\": \"experiment_salt_123\",\n        \"variants\": {\n            \"control\": 50,  # 50% traffic\n            \"new_model\": 50  # 50% traffic\n        },\n        \"eligibility\": {\n            \"min_interactions\": 10\n        }\n    }\n}\n\nab_tester = ABTestFramework(experiments_config)\n\n@app.post(\"/recommend\")\nasync def recommend_with_ab_test(request: RecommendationRequest):\n    \"\"\"Generate recommendations with A/B testing\"\"\"\n    variant = ab_tester.get_variant(request.user_id, \"model_comparison\")\n\n    if variant == \"new_model\":\n        # Use new model\n        recommendations = new_model_service.get_recommendations(\n            request.user_history, request.num_recommendations\n        )\n    else:\n        # Use control model\n        recommendations = model_service.get_recommendations(\n            request.user_history, request.num_recommendations\n        )\n\n    # Log experiment data\n    logger.info(\"Recommendation served\", extra={\n        \"user_id\": request.user_id,\n        \"variant\": variant,\n        \"experiment\": \"model_comparison\"\n    })\n\n    return RecommendationResponse(\n        user_id=request.user_id,\n        recommendations=recommendations\n    )\n</code></pre>"},{"location":"en/deployment/#security-considerations","title":"Security Considerations","text":""},{"location":"en/deployment/#api-authentication","title":"API Authentication","text":"<p>Add JWT authentication:</p> <pre><code>from fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nimport jwt\n\nsecurity = HTTPBearer()\n\ndef verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    \"\"\"Verify JWT token\"\"\"\n    try:\n        payload = jwt.decode(\n            credentials.credentials,\n            SECRET_KEY,\n            algorithms=[\"HS256\"]\n        )\n        return payload\n    except jwt.PyJWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid authentication credentials\"\n        )\n\n@app.post(\"/recommend\")\nasync def recommend(\n    request: RecommendationRequest,\n    token_data: dict = Depends(verify_token)\n):\n    \"\"\"Protected recommendation endpoint\"\"\"\n    # Verify user access\n    if token_data.get(\"user_id\") != request.user_id:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Access denied\"\n        )\n\n    return await model_service.get_recommendations(request)\n</code></pre>"},{"location":"en/deployment/#input-validation","title":"Input Validation","text":"<p>Validate and sanitize inputs:</p> <pre><code>from pydantic import validator\n\nclass RecommendationRequest(BaseModel):\n    user_id: int\n    user_history: List[int]\n    num_recommendations: int = 10\n\n    @validator('user_id')\n    def validate_user_id(cls, v):\n        if v &lt;= 0:\n            raise ValueError('User ID must be positive')\n        return v\n\n    @validator('user_history')\n    def validate_user_history(cls, v):\n        if len(v) &gt; 1000:  # Limit history length\n            raise ValueError('User history too long')\n        if any(item &lt;= 0 for item in v):\n            raise ValueError('Invalid item IDs in history')\n        return v\n\n    @validator('num_recommendations')\n    def validate_num_recommendations(cls, v):\n        if not 1 &lt;= v &lt;= 100:\n            raise ValueError('Number of recommendations must be between 1 and 100')\n        return v\n</code></pre> <p>This deployment guide covers the essential aspects of deploying GenerativeRecommenders models in production, from basic API serving to advanced optimization and monitoring techniques.</p>"},{"location":"en/examples/","title":"Examples","text":"<p>This page contains practical examples for using GenerativeRecommenders.</p>"},{"location":"en/examples/#basic-usage-examples","title":"Basic Usage Examples","text":""},{"location":"en/examples/#training-rqvae-from-scratch","title":"Training RQVAE from Scratch","text":"<pre><code>import torch\nfrom generative_recommenders.models.rqvae import RqVae, QuantizeForwardMode\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\nfrom torch.utils.data import DataLoader\n\n# Create dataset\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n\n# Create model\nmodel = RqVae(\n    input_dim=768,\n    embed_dim=32,\n    hidden_dims=[512, 256, 128],\n    codebook_size=256,\n    n_layers=3,\n    commitment_weight=0.25,\n    codebook_mode=QuantizeForwardMode.ROTATION_TRICK\n)\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n\nfor epoch in range(100):\n    for batch in dataloader:\n        optimizer.zero_grad()\n\n        outputs = model(torch.tensor(batch))\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n</code></pre>"},{"location":"en/examples/#using-dataset-factory","title":"Using Dataset Factory","text":"<pre><code>from generative_recommenders.data.dataset_factory import DatasetFactory\n\n# Create item dataset\nitem_dataset = DatasetFactory.create_item_dataset(\n    \"p5_amazon\",\n    \"dataset/amazon\",\n    split=\"train\"\n)\n\n# Create sequence dataset  \nsequence_dataset = DatasetFactory.create_sequence_dataset(\n    \"p5_amazon\", \n    \"dataset/amazon\",\n    split=\"train\",\n    pretrained_rqvae_path=\"./checkpoints/rqvae.pt\"\n)\n</code></pre>"},{"location":"en/examples/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from generative_recommenders.data.configs import P5AmazonConfig, TextEncodingConfig\n\n# Custom text encoding config\ntext_config = TextEncodingConfig(\n    encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    template=\"Product: {title} | Brand: {brand} | Category: {categories}\",\n    batch_size=32\n)\n\n# Custom dataset config\ndataset_config = P5AmazonConfig(\n    root_dir=\"my_data\",\n    split=\"electronics\",\n    text_config=text_config\n)\n</code></pre>"},{"location":"en/examples/#advanced-examples","title":"Advanced Examples","text":""},{"location":"en/examples/#multi-gpu-training","title":"Multi-GPU Training","text":"<pre><code>from accelerate import Accelerator\n\ndef train_with_accelerate():\n    accelerator = Accelerator()\n\n    # Model, optimizer, dataloader\n    model = RqVae(...)\n    optimizer = torch.optim.AdamW(model.parameters())\n    dataloader = DataLoader(...)\n\n    # Prepare for distributed training\n    model, optimizer, dataloader = accelerator.prepare(\n        model, optimizer, dataloader\n    )\n\n    for epoch in range(epochs):\n        for batch in dataloader:\n            optimizer.zero_grad()\n\n            with accelerator.autocast():\n                outputs = model(batch)\n                loss = outputs.loss\n\n            accelerator.backward(loss)\n            optimizer.step()\n</code></pre>"},{"location":"en/examples/#custom-dataset-implementation","title":"Custom Dataset Implementation","text":"<pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\n\nclass MyCustomDataset(BaseRecommenderDataset):\n    def download(self):\n        # Implement data download logic\n        pass\n\n    def load_raw_data(self):\n        # Load your raw data files\n        return {\"items\": items_df, \"interactions\": interactions_df}\n\n    def preprocess_data(self, raw_data):\n        # Custom preprocessing\n        return processed_data\n\n    def extract_items(self, processed_data):\n        return processed_data[\"items\"]\n\n    def extract_interactions(self, processed_data):\n        return processed_data[\"interactions\"]\n</code></pre>"},{"location":"en/examples/#integration-examples","title":"Integration Examples","text":""},{"location":"en/examples/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<pre><code>import wandb\n\n# Initialize wandb\nwandb.init(\n    project=\"my-recommendation-project\",\n    config={\n        \"learning_rate\": 0.0005,\n        \"batch_size\": 64,\n        \"model_type\": \"rqvae\"\n    }\n)\n\n# Log metrics during training\nfor epoch in range(epochs):\n    # ... training code ...\n\n    wandb.log({\n        \"epoch\": epoch,\n        \"loss\": loss.item(),\n        \"reconstruction_loss\": recon_loss.item(),\n        \"quantization_loss\": quant_loss.item()\n    })\n</code></pre>"},{"location":"en/examples/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code>import optuna\n\ndef objective(trial):\n    # Suggest hyperparameters\n    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n    embed_dim = trial.suggest_categorical(\"embed_dim\", [16, 32, 64])\n\n    # Train model with suggested parameters\n    model = RqVae(embed_dim=embed_dim, ...)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\n    # Training loop\n    val_loss = train_and_evaluate(model, optimizer, batch_size)\n\n    return val_loss\n\n# Run optimization\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=100)\n</code></pre>"},{"location":"en/examples/#evaluation-examples","title":"Evaluation Examples","text":""},{"location":"en/examples/#model-evaluation","title":"Model Evaluation","text":"<pre><code>def evaluate_model(model, test_dataloader, device):\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            batch = batch.to(device)\n            outputs = model(batch)\n\n            total_loss += outputs.loss.item() * len(batch)\n            total_samples += len(batch)\n\n    return total_loss / total_samples\n\n# Evaluate RQVAE\ntest_loss = evaluate_model(rqvae_model, test_dataloader, device)\nprint(f\"Test reconstruction loss: {test_loss:.4f}\")\n</code></pre>"},{"location":"en/examples/#recommendation-generation","title":"Recommendation Generation","text":"<pre><code>def generate_recommendations(tiger_model, user_sequence, top_k=10):\n    \"\"\"Generate top-K recommendations for a user sequence\"\"\"\n    tiger_model.eval()\n\n    with torch.no_grad():\n        # Encode user sequence\n        logits = tiger_model.generate(user_sequence, max_length=top_k)\n\n        # Get top-K items\n        top_items = torch.topk(logits, top_k).indices\n\n    return top_items.tolist()\n\n# Generate recommendations\nuser_seq = [1, 5, 23, 45]  # User's interaction history\nrecommendations = generate_recommendations(tiger_model, user_seq, top_k=10)\nprint(f\"Recommended items: {recommendations}\")\n</code></pre>"},{"location":"en/examples/#utilities-and-helpers","title":"Utilities and Helpers","text":""},{"location":"en/examples/#data-analysis","title":"Data Analysis","text":"<pre><code>from generative_recommenders.data.processors.sequence_processor import SequenceStatistics\n\n# Analyze sequence statistics\nstats = SequenceStatistics.compute_sequence_stats(sequence_data)\nprint(f\"Average sequence length: {stats['avg_seq_length']:.2f}\")\nprint(f\"Number of unique items: {stats['num_unique_items']}\")\n</code></pre>"},{"location":"en/examples/#model-inspection","title":"Model Inspection","text":"<pre><code>def inspect_codebook_usage(rqvae_model, dataloader):\n    \"\"\"Analyze codebook utilization\"\"\"\n    used_codes = set()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            outputs = rqvae_model(batch)\n            semantic_ids = outputs.sem_ids\n            used_codes.update(semantic_ids.flatten().tolist())\n\n    usage_rate = len(used_codes) / rqvae_model.codebook_size\n    print(f\"Codebook usage: {usage_rate:.2%}\")\n\n    return used_codes\n\nused_codes = inspect_codebook_usage(model, dataloader)\n</code></pre>"},{"location":"en/examples/#tips-and-best-practices","title":"Tips and Best Practices","text":""},{"location":"en/examples/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Enable gradient checkpointing for large models\nmodel.gradient_checkpointing_enable()\n\n# Use mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor batch in dataloader:\n    optimizer.zero_grad()\n\n    with autocast():\n        outputs = model(batch)\n        loss = outputs.loss\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre>"},{"location":"en/examples/#debugging","title":"Debugging","text":"<pre><code># Enable detailed logging\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n# Log model statistics\ndef log_model_stats(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    logger.info(f\"Total parameters: {total_params:,}\")\n    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n</code></pre>"},{"location":"en/faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>This page contains common questions and answers when using the GenerativeRecommenders framework.</p>"},{"location":"en/faq/#installation-and-environment","title":"Installation and Environment","text":""},{"location":"en/faq/#q-how-do-i-install-generativerecommenders","title":"Q: How do I install GenerativeRecommenders?","text":"<p>A: Currently supports installation from source:</p> <pre><code>git clone https://github.com/phonism/GenerativeRecommenders.git\ncd GenerativeRecommenders\npip install -e .\n</code></pre>"},{"location":"en/faq/#q-which-python-versions-are-supported","title":"Q: Which Python versions are supported?","text":"<p>A: We recommend Python 3.8 or higher. The framework has been tested on Python 3.8, 3.9, and 3.10.</p>"},{"location":"en/faq/#q-what-are-the-main-dependencies","title":"Q: What are the main dependencies?","text":"<p>A: Main dependencies include: - PyTorch &gt;= 1.11.0 - PyTorch Lightning &gt;= 1.6.0 - sentence-transformers &gt;= 2.2.0 - pandas &gt;= 1.3.0 - numpy &gt;= 1.21.0</p>"},{"location":"en/faq/#q-does-it-support-gpu-training","title":"Q: Does it support GPU training?","text":"<p>A: Yes, the framework fully supports GPU training. Make sure you have the correct PyTorch CUDA version installed.</p>"},{"location":"en/faq/#data-and-datasets","title":"Data and Datasets","text":""},{"location":"en/faq/#q-what-dataset-formats-are-supported","title":"Q: What dataset formats are supported?","text":"<p>A: The framework supports: - JSON format recommendation datasets - CSV format user-item interaction data - Parquet format large-scale datasets - Custom formats (by inheriting base classes)</p>"},{"location":"en/faq/#q-how-do-i-add-a-custom-dataset","title":"Q: How do I add a custom dataset?","text":"<p>A: Inherit from the <code>BaseRecommenderDataset</code> class and implement necessary methods:</p> <pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\n\nclass MyDataset(BaseRecommenderDataset):\n    def download(self):\n        # Implement data download logic\n        pass\n\n    def load_raw_data(self):\n        # Implement data loading logic\n        return {\"items\": items_df, \"interactions\": interactions_df}\n\n    def preprocess_data(self, raw_data):\n        # Implement data preprocessing logic\n        return processed_data\n</code></pre>"},{"location":"en/faq/#q-how-large-is-the-p5-amazon-dataset","title":"Q: How large is the P5 Amazon dataset?","text":"<p>A: Different categories have different sizes: - Beauty: ~500MB - Electronics: ~2GB - Sports: ~1GB - Full dataset may require 10GB+ storage</p>"},{"location":"en/faq/#q-how-are-missing-item-features-handled","title":"Q: How are missing item features handled?","text":"<p>A: The framework automatically handles missing features: - Text fields are filled with \"Unknown\" - Numerical fields are filled with mean or 0 - Filling strategies can be customized in configuration</p>"},{"location":"en/faq/#model-training","title":"Model Training","text":""},{"location":"en/faq/#q-how-long-does-rqvae-training-take","title":"Q: How long does RQVAE training take?","text":"<p>A: Depends on dataset size and hardware: - Small datasets (&lt;100k items): 30 minutes - 2 hours - Medium datasets (100k-1M items): 2-8 hours - Large datasets (&gt;1M items): 8-24 hours</p>"},{"location":"en/faq/#q-what-are-tiger-training-memory-requirements","title":"Q: What are TIGER training memory requirements?","text":"<p>A: Typical memory usage: - Minimum: 8GB GPU memory (small batch size) - Recommended: 16GB GPU memory (medium batch size) - Large-scale: 32GB+ GPU memory (large batch size)</p>"},{"location":"en/faq/#q-how-do-i-choose-appropriate-embedding-dimensions","title":"Q: How do I choose appropriate embedding dimensions?","text":"<p>A: Rule of thumb: - Small datasets: 256-512 dimensions - Medium datasets: 512-768 dimensions - Large datasets: 768-1024 dimensions - Specific choice should be based on validation set performance</p>"},{"location":"en/faq/#q-what-to-do-when-encountering-cuda-out-of-memory-during-training","title":"Q: What to do when encountering CUDA out of memory during training?","text":"<p>A: Solutions: 1. Reduce batch size 2. Use gradient accumulation 3. Enable mixed precision training 4. Reduce model size</p> <pre><code># Reduce batch size\nconfig.batch_size = 16\n\n# Gradient accumulation\nconfig.accumulate_grad_batches = 4\n\n# Mixed precision\nconfig.precision = 16\n</code></pre>"},{"location":"en/faq/#model-usage","title":"Model Usage","text":""},{"location":"en/faq/#q-how-do-i-generate-recommendations","title":"Q: How do I generate recommendations?","text":"<p>A: Basic recommendation generation:</p> <pre><code># Load models\nrqvae = RqVae.load_from_checkpoint(\"rqvae.ckpt\")\ntiger = Tiger.load_from_checkpoint(\"tiger.ckpt\")\n\n# User history (item IDs)\nuser_history = [1, 5, 23, 67]\n\n# Convert to semantic IDs\nsemantic_ids = rqvae.encode_items(user_history)\n\n# Generate recommendations\nrecommendations = tiger.generate(semantic_ids, max_length=10)\n</code></pre>"},{"location":"en/faq/#q-how-do-i-handle-cold-start-users","title":"Q: How do I handle cold start users?","text":"<p>A: For new users: 1. Use popular item recommendations 2. Content-based recommendations using user profile 3. Similarity-based recommendations using item features</p> <pre><code>def recommend_for_new_user(user_profile, k=10):\n    # Find similar items based on user profile\n    similar_items = find_similar_items_by_profile(user_profile)\n    return similar_items[:k]\n</code></pre>"},{"location":"en/faq/#q-how-do-i-ensure-recommendation-diversity","title":"Q: How do I ensure recommendation diversity?","text":"<p>A: Methods to improve diversity: 1. Use top-p sampling instead of greedy sampling 2. Post-processing for deduplication and diversification 3. Add diversity loss during training</p> <pre><code># Use sampling for generation\nrecommendations = tiger.generate(\n    input_seq, \n    temperature=0.8,\n    top_p=0.9,\n    do_sample=True\n)\n</code></pre>"},{"location":"en/faq/#performance-and-optimization","title":"Performance and Optimization","text":""},{"location":"en/faq/#q-how-do-i-improve-inference-speed","title":"Q: How do I improve inference speed?","text":"<p>A: Optimization methods: 1. Model quantization 2. ONNX export 3. TensorRT optimization 4. Batch inference</p> <pre><code># Model quantization\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\n\n# Batch inference\ndef batch_recommend(user_histories, batch_size=32):\n    results = []\n    for i in range(0, len(user_histories), batch_size):\n        batch = user_histories[i:i+batch_size]\n        batch_results = model.batch_generate(batch)\n        results.extend(batch_results)\n    return results\n</code></pre>"},{"location":"en/faq/#q-can-model-size-be-compressed","title":"Q: Can model size be compressed?","text":"<p>A: Compression techniques: - Quantization: 50-75% size reduction - Pruning: Remove unimportant parameters - Knowledge distillation: Train small model to mimic large model</p>"},{"location":"en/faq/#q-how-do-i-monitor-model-performance","title":"Q: How do I monitor model performance?","text":"<p>A: Monitor metrics: - Inference latency - Memory usage - GPU utilization - Recommendation quality metrics (Recall, NDCG)</p>"},{"location":"en/faq/#errors-and-debugging","title":"Errors and Debugging","text":""},{"location":"en/faq/#q-getting-runtimeerror-cuda-out-of-memory-during-training","title":"Q: Getting \"RuntimeError: CUDA out of memory\" during training?","text":"<p>A: Solution steps: 1. Reduce batch_size 2. Enable gradient checkpointing 3. Clear GPU cache</p> <pre><code>import torch\ntorch.cuda.empty_cache()\n\n# Or in training configuration\nconfig.gradient_checkpointing = True\n</code></pre>"},{"location":"en/faq/#q-what-to-do-when-model-loading-fails","title":"Q: What to do when model loading fails?","text":"<p>A: Check items: 1. Check if checkpoint file is complete 2. PyTorch version compatibility 3. Model architecture match</p> <pre><code>try:\n    model = Model.load_from_checkpoint(checkpoint_path)\nexcept Exception as e:\n    print(f\"Loading failed: {e}\")\n    # Try loading state dict\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['state_dict'])\n</code></pre>"},{"location":"en/faq/#q-what-to-do-when-training-loss-doesnt-converge","title":"Q: What to do when training loss doesn't converge?","text":"<p>A: Debugging methods: 1. Check learning rate (may be too large or too small) 2. Check data preprocessing 3. Increase training data amount 4. Adjust model architecture</p> <pre><code># Learning rate scheduling\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, 'min', patience=5, factor=0.5\n)\n</code></pre>"},{"location":"en/faq/#deployment-and-production","title":"Deployment and Production","text":""},{"location":"en/faq/#q-how-do-i-deploy-to-production","title":"Q: How do I deploy to production?","text":"<p>A: Deployment options: 1. REST API service (FastAPI/Flask) 2. Docker containerization 3. Kubernetes cluster 4. Cloud service platforms</p>"},{"location":"en/faq/#q-does-it-support-real-time-recommendations","title":"Q: Does it support real-time recommendations?","text":"<p>A: Yes, the framework supports: - Online inference API - Batch pre-computation - Streaming processing integration</p>"},{"location":"en/faq/#q-how-do-i-conduct-ab-testing","title":"Q: How do I conduct A/B testing?","text":"<p>A: A/B testing framework:</p> <pre><code>class ABTestFramework:\n    def get_variant(self, user_id, experiment_name):\n        # Consistent hash-based user grouping\n        hash_value = hash(f\"{user_id}_{experiment_name}\")\n        return \"A\" if hash_value % 2 == 0 else \"B\"\n\n    def recommend_with_test(self, user_id, user_history):\n        variant = self.get_variant(user_id, \"model_test\")\n        if variant == \"A\":\n            return model_a.recommend(user_history)\n        else:\n            return model_b.recommend(user_history)\n</code></pre>"},{"location":"en/faq/#advanced-usage","title":"Advanced Usage","text":""},{"location":"en/faq/#q-how-do-i-implement-multi-task-learning","title":"Q: How do I implement multi-task learning?","text":"<p>A: Extend model to support multiple objectives:</p> <pre><code>class MultiTaskTiger(Tiger):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.rating_head = nn.Linear(self.embedding_dim, 1)\n        self.category_head = nn.Linear(self.embedding_dim, num_categories)\n\n    def forward(self, x):\n        hidden = super().forward(x)\n\n        # Multiple output heads\n        recommendations = self.recommendation_head(hidden)\n        ratings = self.rating_head(hidden)\n        categories = self.category_head(hidden)\n\n        return recommendations, ratings, categories\n</code></pre>"},{"location":"en/faq/#q-how-do-i-integrate-external-features","title":"Q: How do I integrate external features?","text":"<p>A: Feature fusion methods:</p> <pre><code>class FeatureEnhancedModel(Tiger):\n    def __init__(self, user_feature_dim, item_feature_dim, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.user_feature_proj = nn.Linear(user_feature_dim, self.embedding_dim)\n        self.item_feature_proj = nn.Linear(item_feature_dim, self.embedding_dim)\n\n    def forward(self, item_seq, user_features=None, item_features=None):\n        seq_emb = super().forward(item_seq)\n\n        if user_features is not None:\n            user_emb = self.user_feature_proj(user_features)\n            seq_emb = seq_emb + user_emb.unsqueeze(1)\n\n        return seq_emb\n</code></pre>"},{"location":"en/faq/#q-how-do-i-handle-temporal-information-in-sequences","title":"Q: How do I handle temporal information in sequences?","text":"<p>A: Time-aware recommendations:</p> <pre><code>class TimeAwareTiger(Tiger):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.time_emb = nn.Embedding(24 * 7, self.embedding_dim)  # hours*days\n\n    def forward(self, item_seq, time_seq=None):\n        seq_emb = self.item_embedding(item_seq)\n\n        if time_seq is not None:\n            time_emb = self.time_emb(time_seq)\n            seq_emb = seq_emb + time_emb\n\n        return self.transformer(seq_emb)\n</code></pre> <p>If you have other questions, please check the API documentation or submit an issue on GitHub.</p>"},{"location":"en/getting-started/","title":"Getting Started","text":"<p>This guide will help you quickly get started with the GenerativeRecommenders framework.</p>"},{"location":"en/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>CUDA 11.0+ (if using GPU)</li> <li>8GB+ GPU memory (recommended)</li> </ul>"},{"location":"en/getting-started/#installation","title":"Installation","text":""},{"location":"en/getting-started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/phonism/GenerativeRecommenders.git\ncd GenerativeRecommenders\n</code></pre>"},{"location":"en/getting-started/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"en/getting-started/#3-prepare-data","title":"3. Prepare Data","text":"<p>Download the P5 Amazon dataset:</p> <pre><code># Data will be automatically downloaded to dataset/amazon directory\nmkdir -p dataset/amazon\n</code></pre>"},{"location":"en/getting-started/#first-experiment-training-rqvae","title":"First Experiment: Training RQVAE","text":""},{"location":"en/getting-started/#1-check-configuration-file","title":"1. Check Configuration File","text":"<pre><code>cat config/rqvae/p5_amazon.gin\n</code></pre> <p>Key configuration parameters: - <code>train.iterations=400000</code>: Number of training iterations - <code>train.batch_size=64</code>: Batch size - <code>train.learning_rate=0.0005</code>: Learning rate - <code>train.dataset_folder=\"dataset/amazon\"</code>: Dataset path</p>"},{"location":"en/getting-started/#2-start-training","title":"2. Start Training","text":"<pre><code>python generative_recommenders/trainers/rqvae_trainer.py config/rqvae/p5_amazon.gin\n</code></pre> <p>During training you'll see: - Automatic data download and preprocessing - Text feature encoding progress - Training loss and metrics - Model checkpoint saving</p>"},{"location":"en/getting-started/#3-monitor-training","title":"3. Monitor Training","text":"<p>If Weights &amp; Biases logging is enabled:</p> <pre><code># Set in configuration file\ntrain.wandb_logging=True\ntrain.wandb_project=\"your_project_name\"\n</code></pre> <p>Visit wandb.ai to view training progress.</p>"},{"location":"en/getting-started/#second-experiment-training-tiger","title":"Second Experiment: Training TIGER","text":""},{"location":"en/getting-started/#1-ensure-rqvae-is-trained","title":"1. Ensure RQVAE is Trained","text":"<p>TIGER requires a pre-trained RQVAE model to generate semantic IDs:</p> <pre><code># Check if RQVAE checkpoint exists\nls out/rqvae/p5_amazon/beauty/checkpoint_*.pt\n</code></pre>"},{"location":"en/getting-started/#2-configure-tiger","title":"2. Configure TIGER","text":"<p>Edit <code>config/tiger/p5_amazon.gin</code>:</p> <pre><code>train.pretrained_rqvae_path=\"./out/rqvae/p5_amazon/beauty/checkpoint_299999.pt\"\n</code></pre>"},{"location":"en/getting-started/#3-start-training","title":"3. Start Training","text":"<pre><code>python generative_recommenders/trainers/tiger_trainer.py config/tiger/p5_amazon.gin\n</code></pre>"},{"location":"en/getting-started/#understanding-the-framework-structure","title":"Understanding the Framework Structure","text":""},{"location":"en/getting-started/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>graph TD\n    A[Raw Data] --&gt; B[Data Download]\n    B --&gt; C[Preprocessing]\n    C --&gt; D[Text Encoding]\n    D --&gt; E[Sequence Generation]\n    E --&gt; F[Dataset]</code></pre>"},{"location":"en/getting-started/#model-training-flow","title":"Model Training Flow","text":"<pre><code>graph TD\n    A[Config File] --&gt; B[Dataset Loading]\n    B --&gt; C[Model Initialization]\n    C --&gt; D[Training Loop]\n    D --&gt; E[Evaluation]\n    E --&gt; F[Checkpoint Saving]\n    F --&gt; D</code></pre>"},{"location":"en/getting-started/#custom-configuration","title":"Custom Configuration","text":""},{"location":"en/getting-started/#creating-custom-configuration","title":"Creating Custom Configuration","text":"<pre><code># my_config.gin\nimport generative_recommenders.data.p5_amazon\nimport generative_recommenders.models.rqvae\n\n# Custom parameters\ntrain.batch_size=32\ntrain.learning_rate=0.001\ntrain.vae_hidden_dims=[256, 128, 64]\n\n# Use custom data path\ntrain.dataset_folder=\"path/to/my/data\"\n</code></pre>"},{"location":"en/getting-started/#using-custom-configuration","title":"Using Custom Configuration","text":"<pre><code>python generative_recommenders/trainers/rqvae_trainer.py my_config.gin\n</code></pre>"},{"location":"en/getting-started/#model-evaluation","title":"Model Evaluation","text":""},{"location":"en/getting-started/#rqvae-evaluation","title":"RQVAE Evaluation","text":"<pre><code>from generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\n# Load model\nmodel = RqVae.load_from_checkpoint(\"path/to/checkpoint.pt\")\n\n# Load test data\ntest_dataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\", \n    train_test_split=\"eval\"\n)\n\n# Evaluate reconstruction quality\nreconstruction_loss = model.evaluate(test_dataset)\n</code></pre>"},{"location":"en/getting-started/#tiger-evaluation","title":"TIGER Evaluation","text":"<pre><code>from generative_recommenders.models.tiger import Tiger\nfrom generative_recommenders.modules.metrics import TopKAccumulator\n\n# Load model\nmodel = Tiger.load_from_checkpoint(\"path/to/checkpoint.pt\")\n\n# Calculate Recall@K\nmetrics = TopKAccumulator(k=10)\nrecall = metrics.compute_recall(model, test_dataloader)\n</code></pre>"},{"location":"en/getting-started/#common-issues","title":"Common Issues","text":""},{"location":"en/getting-started/#q-out-of-memory","title":"Q: Out of memory?","text":"<p>A: Adjust these parameters: <pre><code>train.batch_size=16  # Reduce batch size\ntrain.vae_hidden_dims=[256, 128]  # Reduce model size\n</code></pre></p>"},{"location":"en/getting-started/#q-training-too-slow","title":"Q: Training too slow?","text":"<p>A: Optimization suggestions: - Use larger batch sizes - Enable mixed precision training - Use multi-GPU training</p>"},{"location":"en/getting-started/#q-how-to-add-new-datasets","title":"Q: How to add new datasets?","text":"<p>A: Refer to the Custom Dataset Guide</p>"},{"location":"en/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Model Architectures</li> <li>Understand Dataset Processing</li> <li>Check API Documentation</li> <li>Explore Advanced Examples</li> </ul>"},{"location":"en/installation/","title":"Installation Guide","text":"<p>This guide provides detailed installation instructions for the GenerativeRecommenders framework.</p>"},{"location":"en/installation/#system-requirements","title":"System Requirements","text":""},{"location":"en/installation/#hardware-requirements","title":"Hardware Requirements","text":"<p>Minimum Configuration: - CPU: 4 cores - RAM: 8 GB - Storage: 20 GB free space</p> <p>Recommended Configuration: - CPU: 8+ cores - RAM: 16+ GB - GPU: NVIDIA GPU (8GB+ VRAM) - Storage: 50+ GB SSD</p>"},{"location":"en/installation/#software-requirements","title":"Software Requirements","text":"<ul> <li>Python 3.8 - 3.11</li> <li>CUDA 11.0+ (if using GPU)</li> <li>Git</li> </ul>"},{"location":"en/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"en/installation/#method-1-install-from-source-recommended","title":"Method 1: Install from Source (Recommended)","text":""},{"location":"en/installation/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/phonism/GenerativeRecommenders.git\ncd GenerativeRecommenders\n</code></pre>"},{"location":"en/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<p>Using conda: <pre><code>conda create -n genrec python=3.10\nconda activate genrec\n</code></pre></p> <p>Using venv: <pre><code>python -m venv genrec_env\nsource genrec_env/bin/activate  # Linux/Mac\n# or\ngenrec_env\\Scripts\\activate     # Windows\n</code></pre></p>"},{"location":"en/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"en/installation/#method-2-development-installation","title":"Method 2: Development Installation","text":"<p>If you plan to modify the code or contribute:</p> <pre><code>git clone https://github.com/phonism/GenerativeRecommenders.git\ncd GenerativeRecommenders\n\n# Create development environment\npip install -r requirements.txt\npip install -r requirements-dev.txt  # Development dependencies\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"en/installation/#dependencies-overview","title":"Dependencies Overview","text":""},{"location":"en/installation/#core-dependencies","title":"Core Dependencies","text":"<pre><code># Deep learning frameworks\ntorch==2.6.0\ntorchvision==0.21.0\ntorch_geometric==2.6.1\n\n# Distributed training\naccelerate==0.31.0\n\n# Configuration management\ngin_config==0.5.0\n\n# Data processing\npandas==1.5.3\npolars==1.9.0\nnumpy==1.24.3\n\n# Text processing\nsentence_transformers==3.3.1\n\n# Experiment tracking\nwandb==0.19.0\n\n# Utilities\neinops==0.8.0\ntqdm==4.65.0\n</code></pre>"},{"location":"en/installation/#optional-dependencies","title":"Optional Dependencies","text":"<pre><code># Recommendation-specific libraries (optional)\npip install fbgemm_gpu==1.1.0\npip install torchrec==1.1.0\n\n# Development tools (optional)\npip install black isort flake8 pytest\n</code></pre>"},{"location":"en/installation/#gpu-support","title":"GPU Support","text":""},{"location":"en/installation/#check-cuda-installation","title":"Check CUDA Installation","text":"<pre><code>python -c \"import torch; print(f'CUDA Available: {torch.cuda.is_available()}')\"\npython -c \"import torch; print(f'CUDA Version: {torch.version.cuda}')\"\n</code></pre>"},{"location":"en/installation/#install-cuda-enabled-pytorch","title":"Install CUDA-enabled PyTorch","text":"<p>If the automatic installation doesn't include CUDA:</p> <pre><code># CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# CUDA 12.1  \npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n</code></pre>"},{"location":"en/installation/#verify-installation","title":"Verify Installation","text":""},{"location":"en/installation/#basic-verification","title":"Basic Verification","text":"<pre><code>python -c \"\nimport torch\nimport pandas as pd\nimport sentence_transformers\nprint('\u2713 Basic dependencies installed successfully')\n\"\n</code></pre>"},{"location":"en/installation/#framework-verification","title":"Framework Verification","text":"<pre><code>python -c \"\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\nfrom generative_recommenders.models.rqvae import RqVae\nprint('\u2713 GenerativeRecommenders installed successfully')\n\"\n</code></pre>"},{"location":"en/installation/#gpu-verification","title":"GPU Verification","text":"<pre><code>python -c \"\nimport torch\nprint(f'GPU count: {torch.cuda.device_count()}')\nif torch.cuda.is_available():\n    print(f'GPU model: {torch.cuda.get_device_name(0)}')\n    print('\u2713 GPU support available')\nelse:\n    print('\u26a0 GPU not available, will use CPU')\n\"\n</code></pre>"},{"location":"en/installation/#common-issues","title":"Common Issues","text":""},{"location":"en/installation/#q-importerror-no-module-named-torch","title":"Q: ImportError: No module named 'torch'","text":"<p>Solution: <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n# or install CUDA version (see above)\n</code></pre></p>"},{"location":"en/installation/#q-cuda-out-of-memory","title":"Q: CUDA out of memory","text":"<p>Solution: - Reduce batch size: <code>train.batch_size=16</code> - Enable gradient accumulation: <code>train.gradient_accumulate_every=4</code> - Use mixed precision: <code>train.mixed_precision_type=\"fp16\"</code></p>"},{"location":"en/installation/#q-sentence-transformers-download-slow","title":"Q: sentence-transformers download slow","text":"<p>Solution: <pre><code># Set environment variable to use mirror\nexport HF_ENDPOINT=https://hf-mirror.com\n\n# or pre-download model\npython -c \"\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('sentence-transformers/sentence-t5-xl')\n\"\n</code></pre></p>"},{"location":"en/installation/#q-dataset-download-fails","title":"Q: Dataset download fails","text":"<p>Solution: <pre><code># Manually set proxy\nexport HTTP_PROXY=http://your-proxy:port\nexport HTTPS_PROXY=http://your-proxy:port\n\n# or manually download dataset to dataset/ directory\n</code></pre></p>"},{"location":"en/installation/#q-windows-path-issues","title":"Q: Windows path issues","text":"<p>Solution: <pre><code># Use forward slashes or raw strings\ntrain.dataset_folder=\"dataset/amazon\"\n# or\ntrain.dataset_folder=r\"dataset\\amazon\"\n</code></pre></p>"},{"location":"en/installation/#performance-optimization","title":"Performance Optimization","text":""},{"location":"en/installation/#system-level-optimization","title":"System-level Optimization","text":"<pre><code># Linux: Increase shared memory\necho 'vm.overcommit_memory=1' &gt;&gt; /etc/sysctl.conf\n\n# Set PyTorch thread count\nexport OMP_NUM_THREADS=4\nexport MKL_NUM_THREADS=4\n</code></pre>"},{"location":"en/installation/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Set before training\nimport torch\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n</code></pre>"},{"location":"en/installation/#docker-installation-optional","title":"Docker Installation (Optional)","text":""},{"location":"en/installation/#using-pre-built-image","title":"Using Pre-built Image","text":"<pre><code>docker pull pytorch/pytorch:2.6.0-cuda12.1-cudnn9-devel\n\ndocker run -it --gpus all -v $(pwd):/workspace pytorch/pytorch:2.6.0-cuda12.1-cudnn9-devel\ncd /workspace\npip install -r requirements.txt\n</code></pre>"},{"location":"en/installation/#custom-dockerfile","title":"Custom Dockerfile","text":"<pre><code>FROM pytorch/pytorch:2.6.0-cuda12.1-cudnn9-devel\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\nCMD [\"python\", \"-c\", \"print('GenerativeRecommenders ready!')\"]\n</code></pre>"},{"location":"en/installation/#next-steps","title":"Next Steps","text":"<p>After installation, you can:</p> <ol> <li>Read the Getting Started Guide</li> <li>Learn about Dataset Preparation </li> <li>Start your First Training Experiment</li> <li>Check API Documentation</li> </ol> <p>If you encounter other issues, please check our FAQ or submit an Issue on GitHub.</p>"},{"location":"en/api/","title":"API Reference","text":"<p>This section provides detailed API documentation for the GenerativeRecommenders framework.</p>"},{"location":"en/api/#core-modules","title":"Core Modules","text":""},{"location":"en/api/#models","title":"Models","text":"<ul> <li>RQVAE: Residual Quantized Variational Autoencoder</li> <li>TIGER: Transformer-based generative retrieval model</li> </ul>"},{"location":"en/api/#data-processing","title":"Data Processing","text":"<ul> <li>Base Dataset: Abstract base classes for datasets</li> <li>Configurations: Configuration management classes</li> <li>Processors: Text and sequence processing utilities</li> <li>Dataset Factory: Factory pattern for dataset creation</li> </ul>"},{"location":"en/api/#training","title":"Training","text":"<ul> <li>Trainers: Training utilities and scripts</li> <li>Modules: Core building blocks (encoders, losses, metrics)</li> </ul>"},{"location":"en/api/#quick-navigation","title":"Quick Navigation","text":""},{"location":"en/api/#core-components","title":"Core Components","text":"<p>Models: - RQVAE Model Class - Vector quantized variational autoencoder - TIGER Model Class - Transformer-based generative retrieval</p> <p>Data Processing: - Dataset Classes - Data loading and preprocessing - Configuration System - Parameter management - Processors - Text and sequence processing utilities</p> <p>Training: - RQVAE Training - Training procedures and configuration - TIGER Training - Advanced training workflows</p>"},{"location":"en/api/#code-examples","title":"Code Examples","text":"<p>See the Examples page for practical usage patterns and code snippets.</p>"},{"location":"en/api/base-dataset/","title":"Base Dataset API Reference","text":"<p>Detailed documentation for abstract base classes and common data processing interfaces.</p>"},{"location":"en/api/base-dataset/#abstract-base-classes","title":"Abstract Base Classes","text":""},{"location":"en/api/base-dataset/#baserecommenderdataset","title":"BaseRecommenderDataset","text":"<p>Abstract base class for all recommendation datasets.</p> <pre><code>class BaseRecommenderDataset(ABC):\n    def __init__(self, config: DatasetConfig):\n        self.config = config\n        self.root_path = Path(config.root_dir)\n        self.text_processor = TextProcessor(config.text_config)\n</code></pre> <p>Parameters: - <code>config</code>: Dataset configuration object</p> <p>Abstract Methods:</p>"},{"location":"en/api/base-dataset/#download","title":"download()","text":"<p>Download dataset to local storage.</p> <pre><code>@abstractmethod\ndef download(self) -&gt; None:\n    \"\"\"Download dataset to local storage\"\"\"\n    pass\n</code></pre>"},{"location":"en/api/base-dataset/#load_raw_data","title":"load_raw_data()","text":"<p>Load raw data files.</p> <pre><code>@abstractmethod\ndef load_raw_data(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load raw data files\n\n    Returns:\n        Dictionary containing raw data, typically with 'items' and 'interactions' keys\n    \"\"\"\n    pass\n</code></pre>"},{"location":"en/api/base-dataset/#preprocess_dataraw_data","title":"preprocess_data(raw_data)","text":"<p>Preprocess raw data.</p> <pre><code>@abstractmethod\ndef preprocess_data(self, raw_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Preprocess raw data\n\n    Args:\n        raw_data: Raw data dictionary\n\n    Returns:\n        Preprocessed data dictionary\n    \"\"\"\n    pass\n</code></pre>"},{"location":"en/api/base-dataset/#extract_itemsprocessed_data","title":"extract_items(processed_data)","text":"<p>Extract item information.</p> <pre><code>@abstractmethod\ndef extract_items(self, processed_data: Dict[str, Any]) -&gt; pd.DataFrame:\n    \"\"\"\n    Extract item information\n\n    Args:\n        processed_data: Preprocessed data\n\n    Returns:\n        Items DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"en/api/base-dataset/#extract_interactionsprocessed_data","title":"extract_interactions(processed_data)","text":"<p>Extract user interaction information.</p> <pre><code>@abstractmethod\ndef extract_interactions(self, processed_data: Dict[str, Any]) -&gt; pd.DataFrame:\n    \"\"\"\n    Extract user interaction information\n\n    Args:\n        processed_data: Preprocessed data\n\n    Returns:\n        Interactions DataFrame\n    \"\"\"\n    pass\n</code></pre> <p>Public Methods:</p>"},{"location":"en/api/base-dataset/#get_dataset","title":"get_dataset()","text":"<p>Get complete dataset.</p> <pre><code>def get_dataset(self) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Get complete dataset\n\n    Returns:\n        (items_df, interactions_df): Item and interaction DataFrames\n    \"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#filter_low_interactionsinteractions_df-min_user_interactions-min_item_interactions","title":"filter_low_interactions(interactions_df, min_user_interactions, min_item_interactions)","text":"<p>Filter low-frequency users and items.</p> <pre><code>def filter_low_interactions(\n    self,\n    interactions_df: pd.DataFrame,\n    min_user_interactions: int = 5,\n    min_item_interactions: int = 5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter low-frequency users and items\n\n    Args:\n        interactions_df: Interactions DataFrame\n        min_user_interactions: Minimum user interactions\n        min_item_interactions: Minimum item interactions\n\n    Returns:\n        Filtered interactions DataFrame\n    \"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#dataset-wrappers","title":"Dataset Wrappers","text":""},{"location":"en/api/base-dataset/#itemdataset","title":"ItemDataset","text":"<p>Dataset wrapper for item-level data, primarily used for training RQVAE.</p> <pre><code>class ItemDataset(Dataset):\n    def __init__(\n        self,\n        base_dataset: BaseRecommenderDataset,\n        split: str = \"all\",\n        return_text: bool = False\n    ):\n</code></pre> <p>Parameters: - <code>base_dataset</code>: Base dataset instance - <code>split</code>: Data split (\"all\", \"train\", \"val\", \"test\") - <code>return_text</code>: Whether to return text information</p> <p>Methods:</p>"},{"location":"en/api/base-dataset/#len","title":"len()","text":"<p>Return dataset size.</p> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of items in dataset\"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#getitemidx","title":"getitem(idx)","text":"<p>Get single data sample.</p> <pre><code>def __getitem__(self, idx: int) -&gt; Union[torch.Tensor, Dict[str, Any]]:\n    \"\"\"\n    Get single item data\n\n    Args:\n        idx: Item index\n\n    Returns:\n        If return_text=False: Item feature vector (torch.Tensor)\n        If return_text=True: Dictionary containing features and text\n    \"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#get_item_featuresitem_id","title":"get_item_features(item_id)","text":"<p>Get features by item ID.</p> <pre><code>def get_item_features(self, item_id: int) -&gt; torch.Tensor:\n    \"\"\"\n    Get features by item ID\n\n    Args:\n        item_id: Item ID\n\n    Returns:\n        Item feature vector\n    \"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#sequencedataset","title":"SequenceDataset","text":"<p>Dataset wrapper for sequence-level data, primarily used for training TIGER.</p> <pre><code>class SequenceDataset(Dataset):\n    def __init__(\n        self,\n        base_dataset: BaseRecommenderDataset,\n        split: str = \"train\",\n        semantic_encoder: Optional[torch.nn.Module] = None,\n        sequence_config: Optional[SequenceConfig] = None\n    ):\n</code></pre> <p>Parameters: - <code>base_dataset</code>: Base dataset instance - <code>split</code>: Data split (\"train\", \"val\", \"test\") - <code>semantic_encoder</code>: Semantic encoder (RQVAE) - <code>sequence_config</code>: Sequence configuration</p> <p>Methods:</p>"},{"location":"en/api/base-dataset/#len_1","title":"len()","text":"<p>Return dataset size.</p> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of user sequences in dataset\"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#getitemidx_1","title":"getitem(idx)","text":"<p>Get single sequence data.</p> <pre><code>def __getitem__(self, idx: int) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Get single user sequence data\n\n    Args:\n        idx: Sequence index\n\n    Returns:\n        Dictionary containing input and target sequences:\n        - 'input_ids': Input sequence (torch.Tensor)\n        - 'labels': Target sequence (torch.Tensor)  \n        - 'attention_mask': Attention mask (torch.Tensor)\n    \"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#build_sequences","title":"build_sequences()","text":"<p>Build user interaction sequences.</p> <pre><code>def build_sequences(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Build user interaction sequences\n\n    Returns:\n        List of sequences, each containing user ID and item ID list\n    \"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#encode_sequences_to_semantic_idssequences","title":"encode_sequences_to_semantic_ids(sequences)","text":"<p>Encode sequences to semantic IDs.</p> <pre><code>def encode_sequences_to_semantic_ids(\n    self, \n    sequences: List[Dict[str, Any]]\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Encode item sequences to semantic ID sequences\n\n    Args:\n        sequences: Original sequence list\n\n    Returns:\n        Encoded sequence list\n    \"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#data-processing-tools","title":"Data Processing Tools","text":""},{"location":"en/api/base-dataset/#train_test_splitinteractions_df-test_ratio-val_ratio","title":"train_test_split(interactions_df, test_ratio, val_ratio)","text":"<p>Split train, validation and test sets.</p> <pre><code>def train_test_split(\n    interactions_df: pd.DataFrame,\n    test_ratio: float = 0.2,\n    val_ratio: float = 0.1,\n    time_based: bool = True\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Split train, validation and test sets\n\n    Args:\n        interactions_df: Interactions DataFrame\n        test_ratio: Test set ratio\n        val_ratio: Validation set ratio\n        time_based: Whether to split based on time\n\n    Returns:\n        (train_df, val_df, test_df): Split DataFrames\n    \"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#create_item_mappingitems_df","title":"create_item_mapping(items_df)","text":"<p>Create item ID mapping.</p> <pre><code>def create_item_mapping(items_df: pd.DataFrame) -&gt; Tuple[Dict[int, int], Dict[int, int]]:\n    \"\"\"\n    Create item ID mapping\n\n    Args:\n        items_df: Items DataFrame\n\n    Returns:\n        (id_to_index, index_to_id): ID mapping dictionaries\n    \"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#normalize_featuresfeatures","title":"normalize_features(features)","text":"<p>Normalize feature vectors.</p> <pre><code>def normalize_features(features: np.ndarray, method: str = \"l2\") -&gt; np.ndarray:\n    \"\"\"\n    Normalize feature vectors\n\n    Args:\n        features: Feature matrix\n        method: Normalization method (\"l2\", \"minmax\", \"zscore\")\n\n    Returns:\n        Normalized feature matrix\n    \"\"\"\n</code></pre>"},{"location":"en/api/base-dataset/#caching-mechanism","title":"Caching Mechanism","text":""},{"location":"en/api/base-dataset/#cachemanager","title":"CacheManager","text":"<p>Manage data processing cache.</p> <pre><code>class CacheManager:\n    def __init__(self, cache_dir: str):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n    def get_cache_path(self, cache_key: str) -&gt; Path:\n        \"\"\"Get cache file path\"\"\"\n        return self.cache_dir / f\"{cache_key}.pkl\"\n\n    def exists(self, cache_key: str) -&gt; bool:\n        \"\"\"Check if cache exists\"\"\"\n        return self.get_cache_path(cache_key).exists()\n\n    def save(self, cache_key: str, data: Any) -&gt; None:\n        \"\"\"Save data to cache\"\"\"\n        cache_path = self.get_cache_path(cache_key)\n        with open(cache_path, 'wb') as f:\n            pickle.dump(data, f)\n\n    def load(self, cache_key: str) -&gt; Any:\n        \"\"\"Load data from cache\"\"\"\n        cache_path = self.get_cache_path(cache_key)\n        with open(cache_path, 'rb') as f:\n            return pickle.load(f)\n</code></pre>"},{"location":"en/api/base-dataset/#data-validation","title":"Data Validation","text":""},{"location":"en/api/base-dataset/#validate_datasetdataset","title":"validate_dataset(dataset)","text":"<p>Validate dataset integrity.</p> <pre><code>def validate_dataset(dataset: Dataset) -&gt; Dict[str, Any]:\n    \"\"\"\n    Validate dataset integrity\n\n    Args:\n        dataset: Dataset instance\n\n    Returns:\n        Validation results dictionary\n    \"\"\"\n    results = {\n        'size': len(dataset),\n        'sample_shapes': [],\n        'data_types': [],\n        'errors': []\n    }\n\n    try:\n        # Check dataset size\n        if len(dataset) == 0:\n            results['errors'].append(\"Dataset is empty\")\n\n        # Check sample shapes and types\n        for i in range(min(5, len(dataset))):\n            sample = dataset[i]\n            if isinstance(sample, torch.Tensor):\n                results['sample_shapes'].append(sample.shape)\n                results['data_types'].append(sample.dtype)\n            elif isinstance(sample, dict):\n                for key, value in sample.items():\n                    if isinstance(value, torch.Tensor):\n                        results['sample_shapes'].append((key, value.shape))\n                        results['data_types'].append((key, value.dtype))\n\n    except Exception as e:\n        results['errors'].append(f\"Validation error: {str(e)}\")\n\n    return results\n</code></pre>"},{"location":"en/api/base-dataset/#usage-examples","title":"Usage Examples","text":""},{"location":"en/api/base-dataset/#create-custom-dataset","title":"Create Custom Dataset","text":"<pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\nfrom generative_recommenders.data.configs import DatasetConfig\n\nclass MyDataset(BaseRecommenderDataset):\n    def download(self):\n        # Implement download logic\n        pass\n\n    def load_raw_data(self):\n        # Load data\n        return {\"items\": items_df, \"interactions\": interactions_df}\n\n    def preprocess_data(self, raw_data):\n        # Preprocess\n        return raw_data\n\n    def extract_items(self, processed_data):\n        return processed_data[\"items\"]\n\n    def extract_interactions(self, processed_data):\n        return processed_data[\"interactions\"]\n\n# Use dataset\nconfig = DatasetConfig(root_dir=\"data\", split=\"default\")\ndataset = MyDataset(config)\nitems_df, interactions_df = dataset.get_dataset()\n</code></pre>"},{"location":"en/api/base-dataset/#create-item-dataset","title":"Create Item Dataset","text":"<pre><code>from generative_recommenders.data.base_dataset import ItemDataset\n\n# Create item dataset\nitem_dataset = ItemDataset(\n    base_dataset=dataset,\n    split=\"train\",\n    return_text=False\n)\n\n# Use DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(item_dataset, batch_size=32, shuffle=True)\n\nfor batch in dataloader:\n    features = batch  # (batch_size, feature_dim)\n    # Training logic\n</code></pre>"},{"location":"en/api/base-dataset/#create-sequence-dataset","title":"Create Sequence Dataset","text":"<pre><code>from generative_recommenders.data.base_dataset import SequenceDataset\nfrom generative_recommenders.models.rqvae import RqVae\n\n# Load semantic encoder\nsemantic_encoder = RqVae.load_from_checkpoint(\"rqvae.ckpt\")\n\n# Create sequence dataset\nsequence_dataset = SequenceDataset(\n    base_dataset=dataset,\n    split=\"train\",\n    semantic_encoder=semantic_encoder\n)\n\n# Use DataLoader\ndataloader = DataLoader(sequence_dataset, batch_size=16, shuffle=True)\n\nfor batch in dataloader:\n    input_ids = batch['input_ids']  # (batch_size, seq_len)\n    labels = batch['labels']        # (batch_size, seq_len)\n    # Training logic\n</code></pre>"},{"location":"en/api/configs/","title":"Configuration Management API Reference","text":"<p>Detailed documentation for configuration management classes used to manage data processing and model training parameters.</p>"},{"location":"en/api/configs/#base-configuration-classes","title":"Base Configuration Classes","text":""},{"location":"en/api/configs/#datasetconfig","title":"DatasetConfig","text":"<p>Base dataset configuration class.</p> <pre><code>@dataclass\nclass DatasetConfig:\n    root_dir: str\n    split: str = \"default\"\n    force_reload: bool = False\n    text_config: Optional[TextEncodingConfig] = None\n    sequence_config: Optional[SequenceConfig] = None\n    processing_config: Optional[DataProcessingConfig] = None\n\n    def __post_init__(self):\n        \"\"\"Post-initialization processing\"\"\"\n        if self.text_config is None:\n            self.text_config = TextEncodingConfig()\n        if self.sequence_config is None:\n            self.sequence_config = SequenceConfig()\n        if self.processing_config is None:\n            self.processing_config = DataProcessingConfig()\n</code></pre> <p>Parameters: - <code>root_dir</code>: Dataset root directory - <code>split</code>: Data split identifier - <code>force_reload</code>: Whether to force reload - <code>text_config</code>: Text encoding configuration - <code>sequence_config</code>: Sequence processing configuration - <code>processing_config</code>: Data processing configuration</p>"},{"location":"en/api/configs/#text-encoding-configuration","title":"Text Encoding Configuration","text":""},{"location":"en/api/configs/#textencodingconfig","title":"TextEncodingConfig","text":"<p>Text encoding related configuration.</p> <pre><code>@dataclass\nclass TextEncodingConfig:\n    encoder_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    template: str = \"Title: {title}; Brand: {brand}; Category: {category}; Price: {price}\"\n    batch_size: int = 32\n    max_length: int = 512\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    cache_dir: str = \"cache/text_embeddings\"\n    normalize_embeddings: bool = True\n\n    def __post_init__(self):\n        \"\"\"Validate configuration parameters\"\"\"\n        if self.batch_size &lt;= 0:\n            raise ValueError(\"batch_size must be positive\")\n        if self.max_length &lt;= 0:\n            raise ValueError(\"max_length must be positive\")\n</code></pre> <p>Parameters: - <code>encoder_model</code>: Text encoder model name - <code>template</code>: Text template format - <code>batch_size</code>: Batch processing size - <code>max_length</code>: Maximum text length - <code>device</code>: Computing device - <code>cache_dir</code>: Cache directory - <code>normalize_embeddings</code>: Whether to normalize embeddings</p> <p>Methods:</p>"},{"location":"en/api/configs/#get_cache_keysplit-model_name","title":"get_cache_key(split, model_name)","text":"<p>Generate cache key.</p> <pre><code>def get_cache_key(self, split: str, model_name: str = None) -&gt; str:\n    \"\"\"\n    Generate cache key\n\n    Args:\n        split: Data split\n        model_name: Model name\n\n    Returns:\n        Cache key string\n    \"\"\"\n    if model_name is None:\n        model_name = self.encoder_model\n    return f\"{model_name}_{split}_{hash(self.template)}\"\n</code></pre>"},{"location":"en/api/configs/#format_textitem_data","title":"format_text(item_data)","text":"<p>Format item text.</p> <pre><code>def format_text(self, item_data: Dict[str, Any]) -&gt; str:\n    \"\"\"\n    Format item text using template\n\n    Args:\n        item_data: Item data dictionary\n\n    Returns:\n        Formatted text\n    \"\"\"\n    try:\n        return self.template.format(**item_data)\n    except KeyError as e:\n        # Handle missing fields\n        available_fields = set(item_data.keys())\n        template_fields = set(re.findall(r'\\{(\\w+)\\}', self.template))\n        missing_fields = template_fields - available_fields\n\n        # Fill missing fields with default values\n        filled_data = item_data.copy()\n        for field in missing_fields:\n            filled_data[field] = \"Unknown\"\n\n        return self.template.format(**filled_data)\n</code></pre>"},{"location":"en/api/configs/#sequence-processing-configuration","title":"Sequence Processing Configuration","text":""},{"location":"en/api/configs/#sequenceconfig","title":"SequenceConfig","text":"<p>Sequence processing related configuration.</p> <pre><code>@dataclass\nclass SequenceConfig:\n    max_seq_length: int = 50\n    min_seq_length: int = 3\n    padding_token: int = 0\n    truncate_strategy: str = \"recent\"  # \"recent\", \"random\", \"oldest\"\n    sequence_stride: int = 1\n    target_offset: int = 1\n    include_timestamps: bool = False\n    time_encoding_dim: int = 32\n\n    def __post_init__(self):\n        \"\"\"Validate configuration parameters\"\"\"\n        if self.max_seq_length &lt;= self.min_seq_length:\n            raise ValueError(\"max_seq_length must be greater than min_seq_length\")\n        if self.truncate_strategy not in [\"recent\", \"random\", \"oldest\"]:\n            raise ValueError(\"Invalid truncate_strategy\")\n        if self.target_offset &lt;= 0:\n            raise ValueError(\"target_offset must be positive\")\n</code></pre> <p>Parameters: - <code>max_seq_length</code>: Maximum sequence length - <code>min_seq_length</code>: Minimum sequence length - <code>padding_token</code>: Padding token - <code>truncate_strategy</code>: Truncation strategy - <code>sequence_stride</code>: Sequence stride - <code>target_offset</code>: Target offset - <code>include_timestamps</code>: Whether to include timestamps - <code>time_encoding_dim</code>: Time encoding dimension</p> <p>Methods:</p>"},{"location":"en/api/configs/#truncate_sequencesequence-strategy","title":"truncate_sequence(sequence, strategy)","text":"<p>Truncate sequence.</p> <pre><code>def truncate_sequence(\n    self, \n    sequence: List[Any], \n    strategy: str = None\n) -&gt; List[Any]:\n    \"\"\"\n    Truncate sequence according to strategy\n\n    Args:\n        sequence: Input sequence\n        strategy: Truncation strategy, uses config strategy if None\n\n    Returns:\n        Truncated sequence\n    \"\"\"\n    if len(sequence) &lt;= self.max_seq_length:\n        return sequence\n\n    strategy = strategy or self.truncate_strategy\n\n    if strategy == \"recent\":\n        return sequence[-self.max_seq_length:]\n    elif strategy == \"oldest\":\n        return sequence[:self.max_seq_length]\n    elif strategy == \"random\":\n        start_idx = random.randint(0, len(sequence) - self.max_seq_length)\n        return sequence[start_idx:start_idx + self.max_seq_length]\n    else:\n        raise ValueError(f\"Unknown truncate strategy: {strategy}\")\n</code></pre>"},{"location":"en/api/configs/#pad_sequencesequence","title":"pad_sequence(sequence)","text":"<p>Pad sequence.</p> <pre><code>def pad_sequence(self, sequence: List[Any]) -&gt; List[Any]:\n    \"\"\"\n    Pad sequence to maximum length\n\n    Args:\n        sequence: Input sequence\n\n    Returns:\n        Padded sequence\n    \"\"\"\n    if len(sequence) &gt;= self.max_seq_length:\n        return sequence[:self.max_seq_length]\n\n    pad_length = self.max_seq_length - len(sequence)\n    return sequence + [self.padding_token] * pad_length\n</code></pre>"},{"location":"en/api/configs/#data-processing-configuration","title":"Data Processing Configuration","text":""},{"location":"en/api/configs/#dataprocessingconfig","title":"DataProcessingConfig","text":"<p>Data processing related configuration.</p> <pre><code>@dataclass\nclass DataProcessingConfig:\n    min_user_interactions: int = 5\n    min_item_interactions: int = 5\n    remove_duplicates: bool = True\n    normalize_ratings: bool = False\n    rating_scale: Tuple[float, float] = (1.0, 5.0)\n    train_ratio: float = 0.7\n    val_ratio: float = 0.15\n    test_ratio: float = 0.15\n    random_seed: int = 42\n\n    def __post_init__(self):\n        \"\"\"Validate configuration parameters\"\"\"\n        if abs(self.train_ratio + self.val_ratio + self.test_ratio - 1.0) &gt; 1e-6:\n            raise ValueError(\"train_ratio + val_ratio + test_ratio must equal 1.0\")\n        if any(ratio &lt;= 0 for ratio in [self.train_ratio, self.val_ratio, self.test_ratio]):\n            raise ValueError(\"All ratios must be positive\")\n        if self.min_user_interactions &lt;= 0 or self.min_item_interactions &lt;= 0:\n            raise ValueError(\"Minimum interactions must be positive\")\n</code></pre> <p>Parameters: - <code>min_user_interactions</code>: Minimum user interactions - <code>min_item_interactions</code>: Minimum item interactions - <code>remove_duplicates</code>: Whether to remove duplicate interactions - <code>normalize_ratings</code>: Whether to normalize ratings - <code>rating_scale</code>: Rating range - <code>train_ratio</code>: Training set ratio - <code>val_ratio</code>: Validation set ratio - <code>test_ratio</code>: Test set ratio - <code>random_seed</code>: Random seed</p> <p>Methods:</p>"},{"location":"en/api/configs/#get_split_indicestotal_size","title":"get_split_indices(total_size)","text":"<p>Get data split indices.</p> <pre><code>def get_split_indices(self, total_size: int) -&gt; Tuple[List[int], List[int], List[int]]:\n    \"\"\"\n    Get data split indices according to configured ratios\n\n    Args:\n        total_size: Total data size\n\n    Returns:\n        (train_indices, val_indices, test_indices): Split index lists\n    \"\"\"\n    indices = list(range(total_size))\n    random.Random(self.random_seed).shuffle(indices)\n\n    train_size = int(total_size * self.train_ratio)\n    val_size = int(total_size * self.val_ratio)\n\n    train_indices = indices[:train_size]\n    val_indices = indices[train_size:train_size + val_size]\n    test_indices = indices[train_size + val_size:]\n\n    return train_indices, val_indices, test_indices\n</code></pre>"},{"location":"en/api/configs/#normalize_ratingrating","title":"normalize_rating(rating)","text":"<p>Normalize rating.</p> <pre><code>def normalize_rating(self, rating: float) -&gt; float:\n    \"\"\"\n    Normalize rating to [0, 1] range\n\n    Args:\n        rating: Original rating\n\n    Returns:\n        Normalized rating\n    \"\"\"\n    if not self.normalize_ratings:\n        return rating\n\n    min_rating, max_rating = self.rating_scale\n    return (rating - min_rating) / (max_rating - min_rating)\n</code></pre>"},{"location":"en/api/configs/#specific-dataset-configurations","title":"Specific Dataset Configurations","text":""},{"location":"en/api/configs/#p5amazonconfig","title":"P5AmazonConfig","text":"<p>P5 Amazon dataset specific configuration.</p> <pre><code>@dataclass\nclass P5AmazonConfig(DatasetConfig):\n    category: str = \"beauty\"\n    min_rating: float = 4.0\n    include_price: bool = True\n    include_brand: bool = True\n    download_url: str = \"https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/\"\n\n    def __post_init__(self):\n        super().__post_init__()\n\n        # Set specific text template\n        if self.include_price and self.include_brand:\n            template = \"Title: {title}; Brand: {brand}; Category: {category}; Price: {price}\"\n        elif self.include_brand:\n            template = \"Title: {title}; Brand: {brand}; Category: {category}\"\n        else:\n            template = \"Title: {title}; Category: {category}\"\n\n        self.text_config.template = template\n\n    def get_category_url(self) -&gt; str:\n        \"\"\"Get download URL for specific category\"\"\"\n        return f\"{self.download_url}{self.category}.json.gz\"\n</code></pre> <p>Additional Parameters: - <code>category</code>: Product category - <code>min_rating</code>: Minimum rating threshold - <code>include_price</code>: Whether to include price information - <code>include_brand</code>: Whether to include brand information - <code>download_url</code>: Download base URL</p>"},{"location":"en/api/configs/#configuration-validation-and-tools","title":"Configuration Validation and Tools","text":""},{"location":"en/api/configs/#validate_configconfig","title":"validate_config(config)","text":"<p>Validate configuration integrity.</p> <pre><code>def validate_config(config: DatasetConfig) -&gt; List[str]:\n    \"\"\"\n    Validate configuration validity\n\n    Args:\n        config: Dataset configuration\n\n    Returns:\n        List of error messages, empty list means valid configuration\n    \"\"\"\n    errors = []\n\n    # Check root directory\n    if not config.root_dir:\n        errors.append(\"root_dir cannot be empty\")\n\n    # Check text configuration\n    if config.text_config:\n        if not config.text_config.encoder_model:\n            errors.append(\"encoder_model cannot be empty\")\n        if config.text_config.batch_size &lt;= 0:\n            errors.append(\"batch_size must be positive\")\n\n    # Check sequence configuration\n    if config.sequence_config:\n        if config.sequence_config.max_seq_length &lt;= config.sequence_config.min_seq_length:\n            errors.append(\"max_seq_length must be greater than min_seq_length\")\n\n    # Check processing configuration\n    if config.processing_config:\n        ratios_sum = (\n            config.processing_config.train_ratio + \n            config.processing_config.val_ratio + \n            config.processing_config.test_ratio\n        )\n        if abs(ratios_sum - 1.0) &gt; 1e-6:\n            errors.append(\"Split ratios must sum to 1.0\")\n\n    return errors\n</code></pre>"},{"location":"en/api/configs/#load_config_from_fileconfig_path","title":"load_config_from_file(config_path)","text":"<p>Load configuration from file.</p> <pre><code>def load_config_from_file(config_path: str) -&gt; DatasetConfig:\n    \"\"\"\n    Load configuration from YAML or JSON file\n\n    Args:\n        config_path: Configuration file path\n\n    Returns:\n        Dataset configuration object\n    \"\"\"\n    config_path = Path(config_path)\n\n    if config_path.suffix.lower() in ['.yaml', '.yml']:\n        import yaml\n        with open(config_path, 'r') as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.suffix.lower() == '.json':\n        with open(config_path, 'r') as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unsupported config file format: {config_path.suffix}\")\n\n    # Create appropriate object based on configuration type\n    config_type = config_dict.pop('config_type', 'DatasetConfig')\n\n    if config_type == 'P5AmazonConfig':\n        return P5AmazonConfig(**config_dict)\n    else:\n        return DatasetConfig(**config_dict)\n</code></pre>"},{"location":"en/api/configs/#save_config_to_fileconfig-config_path","title":"save_config_to_file(config, config_path)","text":"<p>Save configuration to file.</p> <pre><code>def save_config_to_file(config: DatasetConfig, config_path: str) -&gt; None:\n    \"\"\"\n    Save configuration to YAML or JSON file\n\n    Args:\n        config: Dataset configuration object\n        config_path: Configuration file path\n    \"\"\"\n    config_path = Path(config_path)\n    config_dict = asdict(config)\n\n    # Add configuration type information\n    config_dict['config_type'] = config.__class__.__name__\n\n    if config_path.suffix.lower() in ['.yaml', '.yml']:\n        import yaml\n        with open(config_path, 'w') as f:\n            yaml.dump(config_dict, f, default_flow_style=False)\n    elif config_path.suffix.lower() == '.json':\n        with open(config_path, 'w') as f:\n            json.dump(config_dict, f, indent=2)\n    else:\n        raise ValueError(f\"Unsupported config file format: {config_path.suffix}\")\n</code></pre>"},{"location":"en/api/configs/#usage-examples","title":"Usage Examples","text":""},{"location":"en/api/configs/#basic-configuration-creation","title":"Basic Configuration Creation","text":"<pre><code>from generative_recommenders.data.configs import (\n    DatasetConfig, TextEncodingConfig, SequenceConfig, DataProcessingConfig\n)\n\n# Create basic configuration\nconfig = DatasetConfig(\n    root_dir=\"dataset/amazon\",\n    split=\"beauty\",\n    text_config=TextEncodingConfig(\n        encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n        batch_size=64\n    ),\n    sequence_config=SequenceConfig(\n        max_seq_length=100,\n        min_seq_length=5\n    ),\n    processing_config=DataProcessingConfig(\n        min_user_interactions=10,\n        train_ratio=0.8,\n        val_ratio=0.1,\n        test_ratio=0.1\n    )\n)\n</code></pre>"},{"location":"en/api/configs/#p5-amazon-configuration","title":"P5 Amazon Configuration","text":"<pre><code>from generative_recommenders.data.configs import P5AmazonConfig\n\n# Create P5 Amazon configuration\nconfig = P5AmazonConfig(\n    root_dir=\"dataset/amazon\",\n    category=\"beauty\",\n    min_rating=4.0,\n    include_price=True,\n    include_brand=True\n)\n</code></pre>"},{"location":"en/api/configs/#configuration-file-operations","title":"Configuration File Operations","text":"<pre><code># Save configuration to file\nsave_config_to_file(config, \"config/dataset_config.yaml\")\n\n# Load configuration from file\nloaded_config = load_config_from_file(\"config/dataset_config.yaml\")\n\n# Validate configuration\nerrors = validate_config(loaded_config)\nif errors:\n    print(\"Configuration errors:\")\n    for error in errors:\n        print(f\"  - {error}\")\nelse:\n    print(\"Configuration is valid\")\n</code></pre>"},{"location":"en/api/dataset-factory/","title":"Dataset Factory API Reference","text":"<p>Detailed documentation for the dataset factory pattern used to uniformly manage and create different types of datasets.</p>"},{"location":"en/api/dataset-factory/#core-factory-class","title":"Core Factory Class","text":""},{"location":"en/api/dataset-factory/#datasetfactory","title":"DatasetFactory","text":"<p>Core class for dataset factory, managing dataset registration and creation.</p> <pre><code>class DatasetFactory:\n    \"\"\"Dataset factory class\"\"\"\n\n    _registered_datasets = {}\n\n    @classmethod\n    def register_dataset(\n        cls,\n        name: str,\n        base_class: Type[BaseRecommenderDataset],\n        item_class: Type[ItemDataset],\n        sequence_class: Type[SequenceDataset]\n    ) -&gt; None:\n        \"\"\"\n        Register dataset classes\n\n        Args:\n            name: Dataset name\n            base_class: Base dataset class\n            item_class: Item dataset class\n            sequence_class: Sequence dataset class\n        \"\"\"\n        cls._registered_datasets[name] = {\n            'base': base_class,\n            'item': item_class,\n            'sequence': sequence_class\n        }\n        print(f\"Registered dataset: {name}\")\n</code></pre> <p>Class Methods:</p>"},{"location":"en/api/dataset-factory/#list_datasets","title":"list_datasets()","text":"<p>List all registered datasets.</p> <pre><code>@classmethod\ndef list_datasets(cls) -&gt; List[str]:\n    \"\"\"\n    List all registered datasets\n\n    Returns:\n        Dataset name list\n    \"\"\"\n    return list(cls._registered_datasets.keys())\n</code></pre>"},{"location":"en/api/dataset-factory/#get_dataset_infoname","title":"get_dataset_info(name)","text":"<p>Get dataset information.</p> <pre><code>@classmethod\ndef get_dataset_info(cls, name: str) -&gt; Dict[str, Type]:\n    \"\"\"\n    Get class information for specified dataset\n\n    Args:\n        name: Dataset name\n\n    Returns:\n        Dictionary containing dataset classes\n\n    Raises:\n        ValueError: If dataset is not registered\n    \"\"\"\n    if name not in cls._registered_datasets:\n        available = \", \".join(cls.list_datasets())\n        raise ValueError(f\"Dataset '{name}' not registered. Available: {available}\")\n\n    return cls._registered_datasets[name]\n</code></pre>"},{"location":"en/api/dataset-factory/#create_base_datasetname-kwargs","title":"create_base_dataset(name, **kwargs)","text":"<p>Create base dataset instance.</p> <pre><code>@classmethod\ndef create_base_dataset(cls, name: str, **kwargs) -&gt; BaseRecommenderDataset:\n    \"\"\"\n    Create base dataset instance\n\n    Args:\n        name: Dataset name\n        **kwargs: Parameters passed to dataset constructor\n\n    Returns:\n        Base dataset instance\n    \"\"\"\n    dataset_info = cls.get_dataset_info(name)\n    base_class = dataset_info['base']\n\n    # Create configuration object\n    if 'config' not in kwargs:\n        config_class = cls._get_config_class(base_class)\n        kwargs['config'] = config_class(**kwargs)\n\n    return base_class(kwargs['config'])\n</code></pre>"},{"location":"en/api/dataset-factory/#create_item_datasetname-kwargs","title":"create_item_dataset(name, **kwargs)","text":"<p>Create item dataset instance.</p> <pre><code>@classmethod\ndef create_item_dataset(cls, name: str, **kwargs) -&gt; ItemDataset:\n    \"\"\"\n    Create item dataset instance\n\n    Args:\n        name: Dataset name\n        **kwargs: Parameters passed to dataset constructor\n\n    Returns:\n        Item dataset instance\n    \"\"\"\n    dataset_info = cls.get_dataset_info(name)\n    item_class = dataset_info['item']\n\n    return item_class(**kwargs)\n</code></pre>"},{"location":"en/api/dataset-factory/#create_sequence_datasetname-kwargs","title":"create_sequence_dataset(name, **kwargs)","text":"<p>Create sequence dataset instance.</p> <pre><code>@classmethod\ndef create_sequence_dataset(cls, name: str, **kwargs) -&gt; SequenceDataset:\n    \"\"\"\n    Create sequence dataset instance\n\n    Args:\n        name: Dataset name\n        **kwargs: Parameters passed to dataset constructor\n\n    Returns:\n        Sequence dataset instance\n    \"\"\"\n    dataset_info = cls.get_dataset_info(name)\n    sequence_class = dataset_info['sequence']\n\n    return sequence_class(**kwargs)\n</code></pre>"},{"location":"en/api/dataset-factory/#_get_config_classbase_class","title":"_get_config_class(base_class)","text":"<p>Get configuration class corresponding to dataset.</p> <pre><code>@classmethod\ndef _get_config_class(cls, base_class: Type[BaseRecommenderDataset]) -&gt; Type[DatasetConfig]:\n    \"\"\"\n    Get configuration class corresponding to base dataset class\n\n    Args:\n        base_class: Base dataset class\n\n    Returns:\n        Configuration class\n    \"\"\"\n    # Infer configuration class through class name or annotations\n    if hasattr(base_class, '_config_class'):\n        return base_class._config_class\n\n    # Default configuration class mapping\n    config_mapping = {\n        'P5AmazonDataset': P5AmazonConfig,\n        'MovieLensDataset': MovieLensConfig,\n        # Can continue adding other mappings\n    }\n\n    class_name = base_class.__name__\n    return config_mapping.get(class_name, DatasetConfig)\n</code></pre>"},{"location":"en/api/dataset-factory/#dataset-registry","title":"Dataset Registry","text":""},{"location":"en/api/dataset-factory/#datasetregistry","title":"DatasetRegistry","text":"<p>Dataset registration manager.</p> <pre><code>class DatasetRegistry:\n    \"\"\"Dataset registration manager\"\"\"\n\n    def __init__(self):\n        self.datasets = {}\n        self.auto_register_builtin_datasets()\n\n    def register(\n        self,\n        name: str,\n        base_class: Type[BaseRecommenderDataset],\n        item_class: Type[ItemDataset] = None,\n        sequence_class: Type[SequenceDataset] = None,\n        config_class: Type[DatasetConfig] = None\n    ) -&gt; None:\n        \"\"\"\n        Register dataset\n\n        Args:\n            name: Dataset name\n            base_class: Base dataset class\n            item_class: Item dataset class\n            sequence_class: Sequence dataset class\n            config_class: Configuration class\n        \"\"\"\n        # Auto-generate wrapper classes if not provided\n        if item_class is None:\n            item_class = self._create_item_wrapper(name, base_class)\n\n        if sequence_class is None:\n            sequence_class = self._create_sequence_wrapper(name, base_class)\n\n        # Set configuration class\n        if config_class:\n            base_class._config_class = config_class\n\n        self.datasets[name] = {\n            'base': base_class,\n            'item': item_class,\n            'sequence': sequence_class,\n            'config': config_class or DatasetConfig\n        }\n\n    def _create_item_wrapper(\n        self, \n        name: str, \n        base_class: Type[BaseRecommenderDataset]\n    ) -&gt; Type[ItemDataset]:\n        \"\"\"Dynamically create item dataset wrapper class\"\"\"\n\n        class DynamicItemDataset(ItemDataset):\n            def __init__(self, **kwargs):\n                # Create base dataset\n                config_class = getattr(base_class, '_config_class', DatasetConfig)\n                config = config_class(**kwargs)\n                base_dataset = base_class(config)\n\n                # Initialize item dataset\n                super().__init__(\n                    base_dataset=base_dataset,\n                    split=kwargs.get('train_test_split', 'all'),\n                    return_text=kwargs.get('return_text', False)\n                )\n\n        DynamicItemDataset.__name__ = f\"{name.title()}ItemDataset\"\n        return DynamicItemDataset\n\n    def _create_sequence_wrapper(\n        self, \n        name: str, \n        base_class: Type[BaseRecommenderDataset]\n    ) -&gt; Type[SequenceDataset]:\n        \"\"\"Dynamically create sequence dataset wrapper class\"\"\"\n\n        class DynamicSequenceDataset(SequenceDataset):\n            def __init__(self, **kwargs):\n                # Create base dataset\n                config_class = getattr(base_class, '_config_class', DatasetConfig)\n                config = config_class(**kwargs)\n                base_dataset = base_class(config)\n\n                # Load semantic encoder\n                semantic_encoder = None\n                if 'pretrained_rqvae_path' in kwargs:\n                    from generative_recommenders.models.rqvae import RqVae\n                    semantic_encoder = RqVae.load_from_checkpoint(kwargs['pretrained_rqvae_path'])\n                    semantic_encoder.eval()\n\n                # Initialize sequence dataset\n                super().__init__(\n                    base_dataset=base_dataset,\n                    split=kwargs.get('train_test_split', 'train'),\n                    semantic_encoder=semantic_encoder\n                )\n\n        DynamicSequenceDataset.__name__ = f\"{name.title()}SequenceDataset\"\n        return DynamicSequenceDataset\n\n    def auto_register_builtin_datasets(self) -&gt; None:\n        \"\"\"Auto-register built-in datasets\"\"\"\n        try:\n            from generative_recommenders.data.p5_amazon import (\n                P5AmazonDataset, P5AmazonItemDataset, P5AmazonSequenceDataset\n            )\n            from generative_recommenders.data.configs import P5AmazonConfig\n\n            self.register(\n                name=\"p5_amazon\",\n                base_class=P5AmazonDataset,\n                item_class=P5AmazonItemDataset,\n                sequence_class=P5AmazonSequenceDataset,\n                config_class=P5AmazonConfig\n            )\n        except ImportError:\n            pass\n\n        # Can continue adding other built-in datasets\n</code></pre>"},{"location":"en/api/dataset-factory/#configuration-builder","title":"Configuration Builder","text":""},{"location":"en/api/dataset-factory/#configbuilder","title":"ConfigBuilder","text":"<p>Configuration object builder.</p> <pre><code>class ConfigBuilder:\n    \"\"\"Configuration builder\"\"\"\n\n    def __init__(self, config_class: Type[DatasetConfig]):\n        self.config_class = config_class\n        self.params = {}\n\n    def set_root_dir(self, root_dir: str) -&gt; 'ConfigBuilder':\n        \"\"\"Set root directory\"\"\"\n        self.params['root_dir'] = root_dir\n        return self\n\n    def set_split(self, split: str) -&gt; 'ConfigBuilder':\n        \"\"\"Set data split\"\"\"\n        self.params['split'] = split\n        return self\n\n    def set_text_config(self, **kwargs) -&gt; 'ConfigBuilder':\n        \"\"\"Set text configuration\"\"\"\n        self.params['text_config'] = TextEncodingConfig(**kwargs)\n        return self\n\n    def set_sequence_config(self, **kwargs) -&gt; 'ConfigBuilder':\n        \"\"\"Set sequence configuration\"\"\"\n        self.params['sequence_config'] = SequenceConfig(**kwargs)\n        return self\n\n    def set_processing_config(self, **kwargs) -&gt; 'ConfigBuilder':\n        \"\"\"Set processing configuration\"\"\"\n        self.params['processing_config'] = DataProcessingConfig(**kwargs)\n        return self\n\n    def build(self) -&gt; DatasetConfig:\n        \"\"\"Build configuration object\"\"\"\n        return self.config_class(**self.params)\n</code></pre>"},{"location":"en/api/dataset-factory/#dataset-manager","title":"Dataset Manager","text":""},{"location":"en/api/dataset-factory/#datasetmanager","title":"DatasetManager","text":"<p>Dataset lifecycle manager.</p> <pre><code>class DatasetManager:\n    \"\"\"Dataset manager\"\"\"\n\n    def __init__(self):\n        self.registry = DatasetRegistry()\n        self.cache = {}\n\n    def create_dataset(\n        self,\n        name: str,\n        dataset_type: str = \"item\",\n        cache_key: str = None,\n        **kwargs\n    ) -&gt; Union[ItemDataset, SequenceDataset]:\n        \"\"\"\n        Create dataset instance\n\n        Args:\n            name: Dataset name\n            dataset_type: Dataset type (\"item\" or \"sequence\")\n            cache_key: Cache key\n            **kwargs: Dataset parameters\n\n        Returns:\n            Dataset instance\n        \"\"\"\n        # Check cache\n        if cache_key and cache_key in self.cache:\n            return self.cache[cache_key]\n\n        # Get dataset information\n        if name not in self.registry.datasets:\n            raise ValueError(f\"Dataset '{name}' not registered\")\n\n        dataset_info = self.registry.datasets[name]\n\n        # Create dataset\n        if dataset_type == \"item\":\n            dataset = dataset_info['item'](**kwargs)\n        elif dataset_type == \"sequence\":\n            dataset = dataset_info['sequence'](**kwargs)\n        else:\n            raise ValueError(f\"Invalid dataset_type: {dataset_type}\")\n\n        # Cache result\n        if cache_key:\n            self.cache[cache_key] = dataset\n\n        return dataset\n\n    def get_dataset_config(self, name: str, **kwargs) -&gt; DatasetConfig:\n        \"\"\"\n        Get dataset configuration\n\n        Args:\n            name: Dataset name\n            **kwargs: Configuration parameters\n\n        Returns:\n            Configuration object\n        \"\"\"\n        if name not in self.registry.datasets:\n            raise ValueError(f\"Dataset '{name}' not registered\")\n\n        config_class = self.registry.datasets[name]['config']\n        return config_class(**kwargs)\n\n    def list_datasets(self) -&gt; List[str]:\n        \"\"\"List all available datasets\"\"\"\n        return list(self.registry.datasets.keys())\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cache\"\"\"\n        self.cache.clear()\n</code></pre>"},{"location":"en/api/dataset-factory/#utility-functions","title":"Utility Functions","text":""},{"location":"en/api/dataset-factory/#register_dataset_from_modulemodule_path","title":"register_dataset_from_module(module_path)","text":"<p>Register dataset from module.</p> <pre><code>def register_dataset_from_module(module_path: str) -&gt; None:\n    \"\"\"\n    Auto-register dataset from module\n\n    Args:\n        module_path: Module path, e.g., \"my_package.my_dataset\"\n    \"\"\"\n    import importlib\n\n    module = importlib.import_module(module_path)\n\n    # Find dataset classes\n    base_classes = []\n    item_classes = []\n    sequence_classes = []\n\n    for name in dir(module):\n        obj = getattr(module, name)\n        if isinstance(obj, type):\n            if issubclass(obj, BaseRecommenderDataset) and obj != BaseRecommenderDataset:\n                base_classes.append(obj)\n            elif issubclass(obj, ItemDataset) and obj != ItemDataset:\n                item_classes.append(obj)\n            elif issubclass(obj, SequenceDataset) and obj != SequenceDataset:\n                sequence_classes.append(obj)\n\n    # Auto-match and register\n    for base_class in base_classes:\n        dataset_name = base_class.__name__.lower().replace('dataset', '')\n\n        # Find corresponding wrapper classes\n        item_class = None\n        sequence_class = None\n\n        for cls in item_classes:\n            if dataset_name in cls.__name__.lower():\n                item_class = cls\n                break\n\n        for cls in sequence_classes:\n            if dataset_name in cls.__name__.lower():\n                sequence_class = cls\n                break\n\n        # Register dataset\n        if item_class or sequence_class:\n            DatasetFactory.register_dataset(\n                name=dataset_name,\n                base_class=base_class,\n                item_class=item_class,\n                sequence_class=sequence_class\n            )\n</code></pre>"},{"location":"en/api/dataset-factory/#create_dataset_from_configconfig_path","title":"create_dataset_from_config(config_path)","text":"<p>Create dataset from configuration file.</p> <pre><code>def create_dataset_from_config(config_path: str) -&gt; Union[ItemDataset, SequenceDataset]:\n    \"\"\"\n    Create dataset from configuration file\n\n    Args:\n        config_path: Configuration file path\n\n    Returns:\n        Dataset instance\n    \"\"\"\n    import yaml\n\n    with open(config_path, 'r') as f:\n        config_data = yaml.safe_load(f)\n\n    # Extract dataset information\n    dataset_name = config_data['dataset']['name']\n    dataset_type = config_data['dataset']['type']\n    dataset_params = config_data['dataset'].get('params', {})\n\n    # Create dataset\n    manager = DatasetManager()\n    return manager.create_dataset(\n        name=dataset_name,\n        dataset_type=dataset_type,\n        **dataset_params\n    )\n</code></pre>"},{"location":"en/api/dataset-factory/#usage-examples","title":"Usage Examples","text":""},{"location":"en/api/dataset-factory/#register-custom-dataset","title":"Register Custom Dataset","text":"<pre><code>from generative_recommenders.data.dataset_factory import DatasetFactory\nfrom my_package.my_dataset import MyDataset, MyItemDataset, MySequenceDataset\n\n# Method 1: Manual registration\nDatasetFactory.register_dataset(\n    name=\"my_dataset\",\n    base_class=MyDataset,\n    item_class=MyItemDataset,\n    sequence_class=MySequenceDataset\n)\n\n# Method 2: Using registry\nregistry = DatasetRegistry()\nregistry.register(\n    name=\"my_dataset\",\n    base_class=MyDataset,\n    item_class=MyItemDataset,\n    sequence_class=MySequenceDataset\n)\n</code></pre>"},{"location":"en/api/dataset-factory/#create-dataset-instances","title":"Create Dataset Instances","text":"<pre><code># Using factory methods\nitem_dataset = DatasetFactory.create_item_dataset(\n    \"p5_amazon\",\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n\nsequence_dataset = DatasetFactory.create_sequence_dataset(\n    \"p5_amazon\",\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\",\n    pretrained_rqvae_path=\"checkpoints/rqvae.ckpt\"\n)\n</code></pre>"},{"location":"en/api/dataset-factory/#using-dataset-manager","title":"Using Dataset Manager","text":"<pre><code># Create manager\nmanager = DatasetManager()\n\n# List available datasets\nprint(\"Available datasets:\", manager.list_datasets())\n\n# Create dataset\ndataset = manager.create_dataset(\n    name=\"p5_amazon\",\n    dataset_type=\"item\",\n    cache_key=\"amazon_beauty_train\",\n    root=\"dataset/amazon\",\n    split=\"beauty\"\n)\n</code></pre>"},{"location":"en/api/dataset-factory/#configuration-builder_1","title":"Configuration Builder","text":"<pre><code># Use builder to create configuration\nconfig = (ConfigBuilder(P5AmazonConfig)\n    .set_root_dir(\"dataset/amazon\")\n    .set_split(\"beauty\")\n    .set_text_config(\n        encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n        batch_size=64\n    )\n    .set_sequence_config(\n        max_seq_length=100,\n        min_seq_length=5\n    )\n    .build())\n\n# Use configuration to create dataset\nbase_dataset = P5AmazonDataset(config)\n</code></pre>"},{"location":"en/api/dataset-factory/#create-from-configuration-file","title":"Create from Configuration File","text":"<pre><code># dataset_config.yaml\ndataset:\n  name: p5_amazon\n  type: item\n  params:\n    root: \"dataset/amazon\"\n    split: \"beauty\"\n    train_test_split: \"train\"\n    encoder_model_name: \"sentence-transformers/all-MiniLM-L6-v2\"\n</code></pre> <pre><code># Load from configuration file\ndataset = create_dataset_from_config(\"dataset_config.yaml\")\n</code></pre>"},{"location":"en/api/datasets/","title":"Dataset API Reference","text":"<p>Detailed API documentation for the GenerativeRecommenders dataset module.</p>"},{"location":"en/api/datasets/#base-dataset-classes","title":"Base Dataset Classes","text":""},{"location":"en/api/datasets/#baserecommenderdataset","title":"BaseRecommenderDataset","text":"<p>Abstract base class for all recommender system datasets.</p> <pre><code>class BaseRecommenderDataset:\n    \"\"\"Base class for recommender system datasets\"\"\"\n\n    def __init__(self, config: DatasetConfig):\n        self.config = config\n        self.root_dir = Path(config.root_dir)\n        self._items_df = None\n        self._interactions_df = None\n\n    @abstractmethod\n    def download(self) -&gt; None:\n        \"\"\"Download dataset\"\"\"\n        pass\n\n    @abstractmethod\n    def load_raw_data(self) -&gt; Dict[str, pd.DataFrame]:\n        \"\"\"Load raw data\"\"\"\n        pass\n\n    @abstractmethod\n    def preprocess_data(self, raw_data: Dict[str, pd.DataFrame]) -&gt; Dict[str, pd.DataFrame]:\n        \"\"\"Preprocess data\"\"\"\n        pass\n</code></pre> <p>Main Methods:</p>"},{"location":"en/api/datasets/#load_dataset","title":"load_dataset()","text":"<p>Load the dataset.</p> <pre><code>def load_dataset(self, force_reload: bool = False) -&gt; None:\n    \"\"\"\n    Load dataset\n\n    Args:\n        force_reload: Whether to force reload\n    \"\"\"\n</code></pre>"},{"location":"en/api/datasets/#get_items","title":"get_items()","text":"<p>Get item data.</p> <pre><code>def get_items(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Get item data\n\n    Returns:\n        Items DataFrame\n    \"\"\"\n</code></pre>"},{"location":"en/api/datasets/#get_interactions","title":"get_interactions()","text":"<p>Get interaction data.</p> <pre><code>def get_interactions(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Get user-item interaction data\n\n    Returns:\n        Interactions DataFrame\n    \"\"\"\n</code></pre>"},{"location":"en/api/datasets/#item-dataset-class","title":"Item Dataset Class","text":""},{"location":"en/api/datasets/#itemdataset","title":"ItemDataset","text":"<p>Dataset class for item encoding and feature learning.</p> <pre><code>class ItemDataset(Dataset):\n    \"\"\"Item dataset class\"\"\"\n\n    def __init__(\n        self,\n        base_dataset: BaseRecommenderDataset,\n        split: str = \"all\",\n        return_text: bool = False\n    ):\n        self.base_dataset = base_dataset\n        self.split = split\n        self.return_text = return_text\n</code></pre> <p>Parameters: - <code>base_dataset</code>: Base dataset instance - <code>split</code>: Data split (\"train\", \"val\", \"test\", \"all\") - <code>return_text</code>: Whether to return text features</p> <p>Methods:</p>"},{"location":"en/api/datasets/#getitemidx","title":"getitem(idx)","text":"<p>Get data item.</p> <pre><code>def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get data item at specified index\n\n    Args:\n        idx: Data index\n\n    Returns:\n        Dictionary containing item information\n    \"\"\"\n</code></pre>"},{"location":"en/api/datasets/#sequence-dataset-class","title":"Sequence Dataset Class","text":""},{"location":"en/api/datasets/#sequencedataset","title":"SequenceDataset","text":"<p>Dataset class for sequence generation training.</p> <pre><code>class SequenceDataset(Dataset):\n    \"\"\"Sequence dataset class\"\"\"\n\n    def __init__(\n        self,\n        base_dataset: BaseRecommenderDataset,\n        split: str = \"train\",\n        semantic_encoder: Optional[nn.Module] = None\n    ):\n        self.base_dataset = base_dataset\n        self.split = split\n        self.semantic_encoder = semantic_encoder\n</code></pre> <p>Parameters: - <code>base_dataset</code>: Base dataset instance - <code>split</code>: Data split - <code>semantic_encoder</code>: Semantic encoder (e.g., RQVAE)</p> <p>Methods:</p>"},{"location":"en/api/datasets/#create_sequences","title":"create_sequences()","text":"<p>Create user sequences.</p> <pre><code>def create_sequences(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Create user interaction sequences\n\n    Returns:\n        List of sequences\n    \"\"\"\n</code></pre>"},{"location":"en/api/datasets/#encode_sequence","title":"encode_sequence()","text":"<p>Encode sequences.</p> <pre><code>def encode_sequence(self, item_ids: List[int]) -&gt; torch.Tensor:\n    \"\"\"\n    Encode item ID sequence to semantic representation\n\n    Args:\n        item_ids: List of item IDs\n\n    Returns:\n        Encoded sequence tensor\n    \"\"\"\n</code></pre>"},{"location":"en/api/datasets/#concrete-dataset-implementations","title":"Concrete Dataset Implementations","text":""},{"location":"en/api/datasets/#p5amazondataset","title":"P5AmazonDataset","text":"<p>P5 Amazon dataset implementation.</p> <pre><code>@gin.configurable\nclass P5AmazonDataset(BaseRecommenderDataset):\n    \"\"\"P5 Amazon dataset\"\"\"\n\n    def __init__(self, config: P5AmazonConfig):\n        super().__init__(config)\n        self.category = config.category\n        self.min_rating = config.min_rating\n</code></pre> <p>Key Features: - Supports multiple product categories - Automatic download and preprocessing - Text feature extraction - Rating filtering</p>"},{"location":"en/api/datasets/#p5amazonitemdataset","title":"P5AmazonItemDataset","text":"<p>P5 Amazon item dataset wrapper.</p> <pre><code>@gin.configurable\nclass P5AmazonItemDataset(ItemDataset):\n    \"\"\"P5 Amazon item dataset\"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        split: str = \"beauty\",\n        train_test_split: str = \"all\",\n        return_text: bool = False,\n        **kwargs\n    ):\n</code></pre>"},{"location":"en/api/datasets/#p5amazonsequencedataset","title":"P5AmazonSequenceDataset","text":"<p>P5 Amazon sequence dataset wrapper.</p> <pre><code>@gin.configurable\nclass P5AmazonSequenceDataset(SequenceDataset):\n    \"\"\"P5 Amazon sequence dataset\"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        split: str = \"beauty\", \n        train_test_split: str = \"train\",\n        pretrained_rqvae_path: str = None,\n        **kwargs\n    ):\n</code></pre>"},{"location":"en/api/datasets/#dataset-factory","title":"Dataset Factory","text":""},{"location":"en/api/datasets/#datasetfactory","title":"DatasetFactory","text":"<p>Dataset factory class for unified dataset management and creation.</p> <pre><code>class DatasetFactory:\n    \"\"\"Dataset factory\"\"\"\n\n    _registered_datasets = {}\n\n    @classmethod\n    def register_dataset(\n        cls,\n        name: str,\n        base_class: Type[BaseRecommenderDataset],\n        item_class: Type[ItemDataset],\n        sequence_class: Type[SequenceDataset]\n    ) -&gt; None:\n        \"\"\"Register dataset classes\"\"\"\n</code></pre> <p>Usage Example:</p> <pre><code># Register dataset\nDatasetFactory.register_dataset(\n    \"p5_amazon\",\n    P5AmazonDataset,\n    P5AmazonItemDataset, \n    P5AmazonSequenceDataset\n)\n\n# Create dataset\nitem_dataset = DatasetFactory.create_item_dataset(\n    \"p5_amazon\",\n    root=\"data/amazon\",\n    split=\"beauty\"\n)\n</code></pre>"},{"location":"en/api/datasets/#data-processors","title":"Data Processors","text":""},{"location":"en/api/datasets/#textprocessor","title":"TextProcessor","text":"<p>Text processor for item text feature encoding.</p> <pre><code>class TextProcessor:\n    \"\"\"Text processor\"\"\"\n\n    def __init__(self, config: TextEncodingConfig):\n        self.config = config\n        self.encoder = SentenceTransformer(config.encoder_model)\n</code></pre> <p>Methods:</p>"},{"location":"en/api/datasets/#encode_item_features","title":"encode_item_features()","text":"<p>Encode item text features.</p> <pre><code>def encode_item_features(\n    self,\n    items_df: pd.DataFrame,\n    cache_key: str = None,\n    force_reload: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"\n    Encode item text features\n\n    Args:\n        items_df: Items dataframe\n        cache_key: Cache key\n        force_reload: Whether to force recomputation\n\n    Returns:\n        Item text encoding tensor\n    \"\"\"\n</code></pre>"},{"location":"en/api/datasets/#sequenceprocessor","title":"SequenceProcessor","text":"<p>Sequence processor for sequence data preprocessing.</p> <pre><code>class SequenceProcessor:\n    \"\"\"Sequence processor\"\"\"\n\n    def __init__(self, config: SequenceConfig):\n        self.config = config\n</code></pre> <p>Methods:</p>"},{"location":"en/api/datasets/#process_user_sequence","title":"process_user_sequence()","text":"<p>Process user sequences.</p> <pre><code>def process_user_sequence(\n    self,\n    sequence: List[int],\n    target_offset: int = 1\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Process user interaction sequence\n\n    Args:\n        sequence: Raw sequence\n        target_offset: Target offset\n\n    Returns:\n        Processed sequence data\n    \"\"\"\n</code></pre>"},{"location":"en/api/datasets/#usage-examples","title":"Usage Examples","text":""},{"location":"en/api/datasets/#basic-usage","title":"Basic Usage","text":"<pre><code>from generative_recommenders.data import P5AmazonDataset, P5AmazonConfig\n\n# Create configuration\nconfig = P5AmazonConfig(\n    root_dir=\"data/amazon\",\n    split=\"beauty\"\n)\n\n# Create dataset\ndataset = P5AmazonDataset(config)\ndataset.load_dataset()\n\n# Get data\nitems = dataset.get_items()\ninteractions = dataset.get_interactions()\n</code></pre>"},{"location":"en/api/datasets/#item-dataset-usage","title":"Item Dataset Usage","text":"<pre><code>from generative_recommenders.data import P5AmazonItemDataset\n\n# Create item dataset\nitem_dataset = P5AmazonItemDataset(\n    root=\"data/amazon\",\n    split=\"beauty\",\n    return_text=True\n)\n\n# Use DataLoader\ndataloader = DataLoader(item_dataset, batch_size=32, shuffle=True)\nfor batch in dataloader:\n    item_ids = batch['item_id']\n    text_features = batch['text_features']\n    # Train item encoder...\n</code></pre>"},{"location":"en/api/datasets/#sequence-dataset-usage","title":"Sequence Dataset Usage","text":"<pre><code>from generative_recommenders.data import P5AmazonSequenceDataset\nfrom generative_recommenders.models import RqVae\n\n# Load pretrained RQVAE\nrqvae = RqVae.load_from_checkpoint(\"checkpoints/rqvae.ckpt\")\n\n# Create sequence dataset\nseq_dataset = P5AmazonSequenceDataset(\n    root=\"data/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\",\n    pretrained_rqvae_path=\"checkpoints/rqvae.ckpt\"\n)\n\n# Use DataLoader\ndataloader = DataLoader(seq_dataset, batch_size=16, shuffle=True)\nfor batch in dataloader:\n    input_ids = batch['input_ids']\n    target_ids = batch['target_ids']\n    # Train sequence generation model...\n</code></pre>"},{"location":"en/api/datasets/#related-links","title":"Related Links","text":"<ul> <li>Configurations - Dataset configuration system</li> <li>Processors - Data processing utilities</li> <li>Dataset Factory - Factory pattern for dataset creation</li> <li>Trainers - Model training utilities</li> </ul>"},{"location":"en/api/modules/","title":"Modules API Reference","text":"<p>Detailed documentation for core building blocks including encoders, loss functions, metrics, and utilities.</p>"},{"location":"en/api/modules/#encoder-modules","title":"Encoder Modules","text":""},{"location":"en/api/modules/#transformerencoder","title":"TransformerEncoder","text":"<p>Transformer-based encoder implementation.</p> <pre><code>class TransformerEncoder(nn.Module):\n    \"\"\"Transformer encoder\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        embedding_dim: int,\n        num_heads: int,\n        num_layers: int,\n        attn_dim: int,\n        dropout: float = 0.1,\n        max_seq_length: int = 1024\n    ):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.pos_encoding = PositionalEncoding(embedding_dim, max_seq_length)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embedding_dim,\n            nhead=num_heads,\n            dim_feedforward=attn_dim,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.dropout = nn.Dropout(dropout)\n</code></pre> <p>Methods:</p>"},{"location":"en/api/modules/#forwardinput_ids-attention_mask","title":"forward(input_ids, attention_mask)","text":"<p>Forward pass computation.</p> <pre><code>def forward(\n    self, \n    input_ids: torch.Tensor, \n    attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass\n\n    Args:\n        input_ids: Input sequence (batch_size, seq_len)\n        attention_mask: Attention mask (batch_size, seq_len)\n\n    Returns:\n        Encoded sequence (batch_size, seq_len, embedding_dim)\n    \"\"\"\n    # Embedding and positional encoding\n    embeddings = self.embedding(input_ids)\n    embeddings = self.pos_encoding(embeddings)\n    embeddings = self.dropout(embeddings)\n\n    # Create padding mask\n    if attention_mask is not None:\n        # Convert to format expected by Transformer\n        src_key_padding_mask = (attention_mask == 0)\n    else:\n        src_key_padding_mask = None\n\n    # Transformer encoding\n    encoded = self.transformer(\n        embeddings,\n        src_key_padding_mask=src_key_padding_mask\n    )\n\n    return encoded\n</code></pre>"},{"location":"en/api/modules/#positionalencoding","title":"PositionalEncoding","text":"<p>Positional encoding module.</p> <pre><code>class PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding\"\"\"\n\n    def __init__(self, embedding_dim: int, max_seq_length: int = 5000):\n        super().__init__()\n\n        pe = torch.zeros(max_seq_length, embedding_dim)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n\n        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * \n                           (-math.log(10000.0) / embedding_dim))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Add positional encoding\n\n        Args:\n            x: Input embeddings (batch_size, seq_len, embedding_dim)\n\n        Returns:\n            Embeddings with positional encoding added\n        \"\"\"\n        return x + self.pe[:, :x.size(1)]\n</code></pre>"},{"location":"en/api/modules/#multiheadattention","title":"MultiHeadAttention","text":"<p>Multi-head attention mechanism.</p> <pre><code>class MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head attention\"\"\"\n\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_heads: int,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        assert embedding_dim % num_heads == 0\n\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.head_dim = embedding_dim // num_heads\n\n        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n        self.w_o = nn.Linear(embedding_dim, embedding_dim)\n\n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(self.head_dim)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        mask: Optional[torch.Tensor] = None\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Multi-head attention computation\n\n        Args:\n            query: Query vectors (batch_size, seq_len, embedding_dim)\n            key: Key vectors (batch_size, seq_len, embedding_dim)\n            value: Value vectors (batch_size, seq_len, embedding_dim)\n            mask: Attention mask (batch_size, seq_len, seq_len)\n\n        Returns:\n            (attention_output, attention_weights): Attention output and weights\n        \"\"\"\n        batch_size, seq_len, _ = query.size()\n\n        # Linear transformations\n        Q = self.w_q(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.w_k(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Compute attention scores\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n\n        # Apply mask\n        if mask is not None:\n            mask = mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n            scores.masked_fill_(mask == 0, -1e9)\n\n        # Attention weights\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n\n        # Attention output\n        attention_output = torch.matmul(attention_weights, V)\n        attention_output = attention_output.transpose(1, 2).contiguous().view(\n            batch_size, seq_len, self.embedding_dim\n        )\n\n        output = self.w_o(attention_output)\n\n        return output, attention_weights\n</code></pre>"},{"location":"en/api/modules/#loss-functions","title":"Loss Functions","text":""},{"location":"en/api/modules/#vqvaeloss","title":"VQVAELoss","text":"<p>VQVAE loss function.</p> <pre><code>class VQVAELoss(nn.Module):\n    \"\"\"VQVAE loss function\"\"\"\n\n    def __init__(\n        self,\n        commitment_cost: float = 0.25,\n        beta: float = 1.0\n    ):\n        super().__init__()\n        self.commitment_cost = commitment_cost\n        self.beta = beta\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        x_recon: torch.Tensor,\n        commitment_loss: torch.Tensor,\n        embedding_loss: torch.Tensor\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute VQVAE loss\n\n        Args:\n            x: Original input\n            x_recon: Reconstructed output\n            commitment_loss: Commitment loss\n            embedding_loss: Embedding loss\n\n        Returns:\n            Loss dictionary\n        \"\"\"\n        # Reconstruction loss\n        recon_loss = F.mse_loss(x_recon, x, reduction='mean')\n\n        # Total loss\n        total_loss = (\n            recon_loss + \n            self.commitment_cost * commitment_loss + \n            self.beta * embedding_loss\n        )\n\n        return {\n            'total_loss': total_loss,\n            'reconstruction_loss': recon_loss,\n            'commitment_loss': commitment_loss,\n            'embedding_loss': embedding_loss\n        }\n</code></pre>"},{"location":"en/api/modules/#sequenceloss","title":"SequenceLoss","text":"<p>Sequence modeling loss function.</p> <pre><code>class SequenceLoss(nn.Module):\n    \"\"\"Sequence modeling loss function\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        ignore_index: int = -100,\n        label_smoothing: float = 0.0\n    ):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.ignore_index = ignore_index\n        self.label_smoothing = label_smoothing\n\n        if label_smoothing &gt; 0:\n            self.criterion = nn.CrossEntropyLoss(\n                ignore_index=ignore_index,\n                label_smoothing=label_smoothing\n            )\n        else:\n            self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n\n    def forward(\n        self,\n        logits: torch.Tensor,\n        labels: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute sequence modeling loss\n\n        Args:\n            logits: Model output (batch_size, seq_len, vocab_size)\n            labels: Target labels (batch_size, seq_len)\n            attention_mask: Attention mask (batch_size, seq_len)\n\n        Returns:\n            Loss dictionary\n        \"\"\"\n        # Flatten tensors\n        flat_logits = logits.view(-1, self.vocab_size)\n        flat_labels = labels.view(-1)\n\n        # Compute loss\n        loss = self.criterion(flat_logits, flat_labels)\n\n        # Compute accuracy\n        with torch.no_grad():\n            predictions = torch.argmax(flat_logits, dim=-1)\n            mask = (flat_labels != self.ignore_index)\n            correct = (predictions == flat_labels) &amp; mask\n            accuracy = correct.sum().float() / mask.sum().float()\n\n        return {\n            'loss': loss,\n            'accuracy': accuracy\n        }\n</code></pre>"},{"location":"en/api/modules/#contrastiveloss","title":"ContrastiveLoss","text":"<p>Contrastive learning loss function.</p> <pre><code>class ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive learning loss function\"\"\"\n\n    def __init__(\n        self,\n        temperature: float = 0.1,\n        margin: float = 0.2\n    ):\n        super().__init__()\n        self.temperature = temperature\n        self.margin = margin\n\n    def forward(\n        self,\n        anchor: torch.Tensor,\n        positive: torch.Tensor,\n        negative: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute contrastive loss\n\n        Args:\n            anchor: Anchor embeddings (batch_size, embedding_dim)\n            positive: Positive embeddings (batch_size, embedding_dim)\n            negative: Negative embeddings (batch_size, num_negatives, embedding_dim)\n\n        Returns:\n            Contrastive loss\n        \"\"\"\n        # Normalize embeddings\n        anchor = F.normalize(anchor, dim=-1)\n        positive = F.normalize(positive, dim=-1)\n        negative = F.normalize(negative, dim=-1)\n\n        # Compute similarities\n        pos_sim = torch.sum(anchor * positive, dim=-1) / self.temperature\n        neg_sim = torch.bmm(negative, anchor.unsqueeze(-1)).squeeze(-1) / self.temperature\n\n        # Compute contrastive loss\n        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)\n        labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n\n        loss = F.cross_entropy(logits, labels)\n\n        return loss\n</code></pre>"},{"location":"en/api/modules/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"en/api/modules/#recommendationmetrics","title":"RecommendationMetrics","text":"<p>Recommendation system evaluation metrics.</p> <pre><code>class RecommendationMetrics:\n    \"\"\"Recommendation system evaluation metrics\"\"\"\n\n    @staticmethod\n    def recall_at_k(predictions: List[List[int]], targets: List[List[int]], k: int) -&gt; float:\n        \"\"\"\n        Compute Recall@K\n\n        Args:\n            predictions: Prediction lists\n            targets: Target lists\n            k: Top-K\n\n        Returns:\n            Recall@K value\n        \"\"\"\n        recall_scores = []\n\n        for pred, target in zip(predictions, targets):\n            if len(target) == 0:\n                continue\n\n            top_k_pred = set(pred[:k])\n            target_set = set(target)\n\n            recall = len(top_k_pred &amp; target_set) / len(target_set)\n            recall_scores.append(recall)\n\n        return np.mean(recall_scores) if recall_scores else 0.0\n\n    @staticmethod\n    def precision_at_k(predictions: List[List[int]], targets: List[List[int]], k: int) -&gt; float:\n        \"\"\"Compute Precision@K\"\"\"\n        precision_scores = []\n\n        for pred, target in zip(predictions, targets):\n            if k == 0:\n                continue\n\n            top_k_pred = set(pred[:k])\n            target_set = set(target)\n\n            precision = len(top_k_pred &amp; target_set) / k\n            precision_scores.append(precision)\n\n        return np.mean(precision_scores) if precision_scores else 0.0\n\n    @staticmethod\n    def ndcg_at_k(predictions: List[List[int]], targets: List[List[int]], k: int) -&gt; float:\n        \"\"\"Compute NDCG@K\"\"\"\n        ndcg_scores = []\n\n        for pred, target in zip(predictions, targets):\n            if len(target) == 0:\n                continue\n\n            # Compute DCG\n            dcg = 0\n            for i, item in enumerate(pred[:k]):\n                if item in target:\n                    dcg += 1 / np.log2(i + 2)\n\n            # Compute IDCG\n            idcg = sum(1 / np.log2(i + 2) for i in range(min(len(target), k)))\n\n            # Compute NDCG\n            ndcg = dcg / idcg if idcg &gt; 0 else 0\n            ndcg_scores.append(ndcg)\n\n        return np.mean(ndcg_scores) if ndcg_scores else 0.0\n\n    @staticmethod\n    def hit_rate_at_k(predictions: List[List[int]], targets: List[List[int]], k: int) -&gt; float:\n        \"\"\"Compute Hit Rate@K\"\"\"\n        hits = 0\n        total = 0\n\n        for pred, target in zip(predictions, targets):\n            if len(target) == 0:\n                continue\n\n            top_k_pred = set(pred[:k])\n            target_set = set(target)\n\n            if len(top_k_pred &amp; target_set) &gt; 0:\n                hits += 1\n            total += 1\n\n        return hits / total if total &gt; 0 else 0.0\n\n    @staticmethod\n    def coverage(predictions: List[List[int]], total_items: int) -&gt; float:\n        \"\"\"Compute item coverage\"\"\"\n        recommended_items = set()\n        for pred in predictions:\n            recommended_items.update(pred)\n\n        return len(recommended_items) / total_items\n\n    @staticmethod\n    def diversity(predictions: List[List[int]]) -&gt; float:\n        \"\"\"Compute recommendation diversity (average Jaccard distance)\"\"\"\n        if len(predictions) &lt; 2:\n            return 0.0\n\n        distances = []\n        for i in range(len(predictions)):\n            for j in range(i + 1, len(predictions)):\n                set_i = set(predictions[i])\n                set_j = set(predictions[j])\n\n                if len(set_i | set_j) &gt; 0:\n                    jaccard_sim = len(set_i &amp; set_j) / len(set_i | set_j)\n                    jaccard_dist = 1 - jaccard_sim\n                    distances.append(jaccard_dist)\n\n        return np.mean(distances) if distances else 0.0\n</code></pre>"},{"location":"en/api/modules/#utility-modules","title":"Utility Modules","text":""},{"location":"en/api/modules/#attentionvisualization","title":"AttentionVisualization","text":"<p>Attention visualization tools.</p> <pre><code>class AttentionVisualization:\n    \"\"\"Attention weight visualization\"\"\"\n\n    @staticmethod\n    def plot_attention_heatmap(\n        attention_weights: torch.Tensor,\n        input_tokens: List[str],\n        output_tokens: List[str],\n        save_path: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"\n        Plot attention heatmap\n\n        Args:\n            attention_weights: Attention weights (seq_len_out, seq_len_in)\n            input_tokens: Input token list\n            output_tokens: Output token list\n            save_path: Save path\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        plt.figure(figsize=(10, 8))\n\n        # Create heatmap\n        sns.heatmap(\n            attention_weights.cpu().numpy(),\n            xticklabels=input_tokens,\n            yticklabels=output_tokens,\n            cmap='Blues',\n            annot=True,\n            fmt='.2f'\n        )\n\n        plt.title('Attention Weights')\n        plt.xlabel('Input Tokens')\n        plt.ylabel('Output Tokens')\n        plt.xticks(rotation=45)\n        plt.yticks(rotation=0)\n\n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n\n        plt.show()\n</code></pre>"},{"location":"en/api/modules/#modelutils","title":"ModelUtils","text":"<p>Model utility functions.</p> <pre><code>class ModelUtils:\n    \"\"\"Model utility functions\"\"\"\n\n    @staticmethod\n    def count_parameters(model: nn.Module) -&gt; int:\n        \"\"\"Count model parameters\"\"\"\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    @staticmethod\n    def get_model_size(model: nn.Module) -&gt; str:\n        \"\"\"Get model size in MB\"\"\"\n        param_size = 0\n        buffer_size = 0\n\n        for param in model.parameters():\n            param_size += param.nelement() * param.element_size()\n\n        for buffer in model.buffers():\n            buffer_size += buffer.nelement() * buffer.element_size()\n\n        size_mb = (param_size + buffer_size) / 1024 / 1024\n        return f\"{size_mb:.2f} MB\"\n\n    @staticmethod\n    def freeze_layers(model: nn.Module, layer_names: List[str]) -&gt; None:\n        \"\"\"Freeze specified layers\"\"\"\n        for name, param in model.named_parameters():\n            for layer_name in layer_names:\n                if layer_name in name:\n                    param.requires_grad = False\n                    break\n\n    @staticmethod\n    def unfreeze_layers(model: nn.Module, layer_names: List[str]) -&gt; None:\n        \"\"\"Unfreeze specified layers\"\"\"\n        for name, param in model.named_parameters():\n            for layer_name in layer_names:\n                if layer_name in name:\n                    param.requires_grad = True\n                    break\n\n    @staticmethod\n    def initialize_weights(model: nn.Module, init_type: str = 'xavier') -&gt; None:\n        \"\"\"Initialize model weights\"\"\"\n        for name, param in model.named_parameters():\n            if 'weight' in name:\n                if init_type == 'xavier':\n                    nn.init.xavier_uniform_(param)\n                elif init_type == 'kaiming':\n                    nn.init.kaiming_uniform_(param)\n                elif init_type == 'normal':\n                    nn.init.normal_(param, mean=0, std=0.02)\n            elif 'bias' in name:\n                nn.init.constant_(param, 0)\n</code></pre>"},{"location":"en/api/modules/#usage-examples","title":"Usage Examples","text":""},{"location":"en/api/modules/#using-encoders","title":"Using Encoders","text":"<pre><code>from generative_recommenders.modules import TransformerEncoder\n\n# Create encoder\nencoder = TransformerEncoder(\n    vocab_size=1000,\n    embedding_dim=512,\n    num_heads=8,\n    num_layers=6,\n    attn_dim=2048\n)\n\n# Encode sequence\ninput_ids = torch.randint(0, 1000, (32, 50))  # (batch_size, seq_len)\nattention_mask = torch.ones_like(input_ids)\n\nencoded = encoder(input_ids, attention_mask)\nprint(f\"Encoded shape: {encoded.shape}\")  # (32, 50, 512)\n</code></pre>"},{"location":"en/api/modules/#using-loss-functions","title":"Using Loss Functions","text":"<pre><code>from generative_recommenders.modules import VQVAELoss, SequenceLoss\n\n# VQVAE loss\nvqvae_loss = VQVAELoss(commitment_cost=0.25)\nlosses = vqvae_loss(x, x_recon, commitment_loss, embedding_loss)\n\n# Sequence loss\nseq_loss = SequenceLoss(vocab_size=1000, label_smoothing=0.1)\nlosses = seq_loss(logits, labels, attention_mask)\n</code></pre>"},{"location":"en/api/modules/#computing-evaluation-metrics","title":"Computing Evaluation Metrics","text":"<pre><code>from generative_recommenders.modules import RecommendationMetrics\n\n# Example data\npredictions = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\ntargets = [[1, 3, 5], [7, 9]]\n\n# Compute metrics\nrecall_5 = RecommendationMetrics.recall_at_k(predictions, targets, 5)\nndcg_5 = RecommendationMetrics.ndcg_at_k(predictions, targets, 5)\nhit_rate = RecommendationMetrics.hit_rate_at_k(predictions, targets, 5)\n\nprint(f\"Recall@5: {recall_5:.4f}\")\nprint(f\"NDCG@5: {ndcg_5:.4f}\")\nprint(f\"Hit Rate@5: {hit_rate:.4f}\")\n</code></pre>"},{"location":"en/api/modules/#model-tools","title":"Model Tools","text":"<pre><code>from generative_recommenders.modules import ModelUtils\n\n# Model information\nparam_count = ModelUtils.count_parameters(model)\nmodel_size = ModelUtils.get_model_size(model)\n\nprint(f\"Parameters: {param_count:,}\")\nprint(f\"Model size: {model_size}\")\n\n# Freeze/unfreeze layers\nModelUtils.freeze_layers(model, ['embedding', 'pos_encoding'])\nModelUtils.unfreeze_layers(model, ['transformer'])\n\n# Weight initialization\nModelUtils.initialize_weights(model, init_type='xavier')\n</code></pre>"},{"location":"en/api/processors/","title":"Processors API Reference","text":"<p>Detailed documentation for text and sequence processing utilities.</p>"},{"location":"en/api/processors/#text-processor","title":"Text Processor","text":""},{"location":"en/api/processors/#textprocessor","title":"TextProcessor","text":"<p>Core class for text encoding and processing.</p> <pre><code>class TextProcessor:\n    def __init__(self, config: TextEncodingConfig):\n        self.config = config\n        self.model = None\n        self.device = config.device\n        self.cache_manager = CacheManager(config.cache_dir)\n</code></pre> <p>Parameters: - <code>config</code>: Text encoding configuration object</p> <p>Methods:</p>"},{"location":"en/api/processors/#load_model","title":"load_model()","text":"<p>Load text encoding model.</p> <pre><code>def load_model(self) -&gt; None:\n    \"\"\"\n    Load Sentence Transformer model\n    \"\"\"\n    if self.model is None:\n        from sentence_transformers import SentenceTransformer\n        self.model = SentenceTransformer(self.config.encoder_model)\n        self.model.to(self.device)\n        print(f\"Loaded text encoder: {self.config.encoder_model}\")\n</code></pre>"},{"location":"en/api/processors/#encode_textstexts-cache_key-force_reload","title":"encode_texts(texts, cache_key, force_reload)","text":"<p>Encode text list.</p> <pre><code>def encode_texts(\n    self,\n    texts: List[str],\n    cache_key: Optional[str] = None,\n    force_reload: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Encode text list to embedding vectors\n\n    Args:\n        texts: Text list\n        cache_key: Cache key, will attempt to use cache if provided\n        force_reload: Whether to force recomputation\n\n    Returns:\n        Embedding matrix (num_texts, embedding_dim)\n    \"\"\"\n    # Check cache\n    if cache_key and not force_reload and self.cache_manager.exists(cache_key):\n        print(f\"Loading embeddings from cache: {cache_key}\")\n        return self.cache_manager.load(cache_key)\n\n    # Load model\n    self.load_model()\n\n    # Batch encoding\n    print(f\"Encoding {len(texts)} texts with {self.config.encoder_model}\")\n    embeddings = []\n\n    for i in range(0, len(texts), self.config.batch_size):\n        batch_texts = texts[i:i + self.config.batch_size]\n        batch_embeddings = self.model.encode(\n            batch_texts,\n            convert_to_numpy=True,\n            normalize_embeddings=self.config.normalize_embeddings,\n            show_progress_bar=True\n        )\n        embeddings.append(batch_embeddings)\n\n    # Merge results\n    embeddings = np.vstack(embeddings)\n\n    # Save cache\n    if cache_key:\n        self.cache_manager.save(cache_key, embeddings)\n        print(f\"Saved embeddings to cache: {cache_key}\")\n\n    return embeddings\n</code></pre>"},{"location":"en/api/processors/#encode_item_featuresitems_df-cache_key-force_reload","title":"encode_item_features(items_df, cache_key, force_reload)","text":"<p>Encode item features.</p> <pre><code>def encode_item_features(\n    self,\n    items_df: pd.DataFrame,\n    cache_key: Optional[str] = None,\n    force_reload: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Encode item features to embedding vectors\n\n    Args:\n        items_df: Items DataFrame\n        cache_key: Cache key\n        force_reload: Whether to force recomputation\n\n    Returns:\n        Item embedding matrix (num_items, embedding_dim)\n    \"\"\"\n    # Format text\n    texts = []\n    for _, row in items_df.iterrows():\n        text = self.config.format_text(row.to_dict())\n        texts.append(text)\n\n    return self.encode_texts(texts, cache_key, force_reload)\n</code></pre>"},{"location":"en/api/processors/#encode_single_texttext","title":"encode_single_text(text)","text":"<p>Encode single text.</p> <pre><code>def encode_single_text(self, text: str) -&gt; np.ndarray:\n    \"\"\"\n    Encode single text\n\n    Args:\n        text: Input text\n\n    Returns:\n        Text embedding vector (embedding_dim,)\n    \"\"\"\n    self.load_model()\n\n    embedding = self.model.encode(\n        [text],\n        convert_to_numpy=True,\n        normalize_embeddings=self.config.normalize_embeddings\n    )[0]\n\n    return embedding\n</code></pre>"},{"location":"en/api/processors/#compute_similaritytext1-text2","title":"compute_similarity(text1, text2)","text":"<p>Compute text similarity.</p> <pre><code>def compute_similarity(self, text1: str, text2: str) -&gt; float:\n    \"\"\"\n    Compute cosine similarity between two texts\n\n    Args:\n        text1: First text\n        text2: Second text\n\n    Returns:\n        Cosine similarity value [-1, 1]\n    \"\"\"\n    embedding1 = self.encode_single_text(text1)\n    embedding2 = self.encode_single_text(text2)\n\n    return np.dot(embedding1, embedding2) / (\n        np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n    )\n</code></pre>"},{"location":"en/api/processors/#find_similar_textsquery_text-candidate_texts-top_k","title":"find_similar_texts(query_text, candidate_texts, top_k)","text":"<p>Find similar texts.</p> <pre><code>def find_similar_texts(\n    self,\n    query_text: str,\n    candidate_texts: List[str],\n    top_k: int = 5\n) -&gt; List[Tuple[int, str, float]]:\n    \"\"\"\n    Find texts most similar to query text\n\n    Args:\n        query_text: Query text\n        candidate_texts: Candidate text list\n        top_k: Return top k most similar\n\n    Returns:\n        List of (index, text, similarity) tuples, sorted by similarity descending\n    \"\"\"\n    query_embedding = self.encode_single_text(query_text)\n    candidate_embeddings = self.encode_texts(candidate_texts)\n\n    # Compute similarities\n    similarities = np.dot(candidate_embeddings, query_embedding)\n\n    # Get top-k\n    top_indices = np.argsort(similarities)[-top_k:][::-1]\n\n    results = []\n    for idx in top_indices:\n        results.append((idx, candidate_texts[idx], similarities[idx]))\n\n    return results\n</code></pre>"},{"location":"en/api/processors/#sequence-processor","title":"Sequence Processor","text":""},{"location":"en/api/processors/#sequenceprocessor","title":"SequenceProcessor","text":"<p>Core class for sequence data processing.</p> <pre><code>class SequenceProcessor:\n    def __init__(self, config: SequenceConfig):\n        self.config = config\n\n    def build_user_sequences(\n        self, \n        interactions_df: pd.DataFrame\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Build user interaction sequences\n\n        Args:\n            interactions_df: Interactions DataFrame containing user_id, item_id, timestamp\n\n        Returns:\n            User sequence list, each sequence contains user ID and item sequence\n        \"\"\"\n        sequences = []\n\n        # Group by user and sort by time\n        for user_id, group in interactions_df.groupby('user_id'):\n            user_interactions = group.sort_values('timestamp')\n            item_sequence = user_interactions['item_id'].tolist()\n\n            # Filter sequences that are too short\n            if len(item_sequence) &gt;= self.config.min_seq_length:\n                sequences.append({\n                    'user_id': user_id,\n                    'item_sequence': item_sequence,\n                    'timestamps': user_interactions['timestamp'].tolist() if self.config.include_timestamps else None\n                })\n\n        return sequences\n</code></pre>"},{"location":"en/api/processors/#create_training_samplessequences","title":"create_training_samples(sequences)","text":"<p>Create training samples.</p> <pre><code>def create_training_samples(\n    self,\n    sequences: List[Dict[str, Any]]\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Create training samples from user sequences\n\n    Args:\n        sequences: User sequence list\n\n    Returns:\n        Training sample list, each sample contains input and target sequences\n    \"\"\"\n    training_samples = []\n\n    for seq_data in sequences:\n        item_sequence = seq_data['item_sequence']\n\n        # Create multiple subsequences\n        for i in range(0, len(item_sequence) - self.config.min_seq_length + 1, self.config.sequence_stride):\n            # Determine subsequence length\n            end_idx = min(i + self.config.max_seq_length, len(item_sequence))\n\n            if end_idx - i &gt;= self.config.min_seq_length:\n                input_seq = item_sequence[i:end_idx-self.config.target_offset]\n                target_seq = item_sequence[i+self.config.target_offset:end_idx]\n\n                if len(input_seq) &gt; 0 and len(target_seq) &gt; 0:\n                    sample = {\n                        'user_id': seq_data['user_id'],\n                        'input_sequence': input_seq,\n                        'target_sequence': target_seq\n                    }\n\n                    # Add timestamp information\n                    if self.config.include_timestamps and seq_data['timestamps']:\n                        sample['input_timestamps'] = seq_data['timestamps'][i:end_idx-self.config.target_offset]\n                        sample['target_timestamps'] = seq_data['timestamps'][i+self.config.target_offset:end_idx]\n\n                    training_samples.append(sample)\n\n    return training_samples\n</code></pre>"},{"location":"en/api/processors/#pad_and_truncate_sequencesequence","title":"pad_and_truncate_sequence(sequence)","text":"<p>Pad and truncate sequence.</p> <pre><code>def pad_and_truncate_sequence(self, sequence: List[int]) -&gt; List[int]:\n    \"\"\"\n    Pad and truncate sequence to specified length\n\n    Args:\n        sequence: Input sequence\n\n    Returns:\n        Processed sequence\n    \"\"\"\n    # Truncate\n    if len(sequence) &gt; self.config.max_seq_length:\n        sequence = self.config.truncate_sequence(sequence)\n\n    # Pad\n    if len(sequence) &lt; self.config.max_seq_length:\n        sequence = self.config.pad_sequence(sequence)\n\n    return sequence\n</code></pre>"},{"location":"en/api/processors/#create_attention_masksequence","title":"create_attention_mask(sequence)","text":"<p>Create attention mask.</p> <pre><code>def create_attention_mask(self, sequence: List[int]) -&gt; List[int]:\n    \"\"\"\n    Create attention mask for sequence\n\n    Args:\n        sequence: Input sequence\n\n    Returns:\n        Attention mask, 1 for valid positions, 0 for padding positions\n    \"\"\"\n    mask = []\n    for token in sequence:\n        if token == self.config.padding_token:\n            mask.append(0)\n        else:\n            mask.append(1)\n\n    return mask\n</code></pre>"},{"location":"en/api/processors/#encode_time_featurestimestamps","title":"encode_time_features(timestamps)","text":"<p>Encode time features.</p> <pre><code>def encode_time_features(self, timestamps: List[float]) -&gt; np.ndarray:\n    \"\"\"\n    Encode timestamps to feature vectors\n\n    Args:\n        timestamps: Timestamp list\n\n    Returns:\n        Time feature matrix (seq_len, time_encoding_dim)\n    \"\"\"\n    if not timestamps:\n        return np.zeros((0, self.config.time_encoding_dim))\n\n    # Normalize timestamps\n    timestamps = np.array(timestamps)\n    min_time, max_time = timestamps.min(), timestamps.max()\n\n    if max_time &gt; min_time:\n        normalized_times = (timestamps - min_time) / (max_time - min_time)\n    else:\n        normalized_times = np.zeros_like(timestamps)\n\n    # Create sinusoidal encoding\n    time_features = []\n    for i in range(self.config.time_encoding_dim // 2):\n        freq = 1.0 / (10000 ** (2 * i / self.config.time_encoding_dim))\n        sin_features = np.sin(normalized_times * freq)\n        cos_features = np.cos(normalized_times * freq)\n        time_features.extend([sin_features, cos_features])\n\n    # Transpose and truncate to specified dimension\n    time_features = np.array(time_features[:self.config.time_encoding_dim]).T\n\n    return time_features\n</code></pre>"},{"location":"en/api/processors/#data-augmentation-processor","title":"Data Augmentation Processor","text":""},{"location":"en/api/processors/#dataaugmentor","title":"DataAugmentor","text":"<p>Data augmentation processor.</p> <pre><code>class DataAugmentor:\n    def __init__(self, augmentation_config: Dict[str, Any]):\n        self.config = augmentation_config\n\n    def augment_sequence(self, sequence: List[int]) -&gt; List[int]:\n        \"\"\"\n        Perform data augmentation on sequence\n\n        Args:\n            sequence: Original sequence\n\n        Returns:\n            Augmented sequence\n        \"\"\"\n        augmented = sequence.copy()\n\n        # Random drop\n        if self.config.get('random_drop', False):\n            drop_prob = self.config.get('drop_prob', 0.1)\n            augmented = [item for item in augmented if random.random() &gt; drop_prob]\n\n        # Random shuffle\n        if self.config.get('random_shuffle', False):\n            shuffle_prob = self.config.get('shuffle_prob', 0.1)\n            if random.random() &lt; shuffle_prob:\n                # Only shuffle partial subsequence\n                start = random.randint(0, max(0, len(augmented) - 3))\n                end = min(start + random.randint(2, 4), len(augmented))\n                subseq = augmented[start:end]\n                random.shuffle(subseq)\n                augmented[start:end] = subseq\n\n        # Random replace\n        if self.config.get('random_replace', False):\n            replace_prob = self.config.get('replace_prob', 0.05)\n            vocab_size = self.config.get('vocab_size', 1000)\n\n            for i in range(len(augmented)):\n                if random.random() &lt; replace_prob:\n                    augmented[i] = random.randint(1, vocab_size)\n\n        return augmented\n</code></pre>"},{"location":"en/api/processors/#preprocessing-pipeline","title":"Preprocessing Pipeline","text":""},{"location":"en/api/processors/#preprocessingpipeline","title":"PreprocessingPipeline","text":"<p>Data preprocessing pipeline.</p> <pre><code>class PreprocessingPipeline:\n    def __init__(\n        self,\n        text_processor: TextProcessor,\n        sequence_processor: SequenceProcessor,\n        augmentor: Optional[DataAugmentor] = None\n    ):\n        self.text_processor = text_processor\n        self.sequence_processor = sequence_processor\n        self.augmentor = augmentor\n\n    def process_items(\n        self,\n        items_df: pd.DataFrame,\n        cache_key: str = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Process item data\n\n        Args:\n            items_df: Items DataFrame\n            cache_key: Cache key\n\n        Returns:\n            Processed items DataFrame with feature vectors\n        \"\"\"\n        print(\"Processing item features...\")\n\n        # Encode text features\n        embeddings = self.text_processor.encode_item_features(\n            items_df, cache_key=cache_key\n        )\n\n        # Add features to DataFrame\n        processed_df = items_df.copy()\n        processed_df['features'] = embeddings.tolist()\n\n        return processed_df\n\n    def process_interactions(\n        self,\n        interactions_df: pd.DataFrame,\n        items_df: pd.DataFrame\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Process interaction data to generate sequences\n\n        Args:\n            interactions_df: Interactions DataFrame\n            items_df: Items DataFrame\n\n        Returns:\n            Processed sequence data\n        \"\"\"\n        print(\"Building user sequences...\")\n\n        # Build user sequences\n        sequences = self.sequence_processor.build_user_sequences(interactions_df)\n\n        # Create training samples\n        training_samples = self.sequence_processor.create_training_samples(sequences)\n\n        # Data augmentation\n        if self.augmentor:\n            augmented_samples = []\n            for sample in training_samples:\n                # Original sample\n                augmented_samples.append(sample)\n\n                # Augmented sample\n                aug_input = self.augmentor.augment_sequence(sample['input_sequence'])\n                aug_target = self.augmentor.augment_sequence(sample['target_sequence'])\n\n                augmented_sample = sample.copy()\n                augmented_sample['input_sequence'] = aug_input\n                augmented_sample['target_sequence'] = aug_target\n                augmented_samples.append(augmented_sample)\n\n            training_samples = augmented_samples\n\n        return training_samples\n</code></pre>"},{"location":"en/api/processors/#utility-functions","title":"Utility Functions","text":""},{"location":"en/api/processors/#compute_sequence_statisticssequences","title":"compute_sequence_statistics(sequences)","text":"<p>Compute sequence statistics.</p> <pre><code>def compute_sequence_statistics(sequences: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Compute sequence data statistics\n\n    Args:\n        sequences: Sequence list\n\n    Returns:\n        Statistics dictionary\n    \"\"\"\n    if not sequences:\n        return {}\n\n    lengths = [len(seq['item_sequence']) for seq in sequences]\n    unique_users = len(set(seq['user_id'] for seq in sequences))\n\n    # Compute item frequencies\n    item_counts = {}\n    for seq in sequences:\n        for item_id in seq['item_sequence']:\n            item_counts[item_id] = item_counts.get(item_id, 0) + 1\n\n    stats = {\n        'num_sequences': len(sequences),\n        'num_unique_users': unique_users,\n        'num_unique_items': len(item_counts),\n        'avg_sequence_length': np.mean(lengths),\n        'min_sequence_length': np.min(lengths),\n        'max_sequence_length': np.max(lengths),\n        'median_sequence_length': np.median(lengths),\n        'total_interactions': sum(lengths),\n        'most_popular_items': sorted(item_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n    }\n\n    return stats\n</code></pre>"},{"location":"en/api/processors/#visualize_embeddingsembeddings-labels-method","title":"visualize_embeddings(embeddings, labels, method)","text":"<p>Visualize embedding vectors.</p> <pre><code>def visualize_embeddings(\n    embeddings: np.ndarray,\n    labels: List[str] = None,\n    method: str = 'tsne',\n    save_path: str = None\n) -&gt; None:\n    \"\"\"\n    Visualize high-dimensional embedding vectors\n\n    Args:\n        embeddings: Embedding matrix (n_samples, embedding_dim)\n        labels: Sample labels\n        method: Dimensionality reduction method ('tsne', 'pca', 'umap')\n        save_path: Save path\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    # Dimensionality reduction\n    if method == 'tsne':\n        from sklearn.manifold import TSNE\n        reducer = TSNE(n_components=2, random_state=42)\n    elif method == 'pca':\n        from sklearn.decomposition import PCA\n        reducer = PCA(n_components=2)\n    elif method == 'umap':\n        import umap\n        reducer = umap.UMAP(n_components=2, random_state=42)\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    reduced_embeddings = reducer.fit_transform(embeddings)\n\n    # Plot\n    plt.figure(figsize=(10, 8))\n    if labels:\n        unique_labels = list(set(labels))\n        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n\n        for i, label in enumerate(unique_labels):\n            mask = np.array(labels) == label\n            plt.scatter(\n                reduced_embeddings[mask, 0],\n                reduced_embeddings[mask, 1],\n                c=[colors[i]],\n                label=label,\n                alpha=0.7\n            )\n        plt.legend()\n    else:\n        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)\n\n    plt.title(f'Embedding Visualization ({method.upper()})')\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n\n    plt.show()\n</code></pre>"},{"location":"en/api/processors/#usage-examples","title":"Usage Examples","text":""},{"location":"en/api/processors/#text-processing","title":"Text Processing","text":"<pre><code>from generative_recommenders.data.processors import TextProcessor\nfrom generative_recommenders.data.configs import TextEncodingConfig\n\n# Create configuration\nconfig = TextEncodingConfig(\n    encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    template=\"Title: {title}; Category: {category}\",\n    batch_size=32\n)\n\n# Create processor\nprocessor = TextProcessor(config)\n\n# Encode texts\ntexts = [\"Apple iPhone 13\", \"Samsung Galaxy S21\", \"Sony WH-1000XM4\"]\nembeddings = processor.encode_texts(texts, cache_key=\"sample_texts\")\n\nprint(f\"Embeddings shape: {embeddings.shape}\")\n</code></pre>"},{"location":"en/api/processors/#sequence-processing","title":"Sequence Processing","text":"<pre><code>from generative_recommenders.data.processors import SequenceProcessor\nfrom generative_recommenders.data.configs import SequenceConfig\n\n# Create configuration\nconfig = SequenceConfig(\n    max_seq_length=50,\n    min_seq_length=3,\n    target_offset=1\n)\n\n# Create processor\nprocessor = SequenceProcessor(config)\n\n# Process interaction data\nsequences = processor.build_user_sequences(interactions_df)\ntraining_samples = processor.create_training_samples(sequences)\n\nprint(f\"Generated {len(training_samples)} training samples\")\n</code></pre>"},{"location":"en/api/processors/#complete-preprocessing-pipeline","title":"Complete Preprocessing Pipeline","text":"<pre><code>from generative_recommenders.data.processors import PreprocessingPipeline\n\n# Create pipeline\npipeline = PreprocessingPipeline(\n    text_processor=text_processor,\n    sequence_processor=sequence_processor\n)\n\n# Process data\nprocessed_items = pipeline.process_items(items_df, cache_key=\"items_beauty\")\nprocessed_sequences = pipeline.process_interactions(interactions_df, processed_items)\n\n# View statistics\nstats = compute_sequence_statistics(processed_sequences)\nprint(f\"Dataset statistics: {stats}\")\n</code></pre>"},{"location":"en/api/rqvae/","title":"RQVAE API Reference","text":"<p>Detailed API documentation for the Residual Quantized Variational Autoencoder (RQVAE).</p>"},{"location":"en/api/rqvae/#core-classes","title":"Core Classes","text":""},{"location":"en/api/rqvae/#rqvae","title":"RqVae","text":"<p>Main RQVAE model class.</p> <pre><code>class RqVae(LightningModule):\n    def __init__(\n        self,\n        input_dim: int = 768,\n        hidden_dim: int = 512,\n        latent_dim: int = 256,\n        num_embeddings: int = 1024,\n        commitment_cost: float = 0.25,\n        learning_rate: float = 1e-3\n    )\n</code></pre> <p>Parameters: - <code>input_dim</code>: Input feature dimension - <code>hidden_dim</code>: Hidden layer dimension - <code>latent_dim</code>: Latent space dimension - <code>num_embeddings</code>: Number of embedding vectors - <code>commitment_cost</code>: Commitment loss weight - <code>learning_rate</code>: Learning rate</p> <p>Methods:</p>"},{"location":"en/api/rqvae/#forwardfeatures","title":"forward(features)","text":"<p>Forward pass computation.</p> <pre><code>def forward(self, features: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Args:\n        features: Input features (batch_size, input_dim)\n\n    Returns:\n        reconstructed: Reconstructed features (batch_size, input_dim)\n        commitment_loss: Commitment loss\n        embedding_loss: Embedding loss\n        semantic_ids: Semantic IDs (batch_size,)\n    \"\"\"\n</code></pre>"},{"location":"en/api/rqvae/#encodefeatures","title":"encode(features)","text":"<p>Encode features to latent representation.</p> <pre><code>def encode(self, features: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        features: Input features (batch_size, input_dim)\n\n    Returns:\n        encoded: Encoded latent representation (batch_size, latent_dim)\n    \"\"\"\n</code></pre>"},{"location":"en/api/rqvae/#generate_semantic_idsfeatures","title":"generate_semantic_ids(features)","text":"<p>Generate semantic IDs.</p> <pre><code>def generate_semantic_ids(self, features: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        features: Input features (batch_size, input_dim)\n\n    Returns:\n        semantic_ids: Semantic IDs (batch_size,)\n    \"\"\"\n</code></pre>"},{"location":"en/api/rqvae/#component-classes","title":"Component Classes","text":""},{"location":"en/api/rqvae/#vectorquantizer","title":"VectorQuantizer","text":"<p>Vector quantization layer implementation.</p> <pre><code>class VectorQuantizer(nn.Module):\n    def __init__(\n        self,\n        num_embeddings: int,\n        embedding_dim: int,\n        commitment_cost: float = 0.25\n    )\n</code></pre> <p>Parameters: - <code>num_embeddings</code>: Number of embedding vectors - <code>embedding_dim</code>: Embedding dimension - <code>commitment_cost</code>: Commitment loss weight</p> <p>Methods:</p>"},{"location":"en/api/rqvae/#forwardinputs","title":"forward(inputs)","text":"<p>Quantize input vectors.</p> <pre><code>def forward(self, inputs: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Args:\n        inputs: Input vectors (batch_size, embedding_dim)\n\n    Returns:\n        quantized: Quantized vectors\n        commitment_loss: Commitment loss\n        embedding_loss: Embedding loss\n        encoding_indices: Encoding indices\n    \"\"\"\n</code></pre>"},{"location":"en/api/rqvae/#encoder","title":"Encoder","text":"<p>Encoder network.</p> <pre><code>class Encoder(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        latent_dim: int\n    )\n</code></pre>"},{"location":"en/api/rqvae/#decoder","title":"Decoder","text":"<p>Decoder network.</p> <pre><code>class Decoder(nn.Module):\n    def __init__(\n        self,\n        latent_dim: int,\n        hidden_dim: int,\n        output_dim: int\n    )\n</code></pre>"},{"location":"en/api/rqvae/#training-interface","title":"Training Interface","text":""},{"location":"en/api/rqvae/#training-step","title":"Training Step","text":"<pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step\"\"\"\n    features = batch['features']\n\n    # Forward pass\n    reconstructed, commitment_loss, embedding_loss, semantic_ids = self(features)\n\n    # Compute losses\n    recon_loss = F.mse_loss(reconstructed, features)\n    total_loss = recon_loss + commitment_loss + embedding_loss\n\n    # Log metrics\n    self.log('train_loss', total_loss)\n    self.log('train_recon_loss', recon_loss)\n    self.log('train_commitment_loss', commitment_loss)\n    self.log('train_embedding_loss', embedding_loss)\n\n    return total_loss\n</code></pre>"},{"location":"en/api/rqvae/#validation-step","title":"Validation Step","text":"<pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step\"\"\"\n    features = batch['features']\n\n    # Forward pass\n    reconstructed, commitment_loss, embedding_loss, semantic_ids = self(features)\n\n    # Compute losses\n    recon_loss = F.mse_loss(reconstructed, features)\n    total_loss = recon_loss + commitment_loss + embedding_loss\n\n    # Log metrics\n    self.log('val_loss', total_loss)\n    self.log('val_recon_loss', recon_loss)\n\n    return total_loss\n</code></pre>"},{"location":"en/api/rqvae/#configuration-interface","title":"Configuration Interface","text":""},{"location":"en/api/rqvae/#optimizer-configuration","title":"Optimizer Configuration","text":"<pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizers\"\"\"\n    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=5\n    )\n\n    return {\n        'optimizer': optimizer,\n        'lr_scheduler': {\n            'scheduler': scheduler,\n            'monitor': 'val_loss'\n        }\n    }\n</code></pre>"},{"location":"en/api/rqvae/#utility-functions","title":"Utility Functions","text":""},{"location":"en/api/rqvae/#model-save-and-load","title":"Model Save and Load","text":"<pre><code># Save model\nmodel.save_pretrained(\"path/to/model\")\n\n# Load model\nmodel = RqVae.load_from_checkpoint(\"path/to/checkpoint.ckpt\")\n</code></pre>"},{"location":"en/api/rqvae/#batch-inference","title":"Batch Inference","text":"<pre><code>def batch_inference(model, dataloader, device='cuda'):\n    \"\"\"Batch inference for semantic ID generation\"\"\"\n    model.eval()\n    model.to(device)\n\n    all_semantic_ids = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            features = batch['features'].to(device)\n            semantic_ids = model.generate_semantic_ids(features)\n            all_semantic_ids.append(semantic_ids.cpu())\n\n    return torch.cat(all_semantic_ids, dim=0)\n</code></pre>"},{"location":"en/api/rqvae/#evaluation-interface","title":"Evaluation Interface","text":""},{"location":"en/api/rqvae/#reconstruction-quality-evaluation","title":"Reconstruction Quality Evaluation","text":"<pre><code>def evaluate_reconstruction(model, dataloader, device='cuda'):\n    \"\"\"Evaluate reconstruction quality\"\"\"\n    model.eval()\n    model.to(device)\n\n    total_mse = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            features = batch['features'].to(device)\n            reconstructed, _, _, _ = model(features)\n\n            mse = F.mse_loss(reconstructed, features, reduction='sum')\n            total_mse += mse.item()\n            total_samples += features.size(0)\n\n    avg_mse = total_mse / total_samples\n    return {'mse': avg_mse, 'rmse': avg_mse ** 0.5}\n</code></pre>"},{"location":"en/api/rqvae/#quantization-quality-evaluation","title":"Quantization Quality Evaluation","text":"<pre><code>def evaluate_quantization(model, dataloader, device='cuda'):\n    \"\"\"Evaluate quantization quality\"\"\"\n    model.eval()\n    model.to(device)\n\n    all_indices = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            features = batch['features'].to(device)\n            _, _, _, semantic_ids = model(features)\n            all_indices.append(semantic_ids.cpu())\n\n    all_indices = torch.cat(all_indices, dim=0)\n\n    # Compute usage statistics\n    unique_codes = len(torch.unique(all_indices))\n    total_codes = model.quantizer.num_embeddings\n    usage_rate = unique_codes / total_codes\n\n    # Compute perplexity\n    counts = torch.bincount(all_indices, minlength=total_codes).float()\n    probs = counts / counts.sum()\n    perplexity = torch.exp(-torch.sum(probs * torch.log(probs + 1e-10)))\n\n    return {\n        'usage_rate': usage_rate,\n        'unique_codes': unique_codes,\n        'perplexity': perplexity.item()\n    }\n</code></pre>"},{"location":"en/api/rqvae/#usage-examples","title":"Usage Examples","text":""},{"location":"en/api/rqvae/#basic-training","title":"Basic Training","text":"<pre><code>from generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\nimport pytorch_lightning as pl\n\n# Create dataset\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Create model\nmodel = RqVae(\n    input_dim=768,\n    hidden_dim=512,\n    latent_dim=256,\n    num_embeddings=1024,\n    learning_rate=1e-3\n)\n\n# Train model\ntrainer = pl.Trainer(max_epochs=100, gpus=1)\ntrainer.fit(model, dataloader)\n</code></pre>"},{"location":"en/api/rqvae/#semantic-id-generation","title":"Semantic ID Generation","text":"<pre><code># Load trained model\nmodel = RqVae.load_from_checkpoint(\"checkpoints/rqvae.ckpt\")\nmodel.eval()\n\n# Generate semantic IDs\nwith torch.no_grad():\n    features = torch.randn(10, 768)  # Example features\n    semantic_ids = model.generate_semantic_ids(features)\n    print(f\"Semantic IDs: {semantic_ids}\")\n</code></pre>"},{"location":"en/api/tiger/","title":"TIGER API Reference","text":"<p>Detailed API documentation for the Transformer-based generative retrieval model (TIGER).</p>"},{"location":"en/api/tiger/#core-classes","title":"Core Classes","text":""},{"location":"en/api/tiger/#tiger","title":"Tiger","text":"<p>Main TIGER model class.</p> <pre><code>class Tiger(LightningModule):\n    def __init__(\n        self,\n        vocab_size: int,\n        embedding_dim: int = 512,\n        num_heads: int = 8,\n        num_layers: int = 6,\n        attn_dim: int = 2048,\n        dropout: float = 0.1,\n        max_seq_length: int = 1024,\n        learning_rate: float = 1e-4\n    )\n</code></pre> <p>Parameters: - <code>vocab_size</code>: Vocabulary size - <code>embedding_dim</code>: Embedding dimension - <code>num_heads</code>: Number of attention heads - <code>num_layers</code>: Number of Transformer layers - <code>attn_dim</code>: Attention dimension - <code>dropout</code>: Dropout probability - <code>max_seq_length</code>: Maximum sequence length - <code>learning_rate</code>: Learning rate</p> <p>Methods:</p>"},{"location":"en/api/tiger/#forwardinput_ids-attention_masknone","title":"forward(input_ids, attention_mask=None)","text":"<p>Forward pass computation.</p> <pre><code>def forward(\n    self, \n    input_ids: torch.Tensor, \n    attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        input_ids: Input sequence (batch_size, seq_len)\n        attention_mask: Attention mask (batch_size, seq_len)\n\n    Returns:\n        logits: Output logits (batch_size, seq_len, vocab_size)\n    \"\"\"\n</code></pre>"},{"location":"en/api/tiger/#generateinput_ids-max_length50-temperature10-top_knone-top_pnone","title":"generate(input_ids, max_length=50, temperature=1.0, top_k=None, top_p=None)","text":"<p>Generate recommendation sequences.</p> <pre><code>def generate(\n    self,\n    input_ids: torch.Tensor,\n    max_length: int = 50,\n    temperature: float = 1.0,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        input_ids: Input sequence\n        max_length: Maximum generation length\n        temperature: Temperature parameter\n        top_k: Top-k sampling\n        top_p: Top-p sampling\n\n    Returns:\n        generated: Generated sequence\n    \"\"\"\n</code></pre>"},{"location":"en/api/tiger/#generate_with_trieinput_ids-trie-max_length50","title":"generate_with_trie(input_ids, trie, max_length=50)","text":"<p>Generate with Trie constraints.</p> <pre><code>def generate_with_trie(\n    self,\n    input_ids: torch.Tensor,\n    trie: TrieNode,\n    max_length: int = 50\n) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        input_ids: Input sequence\n        trie: Trie constraint structure\n        max_length: Maximum generation length\n\n    Returns:\n        generated: Constrained generated sequence\n    \"\"\"\n</code></pre>"},{"location":"en/api/tiger/#component-classes","title":"Component Classes","text":""},{"location":"en/api/tiger/#transformerblock","title":"TransformerBlock","text":"<p>Transformer block implementation.</p> <pre><code>class TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_heads: int,\n        attn_dim: int,\n        dropout: float = 0.1\n    )\n</code></pre>"},{"location":"en/api/tiger/#multiheadattention","title":"MultiHeadAttention","text":"<p>Multi-head attention mechanism.</p> <pre><code>class MultiHeadAttention(nn.Module):\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_heads: int,\n        dropout: float = 0.1\n    )\n</code></pre>"},{"location":"en/api/tiger/#positionalencoding","title":"PositionalEncoding","text":"<p>Positional encoding.</p> <pre><code>class PositionalEncoding(nn.Module):\n    def __init__(\n        self,\n        embedding_dim: int,\n        max_seq_length: int = 5000\n    )\n</code></pre>"},{"location":"en/api/tiger/#data-structures","title":"Data Structures","text":""},{"location":"en/api/tiger/#trienode","title":"TrieNode","text":"<p>Trie node for constrained generation.</p> <pre><code>class TrieNode(defaultdict):\n    def __init__(self):\n        super().__init__(TrieNode)\n        self.is_end = False\n\n    def add_sequence(self, sequence: List[int]):\n        \"\"\"Add sequence to Trie\"\"\"\n        node = self\n        for token in sequence:\n            node = node[token]\n        node.is_end = True\n\n    def get_valid_tokens(self) -&gt; List[int]:\n        \"\"\"Get valid tokens at current node\"\"\"\n        return list(self.keys())\n</code></pre>"},{"location":"en/api/tiger/#build-trie","title":"Build Trie","text":"<pre><code>def build_trie(valid_sequences: List[List[int]]) -&gt; TrieNode:\n    \"\"\"Build Trie of valid sequences\"\"\"\n    root = TrieNode()\n    for sequence in valid_sequences:\n        root.add_sequence(sequence)\n    return root\n</code></pre>"},{"location":"en/api/tiger/#training-interface","title":"Training Interface","text":""},{"location":"en/api/tiger/#training-step","title":"Training Step","text":"<pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step\"\"\"\n    input_ids = batch['input_ids']\n    labels = batch['labels']\n    attention_mask = batch.get('attention_mask', None)\n\n    # Forward pass\n    logits = self(input_ids, attention_mask)\n\n    # Compute loss\n    shift_logits = logits[..., :-1, :].contiguous()\n    shift_labels = labels[..., 1:].contiguous()\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fn(\n        shift_logits.view(-1, shift_logits.size(-1)),\n        shift_labels.view(-1)\n    )\n\n    # Log metrics\n    self.log('train_loss', loss)\n\n    return loss\n</code></pre>"},{"location":"en/api/tiger/#validation-step","title":"Validation Step","text":"<pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step\"\"\"\n    input_ids = batch['input_ids']\n    labels = batch['labels']\n    attention_mask = batch.get('attention_mask', None)\n\n    # Forward pass\n    logits = self(input_ids, attention_mask)\n\n    # Compute loss\n    shift_logits = logits[..., :-1, :].contiguous()\n    shift_labels = labels[..., 1:].contiguous()\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fn(\n        shift_logits.view(-1, shift_logits.size(-1)),\n        shift_labels.view(-1)\n    )\n\n    # Log metrics\n    self.log('val_loss', loss)\n\n    return loss\n</code></pre>"},{"location":"en/api/tiger/#inference-interface","title":"Inference Interface","text":""},{"location":"en/api/tiger/#batch-generation","title":"Batch Generation","text":"<pre><code>def batch_generate(\n    model: Tiger,\n    input_sequences: List[torch.Tensor],\n    max_length: int = 50,\n    device: str = 'cuda'\n) -&gt; List[torch.Tensor]:\n    \"\"\"Batch generation for recommendations\"\"\"\n    model.eval()\n    model.to(device)\n\n    results = []\n\n    with torch.no_grad():\n        for input_seq in input_sequences:\n            input_seq = input_seq.to(device)\n            generated = model.generate(input_seq, max_length=max_length)\n            results.append(generated.cpu())\n\n    return results\n</code></pre>"},{"location":"en/api/tiger/#constrained-generation","title":"Constrained Generation","text":"<pre><code>def constrained_generate(\n    model: Tiger,\n    input_ids: torch.Tensor,\n    valid_item_sequences: List[List[int]],\n    max_length: int = 50\n) -&gt; torch.Tensor:\n    \"\"\"Constrained generation for recommendations\"\"\"\n    # Build Trie\n    trie = build_trie(valid_item_sequences)\n\n    # Constrained generation\n    return model.generate_with_trie(input_ids, trie, max_length)\n</code></pre>"},{"location":"en/api/tiger/#evaluation-interface","title":"Evaluation Interface","text":""},{"location":"en/api/tiger/#top-k-recommendation-evaluation","title":"Top-K Recommendation Evaluation","text":"<pre><code>def evaluate_recommendation(\n    model: Tiger,\n    test_dataloader: DataLoader,\n    k_values: List[int] = [5, 10, 20],\n    device: str = 'cuda'\n) -&gt; Dict[str, float]:\n    \"\"\"Evaluate recommendation performance\"\"\"\n    model.eval()\n    model.to(device)\n\n    all_predictions = []\n    all_targets = []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            targets = batch['targets']\n\n            # Generate recommendations\n            generated = model.generate(input_ids, max_length=50)\n\n            all_predictions.extend(generated.cpu().tolist())\n            all_targets.extend(targets.tolist())\n\n    # Compute metrics\n    metrics = {}\n    for k in k_values:\n        recall_k = compute_recall_at_k(all_predictions, all_targets, k)\n        ndcg_k = compute_ndcg_at_k(all_predictions, all_targets, k)\n\n        metrics[f'recall@{k}'] = recall_k\n        metrics[f'ndcg@{k}'] = ndcg_k\n\n    return metrics\n</code></pre>"},{"location":"en/api/tiger/#perplexity-evaluation","title":"Perplexity Evaluation","text":"<pre><code>def evaluate_perplexity(\n    model: Tiger,\n    test_dataloader: DataLoader,\n    device: str = 'cuda'\n) -&gt; float:\n    \"\"\"Evaluate perplexity\"\"\"\n    model.eval()\n    model.to(device)\n\n    total_loss = 0\n    total_tokens = 0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            attention_mask = batch.get('attention_mask', None)\n\n            logits = model(input_ids, attention_mask)\n\n            # Compute loss\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n\n            loss_fn = nn.CrossEntropyLoss(ignore_index=-100, reduction='sum')\n            loss = loss_fn(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1)\n            )\n\n            # Count valid tokens\n            valid_tokens = (shift_labels != -100).sum()\n\n            total_loss += loss.item()\n            total_tokens += valid_tokens.item()\n\n    avg_loss = total_loss / total_tokens\n    perplexity = math.exp(avg_loss)\n\n    return perplexity\n</code></pre>"},{"location":"en/api/tiger/#utility-functions","title":"Utility Functions","text":""},{"location":"en/api/tiger/#sequence-processing","title":"Sequence Processing","text":"<pre><code>def pad_sequences(\n    sequences: List[torch.Tensor],\n    pad_token_id: int = 0,\n    max_length: Optional[int] = None\n) -&gt; torch.Tensor:\n    \"\"\"Pad sequences to same length\"\"\"\n    if max_length is None:\n        max_length = max(len(seq) for seq in sequences)\n\n    padded = []\n    for seq in sequences:\n        if len(seq) &lt; max_length:\n            pad_length = max_length - len(seq)\n            padded_seq = torch.cat([\n                seq, \n                torch.full((pad_length,), pad_token_id, dtype=seq.dtype)\n            ])\n        else:\n            padded_seq = seq[:max_length]\n        padded.append(padded_seq)\n\n    return torch.stack(padded)\n</code></pre>"},{"location":"en/api/tiger/#sampling-strategies","title":"Sampling Strategies","text":"<pre><code>def top_k_top_p_sampling(\n    logits: torch.Tensor,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    temperature: float = 1.0\n) -&gt; torch.Tensor:\n    \"\"\"Top-k and top-p sampling\"\"\"\n    logits = logits / temperature\n\n    # Top-k sampling\n    if top_k is not None:\n        top_k = min(top_k, logits.size(-1))\n        values, indices = torch.topk(logits, top_k)\n        logits[logits &lt; values[..., [-1]]] = float('-inf')\n\n    # Top-p sampling\n    if top_p is not None:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Find positions where cumulative probability exceeds top_p\n        sorted_indices_to_remove = cumulative_probs &gt; top_p\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = float('-inf')\n\n    # Sampling\n    probs = F.softmax(logits, dim=-1)\n    next_token = torch.multinomial(probs, 1)\n\n    return next_token\n</code></pre>"},{"location":"en/api/tiger/#usage-examples","title":"Usage Examples","text":""},{"location":"en/api/tiger/#basic-training","title":"Basic Training","text":"<pre><code>from generative_recommenders.models.tiger import Tiger\nfrom generative_recommenders.data.p5_amazon import P5AmazonSequenceDataset\nimport pytorch_lightning as pl\n\n# Create dataset\ndataset = P5AmazonSequenceDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\",\n    pretrained_rqvae_path=\"checkpoints/rqvae.ckpt\"\n)\n\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# Create model\nmodel = Tiger(\n    vocab_size=1024,\n    embedding_dim=512,\n    num_heads=8,\n    num_layers=6,\n    learning_rate=1e-4\n)\n\n# Train model\ntrainer = pl.Trainer(max_epochs=50, gpus=1)\ntrainer.fit(model, dataloader)\n</code></pre>"},{"location":"en/api/tiger/#recommendation-generation","title":"Recommendation Generation","text":"<pre><code># Load trained model\nmodel = Tiger.load_from_checkpoint(\"checkpoints/tiger.ckpt\")\nmodel.eval()\n\n# User history sequence\nuser_sequence = torch.tensor([10, 25, 67, 89])  # Semantic ID sequence\n\n# Generate recommendations\nwith torch.no_grad():\n    recommendations = model.generate(\n        user_sequence.unsqueeze(0),\n        max_length=20,\n        temperature=0.8,\n        top_k=50\n    )\n\nprint(f\"Recommendations: {recommendations.squeeze().tolist()}\")\n</code></pre>"},{"location":"en/api/trainers/","title":"Trainers API Reference","text":"<p>Detailed documentation for training utilities and scripts.</p>"},{"location":"en/api/trainers/#base-trainer","title":"Base Trainer","text":""},{"location":"en/api/trainers/#basetrainer","title":"BaseTrainer","text":"<p>Base class for all trainers.</p> <pre><code>class BaseTrainer:\n    \"\"\"Base trainer class\"\"\"\n\n    def __init__(\n        self,\n        model: LightningModule,\n        config: TrainingConfig,\n        logger: Optional[Logger] = None\n    ):\n        self.model = model\n        self.config = config\n        self.logger = logger or self._create_default_logger()\n        self.trainer = None\n\n    def _create_default_logger(self) -&gt; Logger:\n        \"\"\"Create default logger\"\"\"\n        from pytorch_lightning.loggers import TensorBoardLogger\n\n        return TensorBoardLogger(\n            save_dir=self.config.log_dir,\n            name=self.config.experiment_name,\n            version=self.config.version\n        )\n</code></pre> <p>Methods:</p>"},{"location":"en/api/trainers/#setup_trainer","title":"setup_trainer()","text":"<p>Setup PyTorch Lightning trainer.</p> <pre><code>def setup_trainer(self) -&gt; None:\n    \"\"\"Setup trainer\"\"\"\n    from pytorch_lightning import Trainer\n    from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\n    # Callbacks\n    callbacks = []\n\n    # Checkpoint callback\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=self.config.checkpoint_dir,\n        filename='{epoch}-{val_loss:.2f}',\n        monitor='val_loss',\n        mode='min',\n        save_top_k=self.config.save_top_k,\n        save_last=True\n    )\n    callbacks.append(checkpoint_callback)\n\n    # Early stopping callback\n    if self.config.early_stopping_patience &gt; 0:\n        early_stop_callback = EarlyStopping(\n            monitor='val_loss',\n            patience=self.config.early_stopping_patience,\n            mode='min'\n        )\n        callbacks.append(early_stop_callback)\n\n    # Create trainer\n    self.trainer = Trainer(\n        max_epochs=self.config.max_epochs,\n        gpus=self.config.gpus,\n        precision=self.config.precision,\n        gradient_clip_val=self.config.gradient_clip_val,\n        accumulate_grad_batches=self.config.accumulate_grad_batches,\n        val_check_interval=self.config.val_check_interval,\n        callbacks=callbacks,\n        logger=self.logger,\n        deterministic=self.config.deterministic,\n        enable_progress_bar=self.config.progress_bar\n    )\n</code></pre>"},{"location":"en/api/trainers/#traintrain_dataloader-val_dataloader","title":"train(train_dataloader, val_dataloader)","text":"<p>Execute training.</p> <pre><code>def train(\n    self,\n    train_dataloader: DataLoader,\n    val_dataloader: Optional[DataLoader] = None\n) -&gt; None:\n    \"\"\"\n    Execute model training\n\n    Args:\n        train_dataloader: Training data loader\n        val_dataloader: Validation data loader\n    \"\"\"\n    if self.trainer is None:\n        self.setup_trainer()\n\n    self.trainer.fit(self.model, train_dataloader, val_dataloader)\n</code></pre>"},{"location":"en/api/trainers/#testtest_dataloader","title":"test(test_dataloader)","text":"<p>Execute testing.</p> <pre><code>def test(self, test_dataloader: DataLoader) -&gt; Dict[str, float]:\n    \"\"\"\n    Execute model testing\n\n    Args:\n        test_dataloader: Test data loader\n\n    Returns:\n        Test results dictionary\n    \"\"\"\n    if self.trainer is None:\n        self.setup_trainer()\n\n    results = self.trainer.test(self.model, test_dataloader)\n    return results[0] if results else {}\n</code></pre>"},{"location":"en/api/trainers/#rqvae-trainer","title":"RQVAE Trainer","text":""},{"location":"en/api/trainers/#rqvaetrainer","title":"RQVAETrainer","text":"<p>Trainer specifically for training RQVAE models.</p> <pre><code>class RQVAETrainer(BaseTrainer):\n    \"\"\"RQVAE trainer\"\"\"\n\n    def __init__(\n        self,\n        model: RqVae,\n        config: RQVAETrainingConfig,\n        dataset: ItemDataset,\n        logger: Optional[Logger] = None\n    ):\n        super().__init__(model, config, logger)\n        self.dataset = dataset\n\n    def create_dataloaders(self) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n        \"\"\"\n        Create data loaders\n\n        Returns:\n            (train_loader, val_loader, test_loader): Data loader tuple\n        \"\"\"\n        # Split dataset\n        train_size = int(0.8 * len(self.dataset))\n        val_size = int(0.1 * len(self.dataset))\n        test_size = len(self.dataset) - train_size - val_size\n\n        train_dataset, val_dataset, test_dataset = random_split(\n            self.dataset, \n            [train_size, val_size, test_size],\n            generator=torch.Generator().manual_seed(self.config.random_seed)\n        )\n\n        # Create data loaders\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            pin_memory=True\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=False,\n            num_workers=self.config.num_workers,\n            pin_memory=True\n        )\n\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=False,\n            num_workers=self.config.num_workers,\n            pin_memory=True\n        )\n\n        return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"en/api/trainers/#train_model","title":"train_model()","text":"<p>Train RQVAE model.</p> <pre><code>def train_model(self) -&gt; RqVae:\n    \"\"\"\n    Train RQVAE model\n\n    Returns:\n        Trained model\n    \"\"\"\n    # Create data loaders\n    train_loader, val_loader, test_loader = self.create_dataloaders()\n\n    # Execute training\n    self.train(train_loader, val_loader)\n\n    # Test model\n    if self.config.run_test:\n        test_results = self.test(test_loader)\n        print(f\"Test results: {test_results}\")\n\n    return self.model\n</code></pre>"},{"location":"en/api/trainers/#evaluate_reconstructiontest_dataloader","title":"evaluate_reconstruction(test_dataloader)","text":"<p>Evaluate reconstruction quality.</p> <pre><code>def evaluate_reconstruction(self, test_dataloader: DataLoader) -&gt; Dict[str, float]:\n    \"\"\"\n    Evaluate reconstruction quality\n\n    Args:\n        test_dataloader: Test data loader\n\n    Returns:\n        Evaluation metrics dictionary\n    \"\"\"\n    self.model.eval()\n    device = next(self.model.parameters()).device\n\n    total_mse = 0\n    total_cosine_sim = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            if isinstance(batch, dict):\n                features = batch['features'].to(device)\n            else:\n                features = batch.to(device)\n\n            # Forward pass\n            reconstructed, _, _, _ = self.model(features)\n\n            # Compute metrics\n            mse = F.mse_loss(reconstructed, features, reduction='sum')\n            cosine_sim = F.cosine_similarity(reconstructed, features, dim=1).sum()\n\n            total_mse += mse.item()\n            total_cosine_sim += cosine_sim.item()\n            total_samples += features.size(0)\n\n    return {\n        'mse': total_mse / total_samples,\n        'rmse': (total_mse / total_samples) ** 0.5,\n        'cosine_similarity': total_cosine_sim / total_samples\n    }\n</code></pre>"},{"location":"en/api/trainers/#generate_semantic_idsdataloader","title":"generate_semantic_ids(dataloader)","text":"<p>Generate semantic IDs for dataset.</p> <pre><code>def generate_semantic_ids(self, dataloader: DataLoader) -&gt; torch.Tensor:\n    \"\"\"\n    Generate semantic IDs for dataset\n\n    Args:\n        dataloader: Data loader\n\n    Returns:\n        Semantic ID tensor (num_samples,)\n    \"\"\"\n    self.model.eval()\n    device = next(self.model.parameters()).device\n\n    all_semantic_ids = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            if isinstance(batch, dict):\n                features = batch['features'].to(device)\n            else:\n                features = batch.to(device)\n\n            semantic_ids = self.model.generate_semantic_ids(features)\n            all_semantic_ids.append(semantic_ids.cpu())\n\n    return torch.cat(all_semantic_ids, dim=0)\n</code></pre>"},{"location":"en/api/trainers/#tiger-trainer","title":"TIGER Trainer","text":""},{"location":"en/api/trainers/#tigertrainer","title":"TIGERTrainer","text":"<p>Trainer specifically for training TIGER models.</p> <pre><code>class TIGERTrainer(BaseTrainer):\n    \"\"\"TIGER trainer\"\"\"\n\n    def __init__(\n        self,\n        model: Tiger,\n        config: TIGERTrainingConfig,\n        dataset: SequenceDataset,\n        logger: Optional[Logger] = None\n    ):\n        super().__init__(model, config, logger)\n        self.dataset = dataset\n        self.collate_fn = self._create_collate_fn()\n\n    def _create_collate_fn(self) -&gt; Callable:\n        \"\"\"Create data collation function\"\"\"\n        def collate_fn(batch):\n            # Extract sequence data\n            input_ids = [item['input_ids'] for item in batch]\n            labels = [item['labels'] for item in batch]\n\n            # Pad sequences\n            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n            labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n\n            # Create attention mask\n            attention_mask = (input_ids != 0).float()\n\n            return {\n                'input_ids': input_ids,\n                'labels': labels,\n                'attention_mask': attention_mask\n            }\n\n        return collate_fn\n</code></pre>"},{"location":"en/api/trainers/#create_dataloaders","title":"create_dataloaders()","text":"<p>Create TIGER data loaders.</p> <pre><code>def create_dataloaders(self) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"\n    Create data loaders\n\n    Returns:\n        (train_loader, val_loader, test_loader): Data loader tuple\n    \"\"\"\n    # Split dataset\n    train_size = int(0.8 * len(self.dataset))\n    val_size = int(0.1 * len(self.dataset))\n    test_size = len(self.dataset) - train_size - val_size\n\n    train_dataset, val_dataset, test_dataset = random_split(\n        self.dataset,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(self.config.random_seed)\n    )\n\n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=self.config.batch_size,\n        shuffle=True,\n        num_workers=self.config.num_workers,\n        collate_fn=self.collate_fn,\n        pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=self.config.batch_size,\n        shuffle=False,\n        num_workers=self.config.num_workers,\n        collate_fn=self.collate_fn,\n        pin_memory=True\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=self.config.batch_size,\n        shuffle=False,\n        num_workers=self.config.num_workers,\n        collate_fn=self.collate_fn,\n        pin_memory=True\n    )\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"en/api/trainers/#evaluate_generationtest_dataloader-k_values","title":"evaluate_generation(test_dataloader, k_values)","text":"<p>Evaluate generation quality.</p> <pre><code>def evaluate_generation(\n    self,\n    test_dataloader: DataLoader,\n    k_values: List[int] = [5, 10, 20]\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Evaluate generation quality\n\n    Args:\n        test_dataloader: Test data loader\n        k_values: Top-K value list\n\n    Returns:\n        Evaluation metrics dictionary\n    \"\"\"\n    self.model.eval()\n    device = next(self.model.parameters()).device\n\n    all_predictions = []\n    all_targets = []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n\n            # Generate recommendations\n            generated = self.model.generate(\n                input_ids,\n                max_length=self.config.max_generation_length,\n                temperature=self.config.generation_temperature,\n                top_k=self.config.generation_top_k\n            )\n\n            # Extract target sequences\n            targets = []\n            for label_seq in labels:\n                target = label_seq[label_seq != -100].cpu().tolist()\n                targets.append(target)\n\n            all_predictions.extend(generated.cpu().tolist())\n            all_targets.extend(targets)\n\n    # Compute metrics\n    metrics = {}\n    for k in k_values:\n        recall_k = self._compute_recall_at_k(all_predictions, all_targets, k)\n        ndcg_k = self._compute_ndcg_at_k(all_predictions, all_targets, k)\n\n        metrics[f'recall@{k}'] = recall_k\n        metrics[f'ndcg@{k}'] = ndcg_k\n\n    return metrics\n</code></pre>"},{"location":"en/api/trainers/#_compute_recall_at_kpredictions-targets-k","title":"_compute_recall_at_k(predictions, targets, k)","text":"<p>Compute Recall@K.</p> <pre><code>def _compute_recall_at_k(\n    self,\n    predictions: List[List[int]],\n    targets: List[List[int]],\n    k: int\n) -&gt; float:\n    \"\"\"Compute Recall@K\"\"\"\n    recall_scores = []\n\n    for pred, target in zip(predictions, targets):\n        if len(target) == 0:\n            continue\n\n        top_k_pred = set(pred[:k])\n        target_set = set(target)\n\n        recall = len(top_k_pred &amp; target_set) / len(target_set)\n        recall_scores.append(recall)\n\n    return np.mean(recall_scores) if recall_scores else 0.0\n</code></pre>"},{"location":"en/api/trainers/#_compute_ndcg_at_kpredictions-targets-k","title":"_compute_ndcg_at_k(predictions, targets, k)","text":"<p>Compute NDCG@K.</p> <pre><code>def _compute_ndcg_at_k(\n    self,\n    predictions: List[List[int]],\n    targets: List[List[int]],\n    k: int\n) -&gt; float:\n    \"\"\"Compute NDCG@K\"\"\"\n    ndcg_scores = []\n\n    for pred, target in zip(predictions, targets):\n        if len(target) == 0:\n            continue\n\n        # Compute DCG\n        dcg = 0\n        for i, item in enumerate(pred[:k]):\n            if item in target:\n                dcg += 1 / np.log2(i + 2)\n\n        # Compute IDCG\n        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(target), k)))\n\n        # Compute NDCG\n        ndcg = dcg / idcg if idcg &gt; 0 else 0\n        ndcg_scores.append(ndcg)\n\n    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n</code></pre>"},{"location":"en/api/trainers/#training-configurations","title":"Training Configurations","text":""},{"location":"en/api/trainers/#trainingconfig","title":"TrainingConfig","text":"<p>Base training configuration.</p> <pre><code>@dataclass\nclass TrainingConfig:\n    # Basic settings\n    max_epochs: int = 100\n    batch_size: int = 32\n    learning_rate: float = 1e-3\n\n    # Hardware settings\n    gpus: int = 1 if torch.cuda.is_available() else 0\n    precision: int = 32\n    num_workers: int = 4\n\n    # Training strategy\n    gradient_clip_val: float = 1.0\n    accumulate_grad_batches: int = 1\n    val_check_interval: float = 1.0\n\n    # Checkpoints and logging\n    checkpoint_dir: str = \"checkpoints\"\n    log_dir: str = \"logs\"\n    experiment_name: str = \"experiment\"\n    version: Optional[str] = None\n    save_top_k: int = 3\n\n    # Early stopping\n    early_stopping_patience: int = 10\n\n    # Others\n    deterministic: bool = True\n    random_seed: int = 42\n    progress_bar: bool = True\n    run_test: bool = True\n</code></pre>"},{"location":"en/api/trainers/#rqvaetrainingconfig","title":"RQVAETrainingConfig","text":"<p>RQVAE training configuration.</p> <pre><code>@dataclass\nclass RQVAETrainingConfig(TrainingConfig):\n    # Model parameters\n    input_dim: int = 768\n    hidden_dim: int = 512\n    latent_dim: int = 256\n    num_embeddings: int = 1024\n    commitment_cost: float = 0.25\n\n    # Training parameters\n    learning_rate: float = 1e-3\n    batch_size: int = 64\n    max_epochs: int = 100\n\n    # Dataset parameters\n    dataset_name: str = \"p5_amazon\"\n    dataset_split: str = \"beauty\"\n\n    # Evaluation parameters\n    eval_reconstruction: bool = True\n    eval_quantization: bool = True\n</code></pre>"},{"location":"en/api/trainers/#tigertrainingconfig","title":"TIGERTrainingConfig","text":"<p>TIGER training configuration.</p> <pre><code>@dataclass\nclass TIGERTrainingConfig(TrainingConfig):\n    # Model parameters\n    vocab_size: int = 1024\n    embedding_dim: int = 512\n    num_heads: int = 8\n    num_layers: int = 6\n    attn_dim: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 100\n\n    # Training parameters\n    learning_rate: float = 1e-4\n    batch_size: int = 16\n    max_epochs: int = 50\n\n    # Generation parameters\n    max_generation_length: int = 50\n    generation_temperature: float = 1.0\n    generation_top_k: int = 50\n    generation_top_p: float = 0.9\n\n    # Dataset parameters\n    dataset_name: str = \"p5_amazon\"\n    dataset_split: str = \"beauty\"\n    pretrained_rqvae_path: str = \"checkpoints/rqvae.ckpt\"\n\n    # Evaluation parameters\n    eval_generation: bool = True\n    eval_k_values: List[int] = field(default_factory=lambda: [5, 10, 20])\n</code></pre>"},{"location":"en/api/trainers/#training-scripts","title":"Training Scripts","text":""},{"location":"en/api/trainers/#train-rqvae","title":"Train RQVAE","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Script for training RQVAE model\"\"\"\n\nimport argparse\nfrom pathlib import Path\n\nfrom generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.dataset_factory import DatasetFactory\nfrom generative_recommenders.trainers import RQVAETrainer, RQVAETrainingConfig\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train RQVAE model\")\n    parser.add_argument(\"--dataset\", default=\"p5_amazon\", help=\"Dataset name\")\n    parser.add_argument(\"--split\", default=\"beauty\", help=\"Dataset split\")\n    parser.add_argument(\"--root\", default=\"dataset\", help=\"Dataset root directory\")\n    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n    parser.add_argument(\"--max_epochs\", type=int, default=100, help=\"Max epochs\")\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate\")\n    parser.add_argument(\"--checkpoint_dir\", default=\"checkpoints\", help=\"Checkpoint directory\")\n\n    args = parser.parse_args()\n\n    # Create configuration\n    config = RQVAETrainingConfig(\n        batch_size=args.batch_size,\n        max_epochs=args.max_epochs,\n        learning_rate=args.learning_rate,\n        checkpoint_dir=args.checkpoint_dir,\n        dataset_name=args.dataset,\n        dataset_split=args.split\n    )\n\n    # Create dataset\n    dataset = DatasetFactory.create_item_dataset(\n        args.dataset,\n        root=args.root,\n        split=args.split,\n        train_test_split=\"all\"\n    )\n\n    # Create model\n    model = RqVae(\n        input_dim=config.input_dim,\n        hidden_dim=config.hidden_dim,\n        latent_dim=config.latent_dim,\n        num_embeddings=config.num_embeddings,\n        commitment_cost=config.commitment_cost,\n        learning_rate=config.learning_rate\n    )\n\n    # Create trainer\n    trainer = RQVAETrainer(model, config, dataset)\n\n    # Train model\n    trained_model = trainer.train_model()\n\n    print(f\"Training completed. Model saved to {config.checkpoint_dir}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"en/api/trainers/#train-tiger","title":"Train TIGER","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Script for training TIGER model\"\"\"\n\nimport argparse\nfrom pathlib import Path\n\nfrom generative_recommenders.models.tiger import Tiger\nfrom generative_recommenders.data.dataset_factory import DatasetFactory\nfrom generative_recommenders.trainers import TIGERTrainer, TIGERTrainingConfig\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train TIGER model\")\n    parser.add_argument(\"--dataset\", default=\"p5_amazon\", help=\"Dataset name\")\n    parser.add_argument(\"--split\", default=\"beauty\", help=\"Dataset split\")\n    parser.add_argument(\"--root\", default=\"dataset\", help=\"Dataset root directory\")\n    parser.add_argument(\"--rqvae_path\", required=True, help=\"Pretrained RQVAE checkpoint path\")\n    parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size\")\n    parser.add_argument(\"--max_epochs\", type=int, default=50, help=\"Max epochs\")\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help=\"Learning rate\")\n    parser.add_argument(\"--checkpoint_dir\", default=\"checkpoints\", help=\"Checkpoint directory\")\n\n    args = parser.parse_args()\n\n    # Create configuration\n    config = TIGERTrainingConfig(\n        batch_size=args.batch_size,\n        max_epochs=args.max_epochs,\n        learning_rate=args.learning_rate,\n        checkpoint_dir=args.checkpoint_dir,\n        dataset_name=args.dataset,\n        dataset_split=args.split,\n        pretrained_rqvae_path=args.rqvae_path\n    )\n\n    # Create dataset\n    dataset = DatasetFactory.create_sequence_dataset(\n        args.dataset,\n        root=args.root,\n        split=args.split,\n        train_test_split=\"train\",\n        pretrained_rqvae_path=args.rqvae_path\n    )\n\n    # Create model\n    model = Tiger(\n        vocab_size=config.vocab_size,\n        embedding_dim=config.embedding_dim,\n        num_heads=config.num_heads,\n        num_layers=config.num_layers,\n        learning_rate=config.learning_rate\n    )\n\n    # Create trainer\n    trainer = TIGERTrainer(model, config, dataset)\n\n    # Train model\n    trained_model = trainer.train_model()\n\n    print(f\"Training completed. Model saved to {config.checkpoint_dir}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"en/api/trainers/#usage-examples","title":"Usage Examples","text":""},{"location":"en/api/trainers/#basic-training","title":"Basic Training","text":"<pre><code>from generative_recommenders.trainers import RQVAETrainer, RQVAETrainingConfig\nfrom generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.dataset_factory import DatasetFactory\n\n# Create configuration\nconfig = RQVAETrainingConfig(\n    batch_size=64,\n    max_epochs=100,\n    learning_rate=1e-3\n)\n\n# Create dataset\ndataset = DatasetFactory.create_item_dataset(\n    \"p5_amazon\",\n    root=\"dataset/amazon\",\n    split=\"beauty\"\n)\n\n# Create model\nmodel = RqVae(\n    input_dim=768,\n    hidden_dim=512,\n    num_embeddings=1024\n)\n\n# Create trainer and train\ntrainer = RQVAETrainer(model, config, dataset)\ntrained_model = trainer.train_model()\n</code></pre>"},{"location":"en/dataset/custom/","title":"Custom Datasets","text":"<p>This guide explains how to add custom datasets to the GenerativeRecommenders framework.</p>"},{"location":"en/dataset/custom/#basic-concepts","title":"Basic Concepts","text":""},{"location":"en/dataset/custom/#dataset-types","title":"Dataset Types","text":"<p>The framework supports two main dataset types:</p> <ol> <li>ItemDataset: Item-level datasets for training RQVAE</li> <li>SequenceDataset: Sequence-level datasets for training TIGER</li> </ol>"},{"location":"en/dataset/custom/#base-architecture","title":"Base Architecture","text":"<p>All datasets inherit from <code>BaseRecommenderDataset</code>:</p> <pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\n\nclass MyCustomDataset(BaseRecommenderDataset):\n    def __init__(self, config):\n        super().__init__(config)\n        # Initialize custom parameters\n\n    def download(self):\n        # Implement data download logic\n        pass\n\n    def load_raw_data(self):\n        # Load raw data files\n        pass\n\n    def preprocess_data(self, raw_data):\n        # Preprocess data\n        pass\n\n    def extract_items(self, processed_data):\n        # Extract item information\n        pass\n\n    def extract_interactions(self, processed_data):\n        # Extract user interaction information\n        pass\n</code></pre>"},{"location":"en/dataset/custom/#implementation-steps","title":"Implementation Steps","text":""},{"location":"en/dataset/custom/#1-create-configuration-class","title":"1. Create Configuration Class","text":"<p>First define dataset-specific configuration:</p> <pre><code>from dataclasses import dataclass\nfrom generative_recommenders.data.configs import DatasetConfig\n\n@dataclass\nclass MyDatasetConfig(DatasetConfig):\n    # Dataset-specific parameters\n    api_key: str = \"\"\n    data_format: str = \"json\"\n    categories: List[str] = None\n</code></pre>"},{"location":"en/dataset/custom/#2-implement-data-download","title":"2. Implement Data Download","text":"<pre><code>def download(self):\n    \"\"\"Download dataset to local storage\"\"\"\n    if self._data_exists():\n        return\n\n    print(\"Downloading custom dataset...\")\n\n    # Example: Download from API\n    import requests\n    response = requests.get(f\"https://api.example.com/data?key={self.config.api_key}\")\n\n    # Save locally\n    data_path = self.root_path / \"raw\" / \"data.json\"\n    data_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(data_path, 'w') as f:\n        json.dump(response.json(), f)\n\n    print(\"Download completed.\")\n\ndef _data_exists(self):\n    \"\"\"Check if data already exists\"\"\"\n    data_path = self.root_path / \"raw\" / \"data.json\"\n    return data_path.exists()\n</code></pre>"},{"location":"en/dataset/custom/#3-implement-data-loading","title":"3. Implement Data Loading","text":"<pre><code>def load_raw_data(self):\n    \"\"\"Load raw data\"\"\"\n    data_path = self.root_path / \"raw\" / \"data.json\"\n\n    with open(data_path, 'r') as f:\n        raw_data = json.load(f)\n\n    # Parse data structure\n    users = raw_data.get('users', [])\n    items = raw_data.get('items', [])\n    interactions = raw_data.get('interactions', [])\n\n    return {\n        \"users\": pd.DataFrame(users),\n        \"items\": pd.DataFrame(items), \n        \"interactions\": pd.DataFrame(interactions)\n    }\n</code></pre>"},{"location":"en/dataset/custom/#4-implement-data-preprocessing","title":"4. Implement Data Preprocessing","text":"<pre><code>def preprocess_data(self, raw_data):\n    \"\"\"Preprocess data\"\"\"\n    # Clean item data\n    items_df = self._clean_items(raw_data[\"items\"])\n\n    # Clean interaction data\n    interactions_df = self._clean_interactions(raw_data[\"interactions\"])\n\n    # Filter low-frequency users and items\n    interactions_df = self.filter_low_interactions(\n        interactions_df,\n        min_user_interactions=self.config.processing_config.min_user_interactions,\n        min_item_interactions=self.config.processing_config.min_item_interactions\n    )\n\n    # Process item features\n    items_df = self._process_item_features(items_df)\n\n    return {\n        \"items\": items_df,\n        \"interactions\": interactions_df\n    }\n\ndef _clean_items(self, items_df):\n    \"\"\"Clean item data\"\"\"\n    # Fill missing values\n    items_df[\"title\"] = items_df[\"title\"].fillna(\"Unknown\")\n    items_df[\"category\"] = items_df[\"category\"].fillna(\"Unknown\")\n\n    # Standardize text\n    items_df[\"title\"] = items_df[\"title\"].str.strip()\n\n    return items_df\n\ndef _clean_interactions(self, interactions_df):\n    \"\"\"Clean interaction data\"\"\"\n    # Remove duplicate interactions\n    interactions_df = interactions_df.drop_duplicates([\"user_id\", \"item_id\"])\n\n    # Add timestamp if missing\n    if \"timestamp\" not in interactions_df.columns:\n        interactions_df[\"timestamp\"] = range(len(interactions_df))\n\n    return interactions_df\n</code></pre>"},{"location":"en/dataset/custom/#5-implement-feature-extraction","title":"5. Implement Feature Extraction","text":"<pre><code>def _process_item_features(self, items_df):\n    \"\"\"Process item features\"\"\"\n    # Use text processor to generate embeddings\n    cache_key = f\"custom_dataset_{self.config.split}\"\n    embeddings = self.text_processor.encode_item_features(\n        items_df,\n        cache_key=cache_key,\n        force_reload=self.config.force_reload\n    )\n\n    # Add embedding features\n    items_df = items_df.copy()\n    items_df[\"features\"] = embeddings.tolist()\n\n    # Create text fields for reference\n    texts = []\n    for _, row in items_df.iterrows():\n        text = f\"Title: {row['title']}; Category: {row['category']}\"\n        texts.append(text)\n\n    items_df[\"text\"] = texts\n\n    return items_df\n\ndef extract_items(self, processed_data):\n    \"\"\"Extract item information\"\"\"\n    return processed_data[\"items\"]\n\ndef extract_interactions(self, processed_data):\n    \"\"\"Extract interaction information\"\"\"\n    return processed_data[\"interactions\"]\n</code></pre>"},{"location":"en/dataset/custom/#create-dataset-wrappers","title":"Create Dataset Wrappers","text":""},{"location":"en/dataset/custom/#item-dataset","title":"Item Dataset","text":"<pre><code>from generative_recommenders.data.base_dataset import ItemDataset\nimport gin\n\n@gin.configurable\nclass MyItemDataset(ItemDataset):\n    \"\"\"Custom item dataset\"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        split: str = \"default\",\n        train_test_split: str = \"all\",\n        api_key: str = \"\",\n        **kwargs\n    ):\n        # Create configuration\n        config = MyDatasetConfig(\n            root_dir=root,\n            split=split,\n            api_key=api_key,\n            **kwargs\n        )\n\n        # Create base dataset\n        base_dataset = MyCustomDataset(config)\n\n        # Initialize item dataset\n        super().__init__(\n            base_dataset=base_dataset,\n            split=train_test_split,\n            return_text=False\n        )\n</code></pre>"},{"location":"en/dataset/custom/#sequence-dataset","title":"Sequence Dataset","text":"<pre><code>from generative_recommenders.data.base_dataset import SequenceDataset\n\n@gin.configurable\nclass MySequenceDataset(SequenceDataset):\n    \"\"\"Custom sequence dataset\"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        split: str = \"default\",\n        train_test_split: str = \"train\",\n        api_key: str = \"\",\n        pretrained_rqvae_path: Optional[str] = None,\n        **kwargs\n    ):\n        # Create configuration\n        config = MyDatasetConfig(\n            root_dir=root,\n            split=split,\n            api_key=api_key,\n            **kwargs\n        )\n\n        # Load semantic encoder\n        semantic_encoder = None\n        if pretrained_rqvae_path:\n            from generative_recommenders.models.rqvae import RqVae\n            semantic_encoder = RqVae.load_from_checkpoint(pretrained_rqvae_path)\n            semantic_encoder.eval()\n\n        # Create base dataset\n        base_dataset = MyCustomDataset(config)\n\n        # Initialize sequence dataset\n        super().__init__(\n            base_dataset=base_dataset,\n            split=train_test_split,\n            semantic_encoder=semantic_encoder\n        )\n</code></pre>"},{"location":"en/dataset/custom/#register-dataset","title":"Register Dataset","text":""},{"location":"en/dataset/custom/#using-factory-pattern","title":"Using Factory Pattern","text":"<pre><code>from generative_recommenders.data.dataset_factory import DatasetFactory\n\n# Register dataset\nDatasetFactory.register_dataset(\n    \"my_dataset\",\n    base_class=MyCustomDataset,\n    item_class=MyItemDataset,\n    sequence_class=MySequenceDataset\n)\n\n# Use factory to create dataset\nitem_dataset = DatasetFactory.create_item_dataset(\n    \"my_dataset\",\n    root_dir=\"path/to/data\",\n    api_key=\"your_api_key\"\n)\n</code></pre>"},{"location":"en/dataset/custom/#configuration-file-integration","title":"Configuration File Integration","text":""},{"location":"en/dataset/custom/#gin-configuration-file","title":"Gin Configuration File","text":"<p>Create configuration file <code>config/my_dataset.gin</code>:</p> <pre><code>import my_module.my_dataset\n\n# Dataset parameters\ntrain.dataset_folder=\"dataset/my_dataset\"\ntrain.dataset=@MyItemDataset\n\n# Custom parameters\nMyItemDataset.api_key=\"your_api_key\"\nMyItemDataset.split=\"category_a\"\n\n# Text encoding parameters\nMyItemDataset.encoder_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n</code></pre>"},{"location":"en/dataset/custom/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"en/dataset/custom/#unit-tests","title":"Unit Tests","text":"<pre><code>import unittest\nfrom my_dataset import MyCustomDataset, MyDatasetConfig\n\nclass TestMyDataset(unittest.TestCase):\n    def setUp(self):\n        self.config = MyDatasetConfig(\n            root_dir=\"test_data\",\n            api_key=\"test_key\"\n        )\n        self.dataset = MyCustomDataset(self.config)\n\n    def test_data_loading(self):\n        \"\"\"Test data loading\"\"\"\n        # Mock data\n        raw_data = self.dataset.load_raw_data()\n        self.assertIn(\"items\", raw_data)\n        self.assertIn(\"interactions\", raw_data)\n\n    def test_preprocessing(self):\n        \"\"\"Test preprocessing\"\"\"\n        raw_data = {\"items\": pd.DataFrame(), \"interactions\": pd.DataFrame()}\n        processed = self.dataset.preprocess_data(raw_data)\n        self.assertIn(\"items\", processed)\n        self.assertIn(\"interactions\", processed)\n</code></pre>"},{"location":"en/dataset/custom/#data-quality-validation","title":"Data Quality Validation","text":"<pre><code>def validate_dataset(dataset):\n    \"\"\"Validate dataset quality\"\"\"\n    # Check data completeness\n    assert len(dataset) &gt; 0, \"Dataset is empty\"\n\n    # Check feature dimensions\n    sample = dataset[0]\n    assert len(sample) == 768, f\"Wrong feature dimension: {len(sample)}\"\n\n    # Check data types\n    assert isinstance(sample, list), \"Wrong data type\"\n\n    print(\"Dataset validation passed\")\n</code></pre>"},{"location":"en/dataset/custom/#best-practices","title":"Best Practices","text":""},{"location":"en/dataset/custom/#1-error-handling","title":"1. Error Handling","text":"<pre><code>def load_raw_data(self):\n    try:\n        # Data loading logic\n        return data\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Data file not found: {self.data_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Data loading failed: {str(e)}\")\n</code></pre>"},{"location":"en/dataset/custom/#2-logging","title":"2. Logging","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef preprocess_data(self, raw_data):\n    logger.info(\"Starting data preprocessing\")\n\n    # Preprocessing logic\n\n    logger.info(f\"Preprocessing completed, items: {len(items_df)}, interactions: {len(interactions_df)}\")\n</code></pre>"},{"location":"en/dataset/custom/#3-configuration-validation","title":"3. Configuration Validation","text":"<pre><code>def __post_init__(self):\n    super().__post_init__()\n\n    if not self.api_key:\n        raise ValueError(\"API key cannot be empty\")\n\n    if self.data_format not in [\"json\", \"csv\"]:\n        raise ValueError(f\"Unsupported data format: {self.data_format}\")\n</code></pre>"},{"location":"en/dataset/custom/#example-movielens-dataset","title":"Example: MovieLens Dataset","text":"<p>Complete MovieLens dataset implementation example:</p> <pre><code>@dataclass\nclass MovieLensConfig(DatasetConfig):\n    \"\"\"MovieLens dataset configuration\"\"\"\n    version: str = \"1m\"  # 1m, 10m, 20m\n\nclass MovieLensDataset(BaseRecommenderDataset):\n    \"\"\"MovieLens dataset implementation\"\"\"\n\n    URLS = {\n        \"1m\": \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\",\n        \"10m\": \"http://files.grouplens.org/datasets/movielens/ml-10m.zip\",\n    }\n\n    def download(self):\n        if self._data_exists():\n            return\n\n        url = self.URLS[self.config.version]\n        # Download and extract logic\n\n    def load_raw_data(self):\n        # Load ratings.dat, movies.dat, users.dat\n        pass\n\n    def preprocess_data(self, raw_data):\n        # MovieLens-specific preprocessing\n        pass\n</code></pre> <p>Through these steps, you can successfully add custom dataset support to the GenerativeRecommenders framework.</p>"},{"location":"en/dataset/overview/","title":"Dataset Overview","text":"<p>GenerativeRecommenders framework supports various recommendation system datasets and provides a flexible data processing pipeline.</p>"},{"location":"en/dataset/overview/#supported-datasets","title":"Supported Datasets","text":""},{"location":"en/dataset/overview/#p5-amazon-dataset","title":"P5 Amazon Dataset","text":"<p>P5 Amazon is a large-scale product recommendation dataset containing user reviews and product metadata.</p> <p>Features: - Multiple product categories (Beauty, Electronics, etc.) - Rich textual information (title, brand, category, price) - User interaction sequence data - Automatic download and preprocessing</p> <p>Usage: <pre><code>from generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n</code></pre></p>"},{"location":"en/dataset/overview/#data-processing-architecture","title":"Data Processing Architecture","text":""},{"location":"en/dataset/overview/#modular-design","title":"Modular Design","text":"<pre><code>graph TD\n    A[Raw Data] --&gt; B[Data Downloader]\n    B --&gt; C[Preprocessor]\n    C --&gt; D[Text Processor]\n    C --&gt; E[Sequence Processor]\n    D --&gt; F[Feature Embeddings]\n    E --&gt; G[Sequence Data]\n    F --&gt; H[Item Dataset]\n    G --&gt; I[Sequence Dataset]</code></pre>"},{"location":"en/dataset/overview/#configuration-system","title":"Configuration System","text":"<p>Use configuration classes to manage data processing parameters:</p> <pre><code>from generative_recommenders.data.configs import P5AmazonConfig, TextEncodingConfig\n\nconfig = P5AmazonConfig(\n    root_dir=\"dataset/amazon\",\n    split=\"beauty\",\n    text_config=TextEncodingConfig(\n        encoder_model=\"sentence-transformers/sentence-t5-xl\",\n        template=\"Title: {title}; Brand: {brand}\"\n    )\n)\n</code></pre>"},{"location":"en/dataset/overview/#data-processing-pipeline","title":"Data Processing Pipeline","text":""},{"location":"en/dataset/overview/#1-data-download","title":"1. Data Download","text":"<ul> <li>Automatic dataset download from cloud</li> <li>Extract and organize files</li> <li>Verify data integrity</li> </ul>"},{"location":"en/dataset/overview/#2-preprocessing","title":"2. Preprocessing","text":"<ul> <li>Data cleaning and normalization</li> <li>Handle missing values</li> <li>ID mapping and re-encoding</li> </ul>"},{"location":"en/dataset/overview/#3-feature-extraction","title":"3. Feature Extraction","text":"<ul> <li>Text feature encoding</li> <li>Categorical feature processing</li> <li>Numerical feature normalization</li> </ul>"},{"location":"en/dataset/overview/#4-sequence-generation","title":"4. Sequence Generation","text":"<ul> <li>Build user interaction sequences</li> <li>Temporal splitting (train/validation/test)</li> <li>Sequence padding and truncation</li> </ul>"},{"location":"en/dataset/overview/#custom-datasets","title":"Custom Datasets","text":"<p>To add new datasets, inherit from base class and implement necessary methods:</p> <pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\n\nclass MyDataset(BaseRecommenderDataset):\n    def download(self):\n        # Implement data download logic\n        pass\n\n    def load_raw_data(self):\n        # Load raw data\n        return {\"items\": items_df, \"interactions\": interactions_df}\n\n    def preprocess_data(self, raw_data):\n        # Data preprocessing\n        return processed_data\n\n    def extract_items(self, processed_data):\n        return processed_data[\"items\"]\n\n    def extract_interactions(self, processed_data):\n        return processed_data[\"interactions\"]\n</code></pre>"},{"location":"en/dataset/overview/#performance-optimization","title":"Performance Optimization","text":""},{"location":"en/dataset/overview/#caching-mechanism","title":"Caching Mechanism","text":"<ul> <li>Text encoding result caching</li> <li>Preprocessed data caching  </li> <li>Smart cache invalidation</li> </ul>"},{"location":"en/dataset/overview/#memory-management","title":"Memory Management","text":"<ul> <li>Lazy loading of large datasets</li> <li>Batch data processing</li> <li>Memory usage monitoring</li> </ul>"},{"location":"en/dataset/overview/#parallel-processing","title":"Parallel Processing","text":"<ul> <li>Multi-process data loading</li> <li>Parallel text encoding</li> <li>Distributed preprocessing</li> </ul>"},{"location":"en/dataset/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Data Validation: Always validate data quality and integrity</li> <li>Version Control: Track dataset versions and changes</li> <li>Documentation: Detailed recording of data processing steps</li> <li>Performance Monitoring: Monitor data loading and processing performance</li> </ol>"},{"location":"en/dataset/overview/#common-issues","title":"Common Issues","text":"<p>Q: How to handle large-scale datasets? A: Use chunked loading, caching, and parallel processing</p> <p>Q: How to add new text encoders? A: Inherit from TextProcessor class and implement custom encoding logic</p> <p>Q: How to handle different data formats? A: Create custom data loaders and preprocessing pipelines</p>"},{"location":"en/dataset/p5-amazon/","title":"P5 Amazon Dataset","text":"<p>The P5 Amazon dataset is a large-scale product recommendation dataset containing user reviews and product metadata from Amazon.</p>"},{"location":"en/dataset/p5-amazon/#overview","title":"Overview","text":"<p>P5 Amazon is derived from the Amazon product data and includes rich textual information about products along with user interaction histories. It's specifically designed for training generative recommendation models.</p> <p>Key Features: - Multiple product categories (Beauty, Electronics, Sports, etc.) - Rich product metadata (title, brand, category, price, description) - User interaction sequences with timestamps - Pre-processed text features suitable for neural models</p>"},{"location":"en/dataset/p5-amazon/#dataset-structure","title":"Dataset Structure","text":""},{"location":"en/dataset/p5-amazon/#data-files","title":"Data Files","text":"<p>When downloaded, the dataset contains: <pre><code>dataset/amazon/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 beauty_5.json.gz          # Raw interaction data\n\u2502   \u251c\u2500\u2500 meta_beauty.json.gz       # Product metadata\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 processed/\n\u2502   \u251c\u2500\u2500 items.parquet             # Processed item features\n\u2502   \u251c\u2500\u2500 interactions.parquet      # Processed interactions\n\u2502   \u2514\u2500\u2500 mappings.pkl              # ID mappings\n\u2514\u2500\u2500 cache/\n    \u2514\u2500\u2500 text_embeddings/          # Cached text embeddings\n</code></pre></p>"},{"location":"en/dataset/p5-amazon/#data-format","title":"Data Format","text":"<p>Items DataFrame: | Column | Type | Description | |--------|------|-------------| | item_id | int | Unique item identifier | | title | str | Product title | | brand | str | Product brand | | category | str | Product category | | price | float | Product price | | features | List[float] | Text embedding features (768-dim) | | text | str | Formatted text for reference |</p> <p>Interactions DataFrame: | Column | Type | Description | |--------|------|-------------| | user_id | int | Unique user identifier | | item_id | int | Item identifier | | rating | float | User rating (1-5) | | timestamp | int | Interaction timestamp |</p>"},{"location":"en/dataset/p5-amazon/#available-categories","title":"Available Categories","text":""},{"location":"en/dataset/p5-amazon/#beauty","title":"Beauty","text":"<ul> <li>Size: ~52K products, ~1.2M interactions</li> <li>Description: Cosmetics, skincare, haircare products</li> <li>Features: Rich brand and category information</li> </ul>"},{"location":"en/dataset/p5-amazon/#electronics","title":"Electronics","text":"<ul> <li>Size: ~63K products, ~1.7M interactions</li> <li>Description: Electronic devices, accessories, gadgets</li> <li>Features: Technical specifications in descriptions</li> </ul>"},{"location":"en/dataset/p5-amazon/#sports","title":"Sports","text":"<ul> <li>Size: ~35K products, ~296K interactions  </li> <li>Description: Sports equipment, outdoor gear, fitness products</li> <li>Features: Activity-specific categorization</li> </ul>"},{"location":"en/dataset/p5-amazon/#all-categories","title":"All Categories","text":"<ul> <li>Total: 29 categories available</li> <li>Combined Size: Multi-GB dataset</li> <li>Use Case: Large-scale experiments</li> </ul>"},{"location":"en/dataset/p5-amazon/#usage","title":"Usage","text":""},{"location":"en/dataset/p5-amazon/#basic-loading","title":"Basic Loading","text":"<pre><code>from generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\n# Load beauty category for item-level training\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n\nprint(f\"Dataset size: {len(dataset)}\")\nprint(f\"Feature dimension: {dataset[0].shape}\")\n</code></pre>"},{"location":"en/dataset/p5-amazon/#sequence-data","title":"Sequence Data","text":"<pre><code>from generative_recommenders.data.p5_amazon import P5AmazonSequenceDataset\n\n# Load for sequence modeling (requires pre-trained RQVAE)\nseq_dataset = P5AmazonSequenceDataset(\n    root=\"dataset/amazon\", \n    split=\"beauty\",\n    train_test_split=\"train\",\n    pretrained_rqvae_path=\"checkpoints/rqvae_beauty.ckpt\"\n)\n\n# Get a sample sequence\nsample = seq_dataset[0]\nprint(f\"Input sequence: {sample['input_ids']}\")\nprint(f\"Target sequence: {sample['labels']}\")\n</code></pre>"},{"location":"en/dataset/p5-amazon/#configuration-options","title":"Configuration Options","text":"<pre><code>from generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\",\n\n    # Text encoding options\n    encoder_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n    force_reload=False,  # Use cached embeddings if available\n\n    # Data filtering\n    min_user_interactions=5,\n    min_item_interactions=5,\n\n    # Text template\n    text_template=\"Title: {title}; Brand: {brand}; Category: {category}\"\n)\n</code></pre>"},{"location":"en/dataset/p5-amazon/#data-processing-pipeline","title":"Data Processing Pipeline","text":""},{"location":"en/dataset/p5-amazon/#1-download-and-extraction","title":"1. Download and Extraction","text":"<pre><code># Automatic download from UCSD repository\ndataset = P5AmazonItemDataset(root=\"dataset/amazon\", split=\"beauty\")\n# Downloads ~500MB for beauty category\n</code></pre>"},{"location":"en/dataset/p5-amazon/#2-text-processing","title":"2. Text Processing","text":"<p>The framework automatically processes product text using sentence transformers:</p> <pre><code># Default text template\ntemplate = \"Title: {title}; Brand: {brand}; Category: {category}; Price: {price}\"\n\n# Example processed text\n\"Title: Maybelline Mascara; Brand: Maybelline; Category: Beauty; Price: $8.99\"\n</code></pre>"},{"location":"en/dataset/p5-amazon/#3-feature-extraction","title":"3. Feature Extraction","text":"<ul> <li>Text Embeddings: 768-dimensional vectors from sentence transformers</li> <li>Caching: Embeddings cached for faster subsequent loading</li> <li>Normalization: L2 normalization applied by default</li> </ul>"},{"location":"en/dataset/p5-amazon/#4-sequence-building","title":"4. Sequence Building","text":"<p>For TIGER training, interactions are converted to sequences:</p> <pre><code># User interaction history\nuser_history = [item1, item2, item3, item4]\n\n# Converted to semantic ID sequence using RQVAE\nsemantic_sequence = [1, 45, 123, 67, 234, 189, 45, 123, 567, 234, 88, 192]\n#                   |--item1--| |--item2--| |--item3--| |--item4--|\n</code></pre>"},{"location":"en/dataset/p5-amazon/#statistics","title":"Statistics","text":""},{"location":"en/dataset/p5-amazon/#beauty-category","title":"Beauty Category","text":"<pre><code>Items: 52,024\nUsers: 40,226  \nInteractions: 1,235,316\nDensity: 0.059%\nAvg items per user: 30.7\nAvg users per item: 23.7\n</code></pre>"},{"location":"en/dataset/p5-amazon/#electronics-category","title":"Electronics Category","text":"<pre><code>Items: 63,001\nUsers: 192,403\nInteractions: 1,689,188\nDensity: 0.014%\nAvg items per user: 8.8\nAvg users per item: 26.8\n</code></pre>"},{"location":"en/dataset/p5-amazon/#data-quality","title":"Data Quality","text":""},{"location":"en/dataset/p5-amazon/#preprocessing-steps","title":"Preprocessing Steps","text":"<ol> <li>Duplicate Removal: Remove duplicate user-item interactions</li> <li>Low Activity Filtering: Filter users/items with &lt; 5 interactions</li> <li>Text Cleaning: Normalize titles, handle missing brands/categories</li> <li>Price Processing: Clean and standardize price formats</li> <li>ID Remapping: Create contiguous ID mappings</li> </ol>"},{"location":"en/dataset/p5-amazon/#quality-checks","title":"Quality Checks","text":"<pre><code>from generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\ndataset = P5AmazonItemDataset(root=\"dataset/amazon\", split=\"beauty\")\n\n# Check data quality\nitems_df, interactions_df = dataset.base_dataset.get_dataset()\n\nprint(\"Data Quality Report:\")\nprint(f\"Items with missing titles: {items_df['title'].isna().sum()}\")\nprint(f\"Items with missing brands: {items_df['brand'].isna().sum()}\")\nprint(f\"Interactions with valid ratings: {(interactions_df['rating'] &gt; 0).sum()}\")\nprint(f\"Feature vector dimension: {len(items_df.iloc[0]['features'])}\")\n</code></pre>"},{"location":"en/dataset/p5-amazon/#advanced-usage","title":"Advanced Usage","text":""},{"location":"en/dataset/p5-amazon/#custom-text-templates","title":"Custom Text Templates","text":"<pre><code># Product-focused template\ntemplate = \"Product: {title} from {brand} in {category} category\"\n\n# Price-aware template  \ntemplate = \"Buy {title} by {brand} for ${price} in {category}\"\n\n# Minimal template\ntemplate = \"{title} - {brand}\"\n</code></pre>"},{"location":"en/dataset/p5-amazon/#batch-processing","title":"Batch Processing","text":"<pre><code>from torch.utils.data import DataLoader\n\ndataset = P5AmazonItemDataset(root=\"dataset/amazon\", split=\"beauty\")\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n\nfor batch in dataloader:\n    # batch shape: (128, 768)\n    features = batch\n    # Process batch...\n</code></pre>"},{"location":"en/dataset/p5-amazon/#multi-category-loading","title":"Multi-Category Loading","text":"<pre><code># Load multiple categories\ncategories = [\"beauty\", \"electronics\", \"sports\"]\ndatasets = []\n\nfor category in categories:\n    dataset = P5AmazonItemDataset(\n        root=\"dataset/amazon\",\n        split=category,\n        train_test_split=\"train\"\n    )\n    datasets.append(dataset)\n\n# Combine datasets\nfrom torch.utils.data import ConcatDataset\ncombined_dataset = ConcatDataset(datasets)\n</code></pre>"},{"location":"en/dataset/p5-amazon/#troubleshooting","title":"Troubleshooting","text":""},{"location":"en/dataset/p5-amazon/#common-issues","title":"Common Issues","text":"<p>Q: Download fails with network error A: Check internet connection and try again. The files are large (100MB-2GB).</p> <p>Q: Text encoding takes very long A: Use cached embeddings by setting <code>force_reload=False</code> and ensure cache directory is writable.</p> <p>Q: Out of memory during loading A: Reduce batch size or use a smaller category like \"beauty\" instead of \"all\".</p> <p>Q: Missing brand/category information A: This is normal - the dataset fills missing values with \"Unknown\".</p>"},{"location":"en/dataset/p5-amazon/#performance-tips","title":"Performance Tips","text":"<pre><code># Use caching for faster subsequent loads\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\", \n    force_reload=False  # Use cache\n)\n\n# Use lighter text encoder for faster processing\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    encoder_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # Smaller model\n)\n\n# Process in smaller batches\nfrom generative_recommenders.data.configs import TextEncodingConfig\ntext_config = TextEncodingConfig(batch_size=16)  # Reduce from default 32\n</code></pre>"},{"location":"en/dataset/p5-amazon/#citation","title":"Citation","text":"<p>If you use the P5 Amazon dataset, please cite:</p> <pre><code>@article{geng2022recommendation,\n  title={Recommendation as language processing (rlp): A unified pretrain, personalized prompt \\&amp; predict paradigm (p5)},\n  author={Geng, Shijie and Liu, Shuchang and Fu, Zuohui and Ge, Yingqiang and Zhang, Yongfeng},\n  journal={arXiv preprint arXiv:2203.13366},\n  year={2022}\n}\n</code></pre>"},{"location":"en/dataset/p5-amazon/#related-documentation","title":"Related Documentation","text":"<ul> <li>Dataset Overview - General dataset concepts</li> <li>Custom Datasets - Creating your own datasets  </li> <li>RQVAE Training - Training item encoders</li> <li>TIGER Training - Training sequence models</li> </ul>"},{"location":"en/models/rqvae/","title":"RQVAE Model","text":"<p>Residual Quantized Variational Autoencoder (RQVAE) is a deep learning model for learning semantic representations of items in recommender systems.</p>"},{"location":"en/models/rqvae/#architecture","title":"Architecture","text":""},{"location":"en/models/rqvae/#core-concept","title":"Core Concept","text":"<p>RQVAE learns to encode item features into discrete semantic IDs through: - Encoder: Maps item features to continuous latent space - Quantizer: Discretizes continuous representations into semantic IDs - Decoder: Reconstructs original features from semantic IDs</p> <pre><code>graph TD\n    A[Item Features] --&gt; B[Encoder]\n    B --&gt; C[Quantization]\n    C --&gt; D[Semantic IDs]\n    C --&gt; E[Decoder]\n    E --&gt; F[Reconstructed Features]</code></pre>"},{"location":"en/models/rqvae/#model-components","title":"Model Components","text":""},{"location":"en/models/rqvae/#1-encoder-network","title":"1. Encoder Network","text":"<pre><code>class Encoder(nn.Module):\n    \"\"\"Feature encoder network\"\"\"\n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, latent_dim)\n        )\n\n    def forward(self, features):\n        return self.layers(features)\n</code></pre>"},{"location":"en/models/rqvae/#2-vector-quantization","title":"2. Vector Quantization","text":"<pre><code>class VectorQuantizer(nn.Module):\n    \"\"\"Vector quantization layer\"\"\"\n    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.commitment_cost = commitment_cost\n\n        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n\n    def forward(self, inputs):\n        # Calculate distances\n        distances = torch.cdist(inputs, self.embeddings.weight)\n\n        # Get closest embeddings\n        encoding_indices = torch.argmin(distances, dim=-1)\n        quantized = self.embeddings(encoding_indices)\n\n        # Calculate losses\n        commitment_loss = F.mse_loss(quantized.detach(), inputs)\n        embedding_loss = F.mse_loss(quantized, inputs.detach())\n\n        quantized = inputs + (quantized - inputs).detach()\n\n        return quantized, commitment_loss, embedding_loss, encoding_indices\n</code></pre>"},{"location":"en/models/rqvae/#3-decoder-network","title":"3. Decoder Network","text":"<pre><code>class Decoder(nn.Module):\n    \"\"\"Feature reconstruction decoder\"\"\"\n    def __init__(self, latent_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, latent):\n        return self.layers(latent)\n</code></pre>"},{"location":"en/models/rqvae/#training-process","title":"Training Process","text":""},{"location":"en/models/rqvae/#loss-function","title":"Loss Function","text":"<p>RQVAE uses a combination of reconstruction loss and quantization losses:</p> <pre><code>def compute_loss(self, features, reconstructed, commitment_loss, embedding_loss):\n    # Reconstruction loss\n    recon_loss = F.mse_loss(reconstructed, features)\n\n    # Total loss\n    total_loss = (\n        recon_loss + \n        self.commitment_cost * commitment_loss + \n        embedding_loss\n    )\n\n    return {\n        'total_loss': total_loss,\n        'reconstruction_loss': recon_loss,\n        'commitment_loss': commitment_loss,\n        'embedding_loss': embedding_loss\n    }\n</code></pre>"},{"location":"en/models/rqvae/#training-loop","title":"Training Loop","text":"<pre><code>def train_step(model, batch, optimizer):\n    optimizer.zero_grad()\n\n    # Forward pass\n    features = batch['features']\n    reconstructed, commitment_loss, embedding_loss, sem_ids = model(features)\n\n    # Compute losses\n    losses = model.compute_loss(features, reconstructed, commitment_loss, embedding_loss)\n\n    # Backward pass\n    losses['total_loss'].backward()\n    optimizer.step()\n\n    return losses, sem_ids\n</code></pre>"},{"location":"en/models/rqvae/#semantic-id-generation","title":"Semantic ID Generation","text":""},{"location":"en/models/rqvae/#item-to-semantic-id-mapping","title":"Item to Semantic ID Mapping","text":"<pre><code>def generate_semantic_ids(model, item_features):\n    \"\"\"Generate semantic IDs for items\"\"\"\n    model.eval()\n\n    with torch.no_grad():\n        encoded = model.encoder(item_features)\n        _, _, _, sem_ids = model.quantizer(encoded)\n\n    return sem_ids\n</code></pre>"},{"location":"en/models/rqvae/#batch-processing","title":"Batch Processing","text":"<pre><code>def batch_generate_semantic_ids(model, dataloader):\n    \"\"\"Generate semantic IDs for entire dataset\"\"\"\n    model.eval()\n    all_sem_ids = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            features = batch['features']\n            sem_ids = model.generate_semantic_ids(features)\n            all_sem_ids.append(sem_ids)\n\n    return torch.cat(all_sem_ids, dim=0)\n</code></pre>"},{"location":"en/models/rqvae/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"en/models/rqvae/#reconstruction-quality","title":"Reconstruction Quality","text":"<pre><code>def compute_reconstruction_metrics(original, reconstructed):\n    \"\"\"Compute reconstruction quality metrics\"\"\"\n    mse = F.mse_loss(reconstructed, original)\n    mae = F.l1_loss(reconstructed, original)\n\n    # Cosine similarity\n    cos_sim = F.cosine_similarity(original, reconstructed, dim=-1).mean()\n\n    return {\n        'mse': mse.item(),\n        'mae': mae.item(),\n        'cosine_similarity': cos_sim.item()\n    }\n</code></pre>"},{"location":"en/models/rqvae/#quantization-metrics","title":"Quantization Metrics","text":"<pre><code>def compute_quantization_metrics(model, dataloader):\n    \"\"\"Compute quantization quality metrics\"\"\"\n    model.eval()\n\n    total_perplexity = 0\n    total_usage = torch.zeros(model.quantizer.num_embeddings)\n\n    with torch.no_grad():\n        for batch in dataloader:\n            features = batch['features']\n            _, _, _, encoding_indices = model(features)\n\n            # Compute perplexity\n            avg_probs = torch.bincount(encoding_indices.flatten(), \n                                     minlength=model.quantizer.num_embeddings).float()\n            avg_probs = avg_probs / avg_probs.sum()\n            perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n            total_perplexity += perplexity\n            total_usage += avg_probs\n\n    # Usage statistics\n    active_codes = (total_usage &gt; 0).sum().item()\n    usage_entropy = -torch.sum(total_usage * torch.log(total_usage + 1e-10))\n\n    return {\n        'perplexity': total_perplexity / len(dataloader),\n        'active_codes': active_codes,\n        'usage_entropy': usage_entropy.item()\n    }\n</code></pre>"},{"location":"en/models/rqvae/#model-optimization","title":"Model Optimization","text":""},{"location":"en/models/rqvae/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<pre><code>class WarmupScheduler:\n    \"\"\"Warmup learning rate scheduler\"\"\"\n    def __init__(self, optimizer, warmup_steps, max_lr):\n        self.optimizer = optimizer\n        self.warmup_steps = warmup_steps\n        self.max_lr = max_lr\n        self.step_count = 0\n\n    def step(self):\n        self.step_count += 1\n        if self.step_count &lt;= self.warmup_steps:\n            lr = self.max_lr * self.step_count / self.warmup_steps\n        else:\n            lr = self.max_lr * 0.95 ** ((self.step_count - self.warmup_steps) // 1000)\n\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n</code></pre>"},{"location":"en/models/rqvae/#gradient-clipping","title":"Gradient Clipping","text":"<pre><code>def train_with_gradient_clipping(model, dataloader, optimizer, max_grad_norm=1.0):\n    \"\"\"Training with gradient clipping\"\"\"\n    for batch in dataloader:\n        optimizer.zero_grad()\n\n        # Forward pass\n        losses, _ = model.train_step(batch)\n\n        # Backward pass with gradient clipping\n        losses['total_loss'].backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n        optimizer.step()\n</code></pre>"},{"location":"en/models/rqvae/#advanced-features","title":"Advanced Features","text":""},{"location":"en/models/rqvae/#multiple-quantizers","title":"Multiple Quantizers","text":"<pre><code>class MultiQuantizer(nn.Module):\n    \"\"\"Multiple quantization layers\"\"\"\n    def __init__(self, num_quantizers, num_embeddings, embedding_dim):\n        super().__init__()\n        self.quantizers = nn.ModuleList([\n            VectorQuantizer(num_embeddings, embedding_dim)\n            for _ in range(num_quantizers)\n        ])\n\n    def forward(self, inputs):\n        quantized_list = []\n        commitment_losses = []\n        embedding_losses = []\n        sem_ids_list = []\n\n        current_input = inputs\n        for quantizer in self.quantizers:\n            quantized, commitment_loss, embedding_loss, sem_ids = quantizer(current_input)\n\n            quantized_list.append(quantized)\n            commitment_losses.append(commitment_loss)\n            embedding_losses.append(embedding_loss)\n            sem_ids_list.append(sem_ids)\n\n            # Residual connection\n            current_input = current_input - quantized.detach()\n\n        # Combine quantized representations\n        final_quantized = sum(quantized_list)\n        total_commitment_loss = sum(commitment_losses)\n        total_embedding_loss = sum(embedding_losses)\n\n        return final_quantized, total_commitment_loss, total_embedding_loss, sem_ids_list\n</code></pre>"},{"location":"en/models/rqvae/#exponential-moving-average","title":"Exponential Moving Average","text":"<pre><code>class EMAQuantizer(VectorQuantizer):\n    \"\"\"Quantizer with exponential moving average updates\"\"\"\n    def __init__(self, num_embeddings, embedding_dim, decay=0.99):\n        super().__init__(num_embeddings, embedding_dim)\n        self.decay = decay\n        self.register_buffer('cluster_size', torch.zeros(num_embeddings))\n        self.register_buffer('embed_avg', self.embeddings.weight.clone())\n\n    def forward(self, inputs):\n        quantized, commitment_loss, _, encoding_indices = super().forward(inputs)\n\n        if self.training:\n            # Update EMA\n            encodings = F.one_hot(encoding_indices, self.num_embeddings).float()\n            self.cluster_size.mul_(self.decay).add_(\n                encodings.sum(0), alpha=1 - self.decay\n            )\n\n            embed_sum = encodings.transpose(0, 1) @ inputs\n            self.embed_avg.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n\n            # Normalize\n            n = self.cluster_size.sum()\n            cluster_size = (\n                (self.cluster_size + 1e-5) / (n + self.num_embeddings * 1e-5) * n\n            )\n            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n            self.embeddings.weight.data.copy_(embed_normalized)\n\n        return quantized, commitment_loss, torch.tensor(0.0), encoding_indices\n</code></pre>"},{"location":"en/models/rqvae/#practical-applications","title":"Practical Applications","text":""},{"location":"en/models/rqvae/#item-similarity","title":"Item Similarity","text":"<pre><code>def compute_item_similarity(model, item_features_1, item_features_2):\n    \"\"\"Compute similarity between items using semantic IDs\"\"\"\n    sem_ids_1 = model.generate_semantic_ids(item_features_1)\n    sem_ids_2 = model.generate_semantic_ids(item_features_2)\n\n    # Exact match similarity\n    exact_match = (sem_ids_1 == sem_ids_2).float().mean()\n\n    # Embedding space similarity\n    embed_1 = model.quantizer.embeddings(sem_ids_1)\n    embed_2 = model.quantizer.embeddings(sem_ids_2)\n    cosine_sim = F.cosine_similarity(embed_1, embed_2, dim=-1).mean()\n\n    return {\n        'exact_match': exact_match.item(),\n        'embedding_similarity': cosine_sim.item()\n    }\n</code></pre>"},{"location":"en/models/rqvae/#recommendation-system-integration","title":"Recommendation System Integration","text":"<pre><code>class RQVAERecommender:\n    \"\"\"RQVAE-based recommender system\"\"\"\n    def __init__(self, rqvae_model):\n        self.rqvae = rqvae_model\n        self.rqvae.eval()\n\n    def encode_items(self, item_features):\n        \"\"\"Encode items to semantic IDs\"\"\"\n        return self.rqvae.generate_semantic_ids(item_features)\n\n    def find_similar_items(self, query_item_features, item_database, top_k=10):\n        \"\"\"Find similar items using semantic IDs\"\"\"\n        query_sem_ids = self.encode_items(query_item_features)\n        db_sem_ids = self.encode_items(item_database)\n\n        # Compute similarities\n        similarities = []\n        for i, db_sem_id in enumerate(db_sem_ids):\n            sim = (query_sem_ids == db_sem_id).float().mean()\n            similarities.append((i, sim.item()))\n\n        # Sort and return top-k\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return similarities[:top_k]\n</code></pre> <p>RQVAE provides a powerful foundation for learning discrete semantic representations that can be effectively used in generative recommendation models like TIGER.</p>"},{"location":"en/models/tiger/","title":"TIGER Model","text":"<p>TIGER (Recommender Systems with Generative Retrieval) is a Transformer-based generative recommendation model that performs item recommendation through sequence modeling.</p>"},{"location":"en/models/tiger/#model-architecture","title":"Model Architecture","text":""},{"location":"en/models/tiger/#core-concept","title":"Core Concept","text":"<p>TIGER transforms recommendation into a sequence generation problem: - Input: User interaction history (semantic ID sequence) - Output: Recommended item semantic ID sequence</p> <pre><code>graph TD\n    A[User History Sequence] --&gt; B[Semantic ID Encoding]\n    B --&gt; C[Transformer Encoder]\n    C --&gt; D[Transformer Decoder]\n    D --&gt; E[Generate Recommendation Sequence]\n    E --&gt; F[Semantic ID Decoding]\n    F --&gt; G[Recommended Items]</code></pre>"},{"location":"en/models/tiger/#model-components","title":"Model Components","text":""},{"location":"en/models/tiger/#1-embedding-layer","title":"1. Embedding Layer","text":"<pre><code>class SemIdEmbedding(nn.Module):\n    \"\"\"Semantic ID embedding layer\"\"\"\n    def __init__(self, num_embeddings, embedding_dim, sem_id_dim=3):\n        super().__init__()\n        self.sem_id_dim = sem_id_dim\n        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n\n    def forward(self, sem_ids):\n        # sem_ids: (batch_size, seq_len, sem_id_dim)\n        embedded = self.embedding(sem_ids)\n        return embedded.mean(dim=-2)  # Aggregate semantic ID dimension\n</code></pre>"},{"location":"en/models/tiger/#2-transformer-architecture","title":"2. Transformer Architecture","text":"<pre><code>class TransformerEncoderDecoder(nn.Module):\n    \"\"\"Transformer encoder-decoder\"\"\"\n    def __init__(self, config):\n        super().__init__()\n\n        # Encoder\n        encoder_layer = TransformerEncoderLayer(\n            d_model=config.embedding_dim,\n            nhead=config.num_heads,\n            dim_feedforward=config.attn_dim,\n            dropout=config.dropout\n        )\n        self.encoder = TransformerEncoder(encoder_layer, config.n_layers)\n\n        # Decoder  \n        decoder_layer = TransformerDecoderLayer(\n            d_model=config.embedding_dim,\n            nhead=config.num_heads,\n            dim_feedforward=config.attn_dim,\n            dropout=config.dropout\n        )\n        self.decoder = TransformerDecoder(decoder_layer, config.n_layers)\n</code></pre>"},{"location":"en/models/tiger/#3-constrained-generation","title":"3. Constrained Generation","text":"<p>TIGER uses Trie data structure to constrain generation:</p> <pre><code>class TrieNode(defaultdict):\n    \"\"\"Trie node\"\"\"\n    def __init__(self):\n        super().__init__(TrieNode)\n        self.is_end = False\n\ndef build_trie(valid_item_ids):\n    \"\"\"Build Trie of valid item IDs\"\"\"\n    root = TrieNode()\n    for seq in valid_item_ids.tolist():\n        node = root\n        for token in seq:\n            node = node[token]\n        node.is_end = True\n    return root\n</code></pre>"},{"location":"en/models/tiger/#semantic-id-mapping","title":"Semantic ID Mapping","text":""},{"location":"en/models/tiger/#from-items-to-semantic-ids","title":"From Items to Semantic IDs","text":"<p>TIGER uses pretrained RQVAE to map item features to semantic IDs:</p> <pre><code># Item features -&gt; Semantic IDs\nitem_features = torch.tensor([...])  # (768,)\nrqvae_output = rqvae(item_features)\nsemantic_ids = rqvae_output.sem_ids  # (3,) Three semantic IDs\n</code></pre>"},{"location":"en/models/tiger/#semantic-id-sequences","title":"Semantic ID Sequences","text":"<p>User interaction history is converted to semantic ID sequences:</p> <pre><code>user_history = [item1, item2, item3, ...]\nsemantic_sequence = []\n\nfor item_id in user_history:\n    item_sem_ids = rqvae.get_semantic_ids(item_features[item_id])\n    semantic_sequence.extend(item_sem_ids.tolist())\n\n# Result: [id1, id2, id3, id4, id5, id6, ...]\n</code></pre>"},{"location":"en/models/tiger/#training-process","title":"Training Process","text":""},{"location":"en/models/tiger/#data-preparation","title":"Data Preparation","text":"<pre><code>class SeqData(NamedTuple):\n    \"\"\"Sequence data format\"\"\"\n    user_id: int\n    item_ids: List[int]      # Input sequence (semantic IDs)\n    target_ids: List[int]    # Target sequence (semantic IDs)\n</code></pre>"},{"location":"en/models/tiger/#loss-function","title":"Loss Function","text":"<p>Uses cross-entropy loss for sequence modeling:</p> <pre><code>def compute_loss(logits, target_ids, mask):\n    \"\"\"Compute sequence modeling loss\"\"\"\n    # logits: (batch_size, seq_len, vocab_size)\n    # target_ids: (batch_size, seq_len)\n    # mask: (batch_size, seq_len)\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n\n    # Reshape to 2D\n    logits_flat = logits.view(-1, logits.size(-1))\n    target_flat = target_ids.view(-1)\n\n    # Compute loss\n    loss = loss_fn(logits_flat, target_flat)\n\n    return loss\n</code></pre>"},{"location":"en/models/tiger/#training-loop","title":"Training Loop","text":"<pre><code>def train_step(model, batch, optimizer):\n    \"\"\"Single training step\"\"\"\n    optimizer.zero_grad()\n\n    # Forward pass\n    user_ids = batch[\"user_input_ids\"]\n    item_ids = batch[\"item_input_ids\"] \n    target_ids = batch[\"target_input_ids\"]\n\n    logits = model(user_ids, item_ids)\n\n    # Compute loss\n    loss = compute_loss(logits, target_ids, batch[\"seq_mask\"])\n\n    # Backward pass\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n</code></pre>"},{"location":"en/models/tiger/#inference-process","title":"Inference Process","text":""},{"location":"en/models/tiger/#generative-recommendation","title":"Generative Recommendation","text":"<pre><code>def generate_recommendations(model, user_sequence, max_length=10):\n    \"\"\"Generate recommendation sequence\"\"\"\n    model.eval()\n\n    with torch.no_grad():\n        # Encode user sequence\n        encoded = model.encode_sequence(user_sequence)\n\n        # Autoregressive generation\n        generated = []\n        current_input = encoded\n\n        for _ in range(max_length):\n            # Predict next token\n            logits = model.decode_step(current_input)\n            next_token = torch.argmax(logits, dim=-1)\n\n            generated.append(next_token.item())\n\n            # Update input\n            current_input = torch.cat([current_input, next_token.unsqueeze(0)])\n\n    return generated\n</code></pre>"},{"location":"en/models/tiger/#trie-constrained-generation","title":"Trie-Constrained Generation","text":"<pre><code>def generate_with_trie_constraint(model, user_sequence, trie, max_length=10):\n    \"\"\"Generation with Trie constraints\"\"\"\n    model.eval()\n\n    generated = []\n    current_node = trie\n\n    with torch.no_grad():\n        for step in range(max_length):\n            # Get valid next tokens\n            valid_tokens = list(current_node.keys())\n            if not valid_tokens:\n                break\n\n            # Predict and constrain\n            logits = model.decode_step(user_sequence + generated)\n            masked_logits = mask_invalid_tokens(logits, valid_tokens)\n\n            next_token = torch.argmax(masked_logits, dim=-1).item()\n            generated.append(next_token)\n\n            # Update Trie position\n            current_node = current_node[next_token]\n\n            # Check if complete item ID\n            if current_node.is_end:\n                break\n\n    return generated\n</code></pre>"},{"location":"en/models/tiger/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"en/models/tiger/#top-k-recommendation-metrics","title":"Top-K Recommendation Metrics","text":"<pre><code>def compute_recall_at_k(predictions, targets, k=10):\n    \"\"\"Compute Recall@K\"\"\"\n    recall_scores = []\n\n    for pred, target in zip(predictions, targets):\n        # Get top-k predictions\n        top_k_pred = set(pred[:k])\n        target_set = set(target)\n\n        # Compute recall\n        if len(target_set) &gt; 0:\n            recall = len(top_k_pred &amp; target_set) / len(target_set)\n            recall_scores.append(recall)\n\n    return np.mean(recall_scores)\n\ndef compute_ndcg_at_k(predictions, targets, k=10):\n    \"\"\"Compute NDCG@K\"\"\"\n    ndcg_scores = []\n\n    for pred, target in zip(predictions, targets):\n        # Compute DCG\n        dcg = 0\n        for i, item in enumerate(pred[:k]):\n            if item in target:\n                dcg += 1 / np.log2(i + 2)  # +2 because log2(1)=0\n\n        # Compute IDCG\n        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(target), k)))\n\n        # Compute NDCG\n        ndcg = dcg / idcg if idcg &gt; 0 else 0\n        ndcg_scores.append(ndcg)\n\n    return np.mean(ndcg_scores)\n</code></pre>"},{"location":"en/models/tiger/#diversity-metrics","title":"Diversity Metrics","text":"<pre><code>def compute_diversity(recommendations):\n    \"\"\"Compute recommendation diversity\"\"\"\n    all_items = set()\n    for rec_list in recommendations:\n        all_items.update(rec_list)\n\n    # Item coverage\n    coverage = len(all_items) / total_items\n\n    # Gini coefficient\n    item_counts = defaultdict(int)\n    for rec_list in recommendations:\n        for item in rec_list:\n            item_counts[item] += 1\n\n    counts = list(item_counts.values())\n    gini = compute_gini_coefficient(counts)\n\n    return {\"coverage\": coverage, \"gini\": gini}\n</code></pre>"},{"location":"en/models/tiger/#model-optimization","title":"Model Optimization","text":""},{"location":"en/models/tiger/#positional-encoding","title":"Positional Encoding","text":"<pre><code>class PositionalEncoding(nn.Module):\n    \"\"\"Positional encoding\"\"\"\n    def __init__(self, d_model, max_seq_length=5000):\n        super().__init__()\n\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n</code></pre>"},{"location":"en/models/tiger/#attention-mechanism-optimization","title":"Attention Mechanism Optimization","text":"<pre><code>class MultiHeadAttention(nn.Module):\n    \"\"\"Optimized multi-head attention\"\"\"\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0\n\n        self.d_k = d_model // num_heads\n        self.num_heads = num_heads\n\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # Linear transformations\n        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n        # Scaled dot-product attention\n        attention, weights = scaled_dot_product_attention(Q, K, V, mask, self.dropout)\n\n        # Concatenate heads\n        attention = attention.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.num_heads * self.d_k\n        )\n\n        output = self.w_o(attention)\n        return output, weights\n</code></pre>"},{"location":"en/models/tiger/#advanced-features","title":"Advanced Features","text":""},{"location":"en/models/tiger/#user-behavior-modeling","title":"User Behavior Modeling","text":"<pre><code>class UserBehaviorEncoder(nn.Module):\n    \"\"\"User behavior encoder\"\"\"\n    def __init__(self, config):\n        super().__init__()\n\n        # Time encoding\n        self.time_encoder = nn.Linear(1, config.embedding_dim)\n\n        # Behavior type encoding\n        self.behavior_embedding = nn.Embedding(\n            config.num_behavior_types, \n            config.embedding_dim\n        )\n\n    def forward(self, sequences, timestamps, behavior_types):\n        # Sequence encoding\n        seq_encoded = self.item_encoder(sequences)\n\n        # Time encoding\n        time_encoded = self.time_encoder(timestamps.unsqueeze(-1))\n\n        # Behavior encoding\n        behavior_encoded = self.behavior_embedding(behavior_types)\n\n        # Fuse features\n        combined = seq_encoded + time_encoded + behavior_encoded\n\n        return combined\n</code></pre>"},{"location":"en/models/tiger/#cold-start-handling","title":"Cold Start Handling","text":"<pre><code>class ColdStartHandler:\n    \"\"\"Cold start handler\"\"\"\n\n    def __init__(self, model, item_features):\n        self.model = model\n        self.item_features = item_features\n\n    def recommend_for_new_user(self, user_profile, k=10):\n        \"\"\"Recommend for new user\"\"\"\n        # Find similar items based on user profile\n        similar_items = self.find_similar_items(user_profile)\n\n        # Use item features for recommendation\n        recommendations = self.model.recommend_by_content(similar_items, k)\n\n        return recommendations\n\n    def recommend_new_item(self, item_features, k=10):\n        \"\"\"Recommend new item\"\"\"\n        # Find feature-similar existing items\n        similar_existing = self.find_similar_existing_items(item_features)\n\n        # Recommend to users who liked similar items\n        target_users = self.get_users_who_liked(similar_existing)\n\n        return target_users[:k]\n</code></pre>"},{"location":"en/models/tiger/#practical-applications","title":"Practical Applications","text":""},{"location":"en/models/tiger/#online-service","title":"Online Service","text":"<pre><code>class TIGERRecommendationService:\n    \"\"\"TIGER recommendation service\"\"\"\n\n    def __init__(self, model_path, device='cuda'):\n        self.model = Tiger.load_from_checkpoint(model_path)\n        self.model.to(device)\n        self.model.eval()\n        self.device = device\n\n    def get_recommendations(self, user_id, user_history, k=10):\n        \"\"\"Get recommendation results\"\"\"\n        # Preprocess user history\n        semantic_sequence = self.preprocess_user_history(user_history)\n\n        # Generate recommendations\n        with torch.no_grad():\n            recommendations = self.model.generate_recommendations(\n                semantic_sequence, max_length=k\n            )\n\n        # Post-process: semantic IDs -&gt; item IDs\n        item_recommendations = self.semantic_to_items(recommendations)\n\n        return item_recommendations\n\n    def batch_recommend(self, user_requests):\n        \"\"\"Batch recommendation\"\"\"\n        batch_results = []\n\n        for user_id, user_history in user_requests:\n            recommendations = self.get_recommendations(user_id, user_history)\n            batch_results.append((user_id, recommendations))\n\n        return batch_results\n</code></pre>"},{"location":"en/models/tiger/#ab-testing-support","title":"A/B Testing Support","text":"<pre><code>class ABTestingFramework:\n    \"\"\"A/B testing framework\"\"\"\n\n    def __init__(self, model_a, model_b):\n        self.model_a = model_a  # Control group model\n        self.model_b = model_b  # Experiment group model\n\n    def recommend_with_ab_test(self, user_id, user_history, test_group=None):\n        \"\"\"A/B test recommendation\"\"\"\n        if test_group is None:\n            # Random assignment\n            test_group = 'A' if hash(user_id) % 2 == 0 else 'B'\n\n        if test_group == 'A':\n            return self.model_a.recommend(user_history), 'A'\n        else:\n            return self.model_b.recommend(user_history), 'B'\n\n    def collect_metrics(self, user_id, recommendations, group, feedback):\n        \"\"\"Collect A/B test metrics\"\"\"\n        # Record user feedback and recommendation results\n        metrics_data = {\n            'user_id': user_id,\n            'group': group,\n            'recommendations': recommendations,\n            'feedback': feedback,\n            'timestamp': time.time()\n        }\n\n        # Store to database or logging system\n        self.save_metrics(metrics_data)\n</code></pre> <p>TIGER model solves recommendation problems through generative methods, featuring powerful sequence modeling capabilities and flexible generation mechanisms, particularly suitable for handling complex user behavior sequences and diverse recommendation scenarios.</p>"},{"location":"en/training/rqvae/","title":"RQVAE Training","text":"<p>This guide provides detailed instructions on how to train the RQVAE model.</p>"},{"location":"en/training/rqvae/#training-preparation","title":"Training Preparation","text":""},{"location":"en/training/rqvae/#1-data-preparation","title":"1. Data Preparation","text":"<p>Ensure the dataset is downloaded and placed in the correct location:</p> <pre><code># Data will be automatically downloaded to the specified directory\nmkdir -p dataset/amazon\n</code></pre>"},{"location":"en/training/rqvae/#2-check-configuration-file","title":"2. Check Configuration File","text":"<p>View the default configuration:</p> <pre><code>cat config/rqvae/p5_amazon.gin\n</code></pre> <p>Key configuration parameters:</p> <pre><code># Training parameters\ntrain.iterations=400000          # Number of training iterations\ntrain.learning_rate=0.0005      # Learning rate\ntrain.batch_size=64             # Batch size\ntrain.weight_decay=0.01         # Weight decay\n\n# Model parameters\ntrain.vae_input_dim=768         # Input dimension\ntrain.vae_embed_dim=32          # Embedding dimension\ntrain.vae_hidden_dims=[512, 256, 128]  # Hidden layer dimensions\ntrain.vae_codebook_size=256     # Codebook size\ntrain.vae_n_layers=3            # Number of quantization layers\n\n# Quantization settings\ntrain.vae_codebook_mode=%generative_recommenders.models.rqvae.QuantizeForwardMode.ROTATION_TRICK\ntrain.commitment_weight=0.25    # Commitment loss weight\n</code></pre>"},{"location":"en/training/rqvae/#start-training","title":"Start Training","text":""},{"location":"en/training/rqvae/#basic-training-command","title":"Basic Training Command","text":"<pre><code>python generative_recommenders/trainers/rqvae_trainer.py config/rqvae/p5_amazon.gin\n</code></pre>"},{"location":"en/training/rqvae/#training-monitoring","title":"Training Monitoring","text":"<p>If Weights &amp; Biases is enabled:</p> <pre><code>train.wandb_logging=True\ntrain.wandb_project=\"my_rqvae_project\"\n</code></pre>"},{"location":"en/training/rqvae/#gpu-training","title":"GPU Training","text":"<p>Using multi-GPU training:</p> <pre><code>accelerate config  # Configure on first run\naccelerate launch generative_recommenders/trainers/rqvae_trainer.py config/rqvae/p5_amazon.gin\n</code></pre>"},{"location":"en/training/rqvae/#custom-configuration","title":"Custom Configuration","text":""},{"location":"en/training/rqvae/#creating-custom-configuration-file","title":"Creating Custom Configuration File","text":"<pre><code># my_rqvae_config.gin\nimport generative_recommenders.data.p5_amazon\nimport generative_recommenders.models.rqvae\n\n# Custom training parameters\ntrain.iterations=200000\ntrain.batch_size=32\ntrain.learning_rate=0.001\n\n# Custom model architecture\ntrain.vae_embed_dim=64\ntrain.vae_hidden_dims=[512, 256, 128, 64]\ntrain.vae_codebook_size=512\n\n# Data paths\ntrain.dataset_folder=\"path/to/my/dataset\"\ntrain.save_dir_root=\"path/to/my/output\"\n\n# Experiment tracking\ntrain.wandb_logging=True\ntrain.wandb_project=\"custom_rqvae_experiment\"\n</code></pre> <p>Using custom configuration:</p> <pre><code>python generative_recommenders/trainers/rqvae_trainer.py my_rqvae_config.gin\n</code></pre>"},{"location":"en/training/rqvae/#training-monitoring_1","title":"Training Monitoring","text":""},{"location":"en/training/rqvae/#key-metrics","title":"Key Metrics","text":"<p>Monitor these metrics during training:</p> <ul> <li>Total Loss: Overall training loss</li> <li>Reconstruction Loss: Reconstruction quality</li> <li>Quantization Loss: Quantization effectiveness</li> <li>Commitment Loss: Encoder commitment</li> </ul>"},{"location":"en/training/rqvae/#sample-log-output","title":"Sample Log Output","text":"<pre><code>Epoch 1000: Loss=2.3456, Recon=2.1234, Quant=0.1234, Commit=0.0988\nEpoch 2000: Loss=1.9876, Recon=1.8234, Quant=0.0987, Commit=0.0655\n...\n</code></pre>"},{"location":"en/training/rqvae/#model-evaluation","title":"Model Evaluation","text":""},{"location":"en/training/rqvae/#reconstruction-quality-assessment","title":"Reconstruction Quality Assessment","text":"<pre><code>from generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\n# Load trained model\nmodel = RqVae.load_from_checkpoint(\"out/rqvae/checkpoint_299999.pt\")\n\n# Evaluation dataset\neval_dataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    train_test_split=\"eval\"\n)\n\n# Calculate reconstruction loss\nmodel.eval()\nwith torch.no_grad():\n    eval_loss = model.evaluate(eval_dataset)\n    print(f\"Evaluation loss: {eval_loss:.4f}\")\n</code></pre>"},{"location":"en/training/rqvae/#codebook-utilization-analysis","title":"Codebook Utilization Analysis","text":"<pre><code>def analyze_codebook_usage(model, dataloader):\n    used_codes = set()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            outputs = model(batch)\n            semantic_ids = outputs.sem_ids\n            used_codes.update(semantic_ids.flatten().tolist())\n\n    usage_rate = len(used_codes) / model.codebook_size\n    print(f\"Codebook usage: {usage_rate:.2%}\")\n    print(f\"Used codes: {len(used_codes)}/{model.codebook_size}\")\n\n    return used_codes\n</code></pre>"},{"location":"en/training/rqvae/#troubleshooting","title":"Troubleshooting","text":""},{"location":"en/training/rqvae/#common-issues","title":"Common Issues","text":"<p>Q: Training loss doesn't converge?</p> <p>A: Try these solutions: - Lower learning rate: <code>train.learning_rate=0.0001</code> - Adjust commitment weight: <code>train.commitment_weight=0.1</code> - Check if data preprocessing is correct</p> <p>Q: Codebook collapse (all samples use the same code)?</p> <p>A:  - Use ROTATION_TRICK mode - Increase commitment weight - Reduce learning rate</p> <p>Q: GPU out of memory?</p> <p>A: - Reduce batch size: <code>train.batch_size=32</code> - Reduce model size: <code>train.vae_hidden_dims=[256, 128]</code> - Enable mixed precision training</p>"},{"location":"en/training/rqvae/#debugging-tips","title":"Debugging Tips","text":"<ol> <li> <p>Gradient checking: <pre><code>for name, param in model.named_parameters():\n    if param.grad is not None:\n        grad_norm = param.grad.norm().item()\n        print(f\"{name}: {grad_norm:.6f}\")\n</code></pre></p> </li> <li> <p>Loss analysis: <pre><code># Print individual loss components\nprint(f\"Reconstruction: {outputs.reconstruction_loss:.4f}\")\nprint(f\"Quantization: {outputs.quantization_loss:.4f}\")\nprint(f\"Commitment: {outputs.commitment_loss:.4f}\")\n</code></pre></p> </li> </ol>"},{"location":"en/training/rqvae/#best-practices","title":"Best Practices","text":""},{"location":"en/training/rqvae/#hyperparameter-tuning-recommendations","title":"Hyperparameter Tuning Recommendations","text":"<ol> <li> <p>Learning rate scheduling: <pre><code># Use cosine annealing\ntrain.scheduler=\"cosine\"\ntrain.min_lr=1e-6\n</code></pre></p> </li> <li> <p>Early stopping strategy: <pre><code>train.early_stopping=True\ntrain.patience=10000\n</code></pre></p> </li> <li> <p>Model saving frequency: <pre><code>train.save_model_every=50000  # Save every 50k iterations\ntrain.eval_every=10000        # Evaluate every 10k iterations\n</code></pre></p> </li> </ol>"},{"location":"en/training/rqvae/#experiment-management","title":"Experiment Management","text":"<p>Recommended to use version control and experiment tracking:</p> <pre><code># Create experiment branch\ngit checkout -b experiment/rqvae-large-codebook\n\n# Modify configuration\nvim config/rqvae/large_codebook.gin\n\n# Run experiment\npython generative_recommenders/trainers/rqvae_trainer.py config/rqvae/large_codebook.gin\n\n# Record results\ngit add .\ngit commit -m \"Experiment: large codebook (size=1024)\"\n</code></pre>"},{"location":"en/training/rqvae/#next-steps","title":"Next Steps","text":"<p>After training completion, you can:</p> <ol> <li>Use the trained RQVAE for TIGER training</li> <li>Analyze model performance</li> <li>Try different datasets</li> </ol>"},{"location":"en/training/tiger/","title":"TIGER Training","text":"<p>This guide provides detailed instructions on how to train the TIGER model.</p>"},{"location":"en/training/tiger/#prerequisites","title":"Prerequisites","text":""},{"location":"en/training/tiger/#1-pre-trained-rqvae-model","title":"1. Pre-trained RQVAE Model","text":"<p>TIGER requires a pre-trained RQVAE model to generate semantic IDs:</p> <pre><code># Ensure RQVAE model has been trained\nls out/rqvae/p5_amazon/beauty/checkpoint_*.pt\n</code></pre> <p>If not available, please complete RQVAE training first.</p>"},{"location":"en/training/tiger/#2-data-preparation","title":"2. Data Preparation","text":"<p>Ensure using the same dataset as RQVAE:</p> <pre><code># Data should already exist\nls dataset/amazon/\n</code></pre>"},{"location":"en/training/tiger/#training-configuration","title":"Training Configuration","text":""},{"location":"en/training/tiger/#default-configuration","title":"Default Configuration","text":"<p>View the TIGER configuration file:</p> <pre><code>cat config/tiger/p5_amazon.gin\n</code></pre> <p>Key parameters:</p> <pre><code># Training parameters\ntrain.epochs=5000               # Training epochs\ntrain.learning_rate=3e-4        # Learning rate\ntrain.batch_size=256            # Batch size\ntrain.weight_decay=0.035        # Weight decay\n\n# Model parameters\ntrain.embedding_dim=128         # Embedding dimension\ntrain.attn_dim=512             # Attention dimension\ntrain.dropout=0.3              # Dropout rate\ntrain.num_heads=8              # Number of attention heads\ntrain.n_layers=8               # Number of transformer layers\n\n# Sequence parameters\ntrain.max_seq_len=512          # Maximum sequence length\ntrain.num_item_embeddings=256  # Number of item embeddings\ntrain.num_user_embeddings=2000 # Number of user embeddings\ntrain.sem_id_dim=3             # Semantic ID dimension\n\n# Pre-trained model path\ntrain.pretrained_rqvae_path=\"./out/rqvae/p5_amazon/beauty/checkpoint_299999.pt\"\n</code></pre>"},{"location":"en/training/tiger/#start-training","title":"Start Training","text":""},{"location":"en/training/tiger/#basic-training-command","title":"Basic Training Command","text":"<pre><code>python generative_recommenders/trainers/tiger_trainer.py config/tiger/p5_amazon.gin\n</code></pre>"},{"location":"en/training/tiger/#distributed-training","title":"Distributed Training","text":"<p>Using multi-GPU training:</p> <pre><code>accelerate config\naccelerate launch generative_recommenders/trainers/tiger_trainer.py config/tiger/p5_amazon.gin\n</code></pre>"},{"location":"en/training/tiger/#training-process","title":"Training Process","text":"<p>During training you'll see:</p> <ol> <li>Data Loading: Sequence dataset loading and semantic ID generation</li> <li>Model Initialization: Transformer model initialization</li> <li>Training Loop: Loss reduction and metric monitoring</li> <li>Validation Evaluation: Periodic performance assessment</li> </ol>"},{"location":"en/training/tiger/#custom-configuration","title":"Custom Configuration","text":""},{"location":"en/training/tiger/#creating-custom-configuration","title":"Creating Custom Configuration","text":"<pre><code># my_tiger_config.gin\nimport generative_recommenders.data.p5_amazon\n\n# Adjust model scale\ntrain.embedding_dim=256\ntrain.attn_dim=1024\ntrain.n_layers=12\ntrain.num_heads=16\n\n# Adjust training parameters\ntrain.learning_rate=1e-4\ntrain.batch_size=128\ntrain.epochs=10000\n\n# Custom paths\ntrain.dataset_folder=\"my_dataset\"\ntrain.pretrained_rqvae_path=\"my_rqvae/checkpoint.pt\"\ntrain.save_dir_root=\"my_tiger_output/\"\n\n# Experiment tracking\ntrain.wandb_logging=True\ntrain.wandb_project=\"my_tiger_experiment\"\n</code></pre>"},{"location":"en/training/tiger/#model-architecture-overview","title":"Model Architecture Overview","text":""},{"location":"en/training/tiger/#transformer-structure","title":"Transformer Structure","text":"<p>TIGER uses an encoder-decoder architecture:</p> <pre><code>class Tiger(nn.Module):\n    def __init__(self, config):\n        # User and item embeddings\n        self.user_embedding = UserIdEmbedding(...)\n        self.item_embedding = SemIdEmbedding(...)\n\n        # Transformer encoder-decoder\n        self.transformer = TransformerEncoderDecoder(...)\n\n        # Output projection\n        self.output_projection = nn.Linear(...)\n</code></pre>"},{"location":"en/training/tiger/#semantic-id-mapping","title":"Semantic ID Mapping","text":"<p>TIGER converts items to semantic ID sequences:</p> <pre><code># Item -&gt; Semantic ID sequence\nitem_id = 123\nsemantic_ids = rqvae.get_semantic_ids(item_features[item_id])\n# semantic_ids: [45, 67, 89]  # length = sem_id_dim\n</code></pre>"},{"location":"en/training/tiger/#training-monitoring","title":"Training Monitoring","text":""},{"location":"en/training/tiger/#key-metrics","title":"Key Metrics","text":"<ul> <li>Training Loss: Sequence modeling loss</li> <li>Validation Loss: Validation set performance</li> <li>Recall@K: Top-K recall rate</li> <li>NDCG@K: Normalized Discounted Cumulative Gain</li> </ul>"},{"location":"en/training/tiger/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<p>Enable experiment tracking:</p> <pre><code>train.wandb_logging=True\ntrain.wandb_project=\"tiger_p5_amazon\"\ntrain.wandb_log_interval=100\n</code></pre> <p>View training curves: - Visit wandb.ai - Find your project and experiment</p>"},{"location":"en/training/tiger/#model-evaluation","title":"Model Evaluation","text":""},{"location":"en/training/tiger/#recommendation-quality-assessment","title":"Recommendation Quality Assessment","text":"<pre><code>from generative_recommenders.models.tiger import Tiger\nfrom generative_recommenders.modules.metrics import TopKAccumulator\n\n# Load model\nmodel = Tiger.load_from_checkpoint(\"out/tiger/checkpoint.pt\")\n\n# Create evaluator\nevaluator = TopKAccumulator(k=[5, 10, 20])\n\n# Evaluate on test set\ntest_dataloader = DataLoader(test_dataset, batch_size=256)\nmetrics = evaluator.evaluate(model, test_dataloader)\n\nprint(f\"Recall@10: {metrics['recall@10']:.4f}\")\nprint(f\"NDCG@10: {metrics['ndcg@10']:.4f}\")\n</code></pre>"},{"location":"en/training/tiger/#generative-recommendation","title":"Generative Recommendation","text":"<pre><code>def generate_recommendations(model, user_sequence, top_k=10):\n    \"\"\"Generate recommendations for user\"\"\"\n    model.eval()\n\n    with torch.no_grad():\n        # Encode user sequence\n        sequence_embedding = model.encode_sequence(user_sequence)\n\n        # Generate recommendations\n        logits = model.generate(sequence_embedding, max_length=top_k)\n\n        # Get Top-K items\n        recommendations = torch.topk(logits, top_k).indices\n\n    return recommendations.tolist()\n\n# Usage example\nuser_history = [item1_semantic_ids, item2_semantic_ids, ...]\nrecommendations = generate_recommendations(model, user_history, top_k=10)\n</code></pre>"},{"location":"en/training/tiger/#advanced-features","title":"Advanced Features","text":""},{"location":"en/training/tiger/#trie-constrained-generation","title":"Trie-Constrained Generation","text":"<p>TIGER supports Trie-based constrained generation:</p> <pre><code>from generative_recommenders.models.tiger import build_trie\n\n# Build Trie for valid item IDs\nvalid_items = torch.tensor([[1, 2, 3], [4, 5, 6], ...])  # Semantic ID sequences\ntrie = build_trie(valid_items)\n\n# Constrained generation\nconstrained_output = model.generate_with_trie(\n    user_sequence, \n    trie=trie,\n    max_length=10\n)\n</code></pre>"},{"location":"en/training/tiger/#sequence-augmentation","title":"Sequence Augmentation","text":"<p>Training supports sequence augmentation:</p> <pre><code>train.subsample=True  # Dynamic subsampling\ntrain.augmentation=True  # Sequence augmentation\n</code></pre>"},{"location":"en/training/tiger/#troubleshooting","title":"Troubleshooting","text":""},{"location":"en/training/tiger/#common-issues","title":"Common Issues","text":"<p>Q: RQVAE checkpoint not found?</p> <p>A: Check if path is correct: <pre><code># Confirm file exists\nls -la out/rqvae/p5_amazon/beauty/checkpoint_299999.pt\n\n# Update path in config file\ntrain.pretrained_rqvae_path=\"actual_checkpoint_path\"\n</code></pre></p> <p>Q: Training is slow?</p> <p>A: Optimization suggestions: - Increase batch size: <code>train.batch_size=512</code> - Reduce sequence length: <code>train.max_seq_len=256</code> - Use multi-GPU training</p> <p>Q: Poor recommendation performance?</p> <p>A: Tuning suggestions: - Increase model size: <code>train.n_layers=12</code> - Adjust learning rate: <code>train.learning_rate=1e-4</code> - Increase training epochs: <code>train.epochs=10000</code></p>"},{"location":"en/training/tiger/#debugging-tips","title":"Debugging Tips","text":"<ol> <li> <p>Check semantic ID generation: <pre><code># Verify RQVAE is working correctly\nrqvae = RqVae.load_from_checkpoint(pretrained_path)\nsample_item = dataset[0]\nsemantic_ids = rqvae.get_semantic_ids(sample_item)\nprint(f\"Semantic IDs: {semantic_ids}\")\n</code></pre></p> </li> <li> <p>Monitor attention weights: <pre><code># Check if model learns meaningful attention patterns\nattention_weights = model.get_attention_weights(user_sequence)\nprint(f\"Attention shape: {attention_weights.shape}\")\n</code></pre></p> </li> </ol>"},{"location":"en/training/tiger/#performance-optimization","title":"Performance Optimization","text":""},{"location":"en/training/tiger/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Reduce memory usage\ntrain.gradient_accumulate_every=4  # Gradient accumulation\ntrain.batch_size=64               # Smaller batch size\ntrain.max_seq_len=256            # Shorter sequences\n</code></pre>"},{"location":"en/training/tiger/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>train.mixed_precision_type=\"fp16\"  # Use half precision\n</code></pre>"},{"location":"en/training/tiger/#experiment-suggestions","title":"Experiment Suggestions","text":""},{"location":"en/training/tiger/#hyperparameter-grid-search","title":"Hyperparameter Grid Search","text":"<pre><code># Suggested hyperparameter ranges\nlearning_rates = [1e-4, 3e-4, 1e-3]\nbatch_sizes = [128, 256, 512]\nmodel_dims = [128, 256, 512]\nn_layers = [6, 8, 12]\n\nfor lr in learning_rates:\n    for bs in batch_sizes:\n        # Create config and train\n        config = create_config(lr=lr, batch_size=bs)\n        train_model(config)\n</code></pre>"},{"location":"en/training/tiger/#ab-testing","title":"A/B Testing","text":"<p>Compare different architectures:</p> <pre><code># Experiment A: Standard TIGER\ntrain.n_layers=8\ntrain.num_heads=8\n\n# Experiment B: Deeper model\ntrain.n_layers=12\ntrain.num_heads=16\n\n# Experiment C: Wider model\ntrain.embedding_dim=256\ntrain.attn_dim=1024\n</code></pre>"},{"location":"en/training/tiger/#next-steps","title":"Next Steps","text":"<p>After training completion:</p> <ol> <li>Evaluate recommendation effectiveness</li> <li>Deploy to production environment</li> <li>Try other datasets</li> </ol>"},{"location":"zh/","title":"GenerativeRecommenders","text":"<p>\u57fa\u4e8e PyTorch \u7684\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u6846\u67b6\u3002</p>"},{"location":"zh/#_1","title":"\u6982\u8ff0","text":"<p>GenerativeRecommenders \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u6700\u65b0\u7684\u751f\u6210\u5f0f\u63a8\u8350\u7b97\u6cd5\u3002\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u5e72\u51c0\u7684\u4ee3\u7801\u67b6\u6784\u3001\u7075\u6d3b\u7684\u914d\u7f6e\u7cfb\u7edf\u4ee5\u53ca\u6613\u4e8e\u6269\u5c55\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\u3002</p>"},{"location":"zh/#_2","title":"\u6838\u5fc3\u7279\u6027","text":"<ul> <li>\u2728 \u6a21\u5757\u5316\u8bbe\u8ba1: \u6e05\u6670\u7684\u7ec4\u4ef6\u5206\u79bb\uff0c\u6613\u4e8e\u7406\u89e3\u548c\u6269\u5c55</li> <li>\ud83d\udd27 \u914d\u7f6e\u9a71\u52a8: \u57fa\u4e8e Gin \u7684\u7075\u6d3b\u914d\u7f6e\u7cfb\u7edf</li> <li>\ud83d\udcca \u591a\u79cd\u6a21\u578b: RQVAE\u3001TIGER \u7b49\u6700\u65b0\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b</li> <li>\ud83c\udfaf \u6570\u636e\u96c6\u652f\u6301: P5 Amazon \u7b49\u4e3b\u6d41\u63a8\u8350\u6570\u636e\u96c6</li> <li>\ud83d\ude80 \u5206\u5e03\u5f0f\u8bad\u7ec3: \u57fa\u4e8e Accelerate \u7684\u591a GPU \u8bad\u7ec3\u652f\u6301</li> <li>\ud83d\udcc8 \u5b9e\u9a8c\u8ddf\u8e2a: \u96c6\u6210 Weights &amp; Biases \u8fdb\u884c\u5b9e\u9a8c\u7ba1\u7406</li> <li>\ud83d\udd0d \u7f13\u5b58\u4f18\u5316: \u667a\u80fd\u7684\u6570\u636e\u9884\u5904\u7406\u7f13\u5b58\u673a\u5236</li> </ul>"},{"location":"zh/#_3","title":"\u652f\u6301\u7684\u6a21\u578b","text":""},{"location":"zh/#rqvae-residual-quantized-variational-autoencoder","title":"RQVAE (Residual Quantized Variational Autoencoder)","text":"<ul> <li>\u57fa\u4e8e\u5411\u91cf\u91cf\u5316\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668</li> <li>\u652f\u6301\u591a\u79cd\u91cf\u5316\u7b56\u7565\uff1aGumbel-Softmax\u3001STE\u3001Rotation Trick\u3001Sinkhorn</li> <li>\u7528\u4e8e\u5b66\u4e60\u7269\u54c1\u7684\u8bed\u4e49\u8868\u793a</li> </ul>"},{"location":"zh/#tiger-recommender-systems-with-generative-retrieval","title":"TIGER (Recommender Systems with Generative Retrieval)","text":"<ul> <li>\u57fa\u4e8e Transformer \u7684\u751f\u6210\u5f0f\u68c0\u7d22\u6a21\u578b</li> <li>\u4f7f\u7528\u8bed\u4e49 ID \u8fdb\u884c\u5e8f\u5217\u5efa\u6a21</li> <li>\u652f\u6301 Trie \u7ea6\u675f\u7684\u751f\u6210\u8fc7\u7a0b</li> </ul>"},{"location":"zh/#_4","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"zh/#_5","title":"\u5b89\u88c5","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"zh/#rqvae","title":"\u8bad\u7ec3 RQVAE","text":"<pre><code>python generative_recommenders/trainers/rqvae_trainer.py config/rqvae/p5_amazon.gin\n</code></pre>"},{"location":"zh/#tiger","title":"\u8bad\u7ec3 TIGER","text":"<pre><code>python generative_recommenders/trainers/tiger_trainer.py config/tiger/p5_amazon.gin\n</code></pre>"},{"location":"zh/#_6","title":"\u9879\u76ee\u7ed3\u6784","text":"<pre><code>GenerativeRecommenders/\n\u251c\u2500\u2500 generative_recommenders/          # \u6838\u5fc3\u4ee3\u7801\n\u2502   \u251c\u2500\u2500 data/                        # \u6570\u636e\u5904\u7406\u6a21\u5757\n\u2502   \u2502   \u251c\u2500\u2500 configs.py               # \u914d\u7f6e\u7c7b\u5b9a\u4e49\n\u2502   \u2502   \u251c\u2500\u2500 base_dataset.py          # \u6570\u636e\u96c6\u62bd\u8c61\u57fa\u7c7b\n\u2502   \u2502   \u251c\u2500\u2500 p5_amazon.py             # P5 Amazon \u6570\u636e\u96c6\n\u2502   \u2502   \u251c\u2500\u2500 processors/              # \u6570\u636e\u5904\u7406\u5668\n\u2502   \u2502   \u2514\u2500\u2500 dataset_factory.py       # \u6570\u636e\u96c6\u5de5\u5382\n\u2502   \u251c\u2500\u2500 models/                      # \u6a21\u578b\u5b9e\u73b0\n\u2502   \u2502   \u251c\u2500\u2500 rqvae.py                 # RQVAE \u6a21\u578b\n\u2502   \u2502   \u2514\u2500\u2500 tiger.py                 # TIGER \u6a21\u578b\n\u2502   \u251c\u2500\u2500 modules/                     # \u57fa\u7840\u6a21\u5757\n\u2502   \u2502   \u251c\u2500\u2500 embedding.py             # \u5d4c\u5165\u5c42\n\u2502   \u2502   \u251c\u2500\u2500 encoder.py               # \u7f16\u7801\u5668\n\u2502   \u2502   \u251c\u2500\u2500 loss.py                  # \u635f\u5931\u51fd\u6570\n\u2502   \u2502   \u2514\u2500\u2500 metrics.py               # \u8bc4\u4f30\u6307\u6807\n\u2502   \u2514\u2500\u2500 trainers/                    # \u8bad\u7ec3\u811a\u672c\n\u2502       \u251c\u2500\u2500 rqvae_trainer.py         # RQVAE \u8bad\u7ec3\u5668\n\u2502       \u2514\u2500\u2500 tiger_trainer.py         # TIGER \u8bad\u7ec3\u5668\n\u251c\u2500\u2500 config/                          # \u914d\u7f6e\u6587\u4ef6\n\u2502   \u251c\u2500\u2500 rqvae/                       # RQVAE \u914d\u7f6e\n\u2502   \u2514\u2500\u2500 tiger/                       # TIGER \u914d\u7f6e\n\u2514\u2500\u2500 docs/                           # \u6587\u6863\n</code></pre>"},{"location":"zh/#_7","title":"\u4e3b\u8981\u6539\u8fdb","text":"<p>\u76f8\u6bd4\u539f\u59cb\u5b9e\u73b0\uff0c\u6211\u4eec\u7684\u91cd\u6784\u7248\u672c\u63d0\u4f9b\u4e86\uff1a</p> <ol> <li>\u66f4\u6e05\u6670\u7684\u4ee3\u7801\u7ed3\u6784: \u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u804c\u8d23\u5206\u660e</li> <li>\u914d\u7f6e\u5316\u7ba1\u7406: \u652f\u6301\u7075\u6d3b\u7684\u53c2\u6570\u914d\u7f6e\u548c\u5b9e\u9a8c\u7ba1\u7406</li> <li>\u901a\u7528\u6027\u589e\u5f3a: \u6613\u4e8e\u6269\u5c55\u5230\u65b0\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b</li> <li>\u6027\u80fd\u4f18\u5316: \u7f13\u5b58\u673a\u5236\u548c\u5185\u5b58\u6548\u7387\u63d0\u5347</li> <li>\u66f4\u597d\u7684\u6587\u6863: \u5b8c\u6574\u7684 API \u6587\u6863\u548c\u4f7f\u7528\u793a\u4f8b</li> </ol>"},{"location":"zh/#_8","title":"\u57fa\u51c6\u7ed3\u679c","text":"\u6570\u636e\u96c6 \u6a21\u578b \u6307\u6807 \u7ed3\u679c P5 Amazon-Beauty TIGER Recall@10 0.42"},{"location":"zh/#_9","title":"\u8d21\u732e","text":"<p>\u6b22\u8fce\u63d0\u4ea4 Issue \u548c Pull Request\uff01\u8bf7\u53c2\u8003\u6211\u4eec\u7684\u8d21\u732e\u6307\u5357\u3002</p>"},{"location":"zh/#_10","title":"\u8bb8\u53ef\u8bc1","text":"<p>\u672c\u9879\u76ee\u91c7\u7528 MIT \u8bb8\u53ef\u8bc1\u3002\u8be6\u89c1 LICENSE \u6587\u4ef6\u3002</p>"},{"location":"zh/#_11","title":"\u5f15\u7528","text":"<p>\u5982\u679c\u60a8\u5728\u7814\u7a76\u4e2d\u4f7f\u7528\u4e86\u672c\u6846\u67b6\uff0c\u8bf7\u5f15\u7528\u76f8\u5173\u8bba\u6587\uff1a</p> <pre><code>@inproceedings{rqvae2023,\n  title={RQ-VAE Recommender},\n  author={Botta, Edoardo},\n  year={2023}\n}\n\n@article{tiger2023,\n  title={TIGER: Recommender Systems with Generative Retrieval},\n  year={2023}\n}\n</code></pre>"},{"location":"zh/contributing/","title":"\u8d21\u732e\u6307\u5357","text":"<p>\u611f\u8c22\u60a8\u5bf9 GenerativeRecommenders \u9879\u76ee\u7684\u5173\u6ce8\uff01\u6211\u4eec\u6b22\u8fce\u793e\u533a\u7684\u8d21\u732e\uff0c\u65e0\u8bba\u662f\u4ee3\u7801\u3001\u6587\u6863\u3001\u6d4b\u8bd5\u8fd8\u662f\u53cd\u9988\u3002</p>"},{"location":"zh/contributing/#_2","title":"\u5f00\u59cb\u8d21\u732e","text":""},{"location":"zh/contributing/#_3","title":"\u73af\u5883\u8bbe\u7f6e","text":"<ol> <li> <p>Fork \u9879\u76ee <pre><code># \u5728 GitHub \u4e0a fork \u9879\u76ee\u5230\u60a8\u7684\u8d26\u6237\n# \u7136\u540e\u514b\u9686\u60a8\u7684 fork\ngit clone https://github.com/YOUR_USERNAME/GenerativeRecommenders.git\ncd GenerativeRecommenders\n</code></pre></p> </li> <li> <p>\u8bbe\u7f6e\u5f00\u53d1\u73af\u5883 <pre><code># \u521b\u5efa\u865a\u62df\u73af\u5883\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# \u6216 venv\\Scripts\\activate  # Windows\n\n# \u5b89\u88c5\u5f00\u53d1\u4f9d\u8d56\npip install -e \".[dev]\"\n</code></pre></p> </li> <li> <p>\u5b89\u88c5\u9884\u63d0\u4ea4\u94a9\u5b50 <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"zh/contributing/#_4","title":"\u5f00\u53d1\u5de5\u4f5c\u6d41","text":"<ol> <li> <p>\u521b\u5efa\u529f\u80fd\u5206\u652f <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>\u8fdb\u884c\u5f00\u53d1</p> </li> <li>\u9075\u5faa\u4ee3\u7801\u89c4\u8303</li> <li>\u6dfb\u52a0\u5fc5\u8981\u7684\u6d4b\u8bd5</li> <li> <p>\u66f4\u65b0\u76f8\u5173\u6587\u6863</p> </li> <li> <p>\u63d0\u4ea4\u66f4\u6539 <pre><code>git add .\ngit commit -m \"Add: \u7b80\u6d01\u63cf\u8ff0\u60a8\u7684\u66f4\u6539\"\n</code></pre></p> </li> <li> <p>\u63a8\u9001\u5230\u60a8\u7684 fork <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>\u521b\u5efa Pull Request</p> </li> <li>\u5728 GitHub \u4e0a\u521b\u5efa PR</li> <li>\u586b\u5199\u8be6\u7ec6\u7684 PR \u63cf\u8ff0</li> <li>\u7b49\u5f85\u4ee3\u7801\u5ba1\u67e5</li> </ol>"},{"location":"zh/contributing/#_5","title":"\u8d21\u732e\u7c7b\u578b","text":""},{"location":"zh/contributing/#bug","title":"\ud83d\udc1b Bug \u4fee\u590d","text":"<p>\u53d1\u73b0\u4e86 bug\uff1f\u8bf7\uff1a 1. \u67e5\u770b\u73b0\u6709 issues \u662f\u5426\u5df2\u62a5\u544a 2. \u5982\u679c\u6ca1\u6709\uff0c\u521b\u5efa\u65b0\u7684 issue \u63cf\u8ff0\u95ee\u9898 3. \u63d0\u4f9b\u590d\u73b0\u6b65\u9aa4\u548c\u73af\u5883\u4fe1\u606f 4. \u5982\u679c\u80fd\u4fee\u590d\uff0c\u63d0\u4ea4 PR</p> <p>Bug \u62a5\u544a\u6a21\u677f\uff1a <pre><code>## Bug \u63cf\u8ff0\n\u7b80\u8981\u63cf\u8ff0 bug \u7684\u8868\u73b0\n\n## \u590d\u73b0\u6b65\u9aa4\n1. \u6267\u884c\u6b65\u9aa41\n2. \u6267\u884c\u6b65\u9aa42\n3. \u89c2\u5bdf\u5230\u9519\u8bef\n\n## \u9884\u671f\u884c\u4e3a\n\u63cf\u8ff0\u60a8\u671f\u671b\u7684\u6b63\u786e\u884c\u4e3a\n\n## \u73af\u5883\u4fe1\u606f\n- Python \u7248\u672c\uff1a\n- PyTorch \u7248\u672c\uff1a\n- \u64cd\u4f5c\u7cfb\u7edf\uff1a\n- GPU \u4fe1\u606f\uff08\u5982\u679c\u76f8\u5173\uff09\uff1a\n</code></pre></p>"},{"location":"zh/contributing/#_6","title":"\u2728 \u65b0\u529f\u80fd","text":"<p>\u6dfb\u52a0\u65b0\u529f\u80fd\u524d\uff1a 1. \u521b\u5efa\u529f\u80fd\u8bf7\u6c42 issue \u8ba8\u8bba\u8bbe\u8ba1 2. \u786e\u4fdd\u529f\u80fd\u7b26\u5408\u9879\u76ee\u76ee\u6807 3. \u8003\u8651\u5411\u540e\u517c\u5bb9\u6027 4. \u6dfb\u52a0\u5b8c\u6574\u7684\u6d4b\u8bd5\u548c\u6587\u6863</p> <p>\u529f\u80fd\u8bf7\u6c42\u6a21\u677f\uff1a <pre><code>## \u529f\u80fd\u63cf\u8ff0\n\u63cf\u8ff0\u60a8\u5e0c\u671b\u6dfb\u52a0\u7684\u529f\u80fd\n\n## \u4f7f\u7528\u573a\u666f\n\u89e3\u91ca\u8fd9\u4e2a\u529f\u80fd\u7684\u4f7f\u7528\u573a\u666f\u548c\u4ef7\u503c\n\n## \u8bbe\u8ba1\u63d0\u6848\n\u5982\u679c\u6709\u5177\u4f53\u7684\u8bbe\u8ba1\u60f3\u6cd5\uff0c\u8bf7\u8be6\u7ec6\u63cf\u8ff0\n\n## \u66ff\u4ee3\u65b9\u6848\n\u8003\u8651\u8fc7\u7684\u5176\u4ed6\u89e3\u51b3\u65b9\u6848\n</code></pre></p>"},{"location":"zh/contributing/#_7","title":"\ud83d\udcda \u6587\u6863\u6539\u8fdb","text":"<p>\u6587\u6863\u8d21\u732e\u5305\u62ec\uff1a - \u4fee\u590d\u9519\u522b\u5b57\u548c\u8bed\u6cd5\u9519\u8bef - \u6539\u8fdb\u4ee3\u7801\u793a\u4f8b - \u6dfb\u52a0\u6559\u7a0b\u548c\u6307\u5357 - \u7ffb\u8bd1\u5185\u5bb9</p>"},{"location":"zh/contributing/#_8","title":"\ud83e\uddea \u6d4b\u8bd5","text":"<p>\u6d4b\u8bd5\u8d21\u732e\u5305\u62ec\uff1a - \u6dfb\u52a0\u5355\u5143\u6d4b\u8bd5 - \u6539\u8fdb\u6d4b\u8bd5\u8986\u76d6\u7387 - \u6dfb\u52a0\u96c6\u6210\u6d4b\u8bd5 - \u6027\u80fd\u6d4b\u8bd5</p>"},{"location":"zh/contributing/#_9","title":"\u4ee3\u7801\u89c4\u8303","text":""},{"location":"zh/contributing/#python","title":"Python \u4ee3\u7801\u98ce\u683c","text":"<p>\u6211\u4eec\u4f7f\u7528\u4ee5\u4e0b\u5de5\u5177\u4fdd\u6301\u4ee3\u7801\u4e00\u81f4\u6027\uff1a</p> <ul> <li>Black: \u4ee3\u7801\u683c\u5f0f\u5316</li> <li>isort: \u5bfc\u5165\u6392\u5e8f</li> <li>flake8: \u4ee3\u7801\u68c0\u67e5</li> <li>mypy: \u7c7b\u578b\u68c0\u67e5</li> </ul> <p>\u914d\u7f6e\u6587\u4ef6\u5728\u9879\u76ee\u6839\u76ee\u5f55\uff1a - <code>pyproject.toml</code> - <code>.flake8</code> - <code>mypy.ini</code></p>"},{"location":"zh/contributing/#_10","title":"\u4ee3\u7801\u98ce\u683c\u793a\u4f8b","text":"<pre><code>from typing import List, Optional, Dict, Any\nimport torch\nimport torch.nn as nn\nfrom generative_recommenders.base import BaseModel\n\n\nclass ExampleModel(BaseModel):\n    \"\"\"\u793a\u4f8b\u6a21\u578b\u7c7b.\n\n    Args:\n        input_dim: \u8f93\u5165\u7ef4\u5ea6\n        hidden_dim: \u9690\u85cf\u5c42\u7ef4\u5ea6\n        output_dim: \u8f93\u51fa\u7ef4\u5ea6\n        dropout: Dropout \u6982\u7387\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        dropout: float = 0.1,\n    ) -&gt; None:\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n\n        # \u5b9a\u4e49\u5c42\n        self.linear1 = nn.Linear(input_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\u524d\u5411\u4f20\u64ad.\n\n        Args:\n            x: \u8f93\u5165\u5f20\u91cf (batch_size, input_dim)\n\n        Returns:\n            \u8f93\u51fa\u5f20\u91cf (batch_size, output_dim)\n        \"\"\"\n        h = self.linear1(x)\n        h = torch.relu(h)\n        h = self.dropout(h)\n        output = self.linear2(h)\n\n        return output\n</code></pre>"},{"location":"zh/contributing/#_11","title":"\u63d0\u4ea4\u4fe1\u606f\u683c\u5f0f","text":"<p>\u4f7f\u7528\u6e05\u6670\u7684\u63d0\u4ea4\u4fe1\u606f\uff1a</p> <pre><code>\u7c7b\u578b: \u7b80\u77ed\u63cf\u8ff0 (\u4e0d\u8d85\u8fc7 50 \u5b57\u7b26)\n\n\u8be6\u7ec6\u63cf\u8ff0\uff08\u5982\u679c\u9700\u8981\uff09\uff1a\n- \u89e3\u91ca\u4e3a\u4ec0\u4e48\u505a\u8fd9\u4e2a\u66f4\u6539\n- \u63d0\u53ca\u4efb\u4f55\u91cd\u8981\u7684\u6280\u672f\u7ec6\u8282\n- \u5f15\u7528\u76f8\u5173\u7684 issue (#123)\n\nBreaking change: \u5982\u679c\u6709\u7834\u574f\u6027\u66f4\u6539\uff0c\u8bf4\u660e\u5f71\u54cd\n</code></pre> <p>\u63d0\u4ea4\u7c7b\u578b\uff1a - <code>Add</code>: \u65b0\u529f\u80fd - <code>Fix</code>: Bug \u4fee\u590d - <code>Update</code>: \u66f4\u65b0\u73b0\u6709\u529f\u80fd - <code>Remove</code>: \u5220\u9664\u529f\u80fd - <code>Refactor</code>: \u91cd\u6784\u4ee3\u7801 - <code>Docs</code>: \u6587\u6863\u66f4\u6539 - <code>Test</code>: \u6d4b\u8bd5\u76f8\u5173 - <code>Style</code>: \u4ee3\u7801\u98ce\u683c\u8c03\u6574</p>"},{"location":"zh/contributing/#_12","title":"\u6d4b\u8bd5\u6307\u5357","text":""},{"location":"zh/contributing/#_13","title":"\u8fd0\u884c\u6d4b\u8bd5","text":"<pre><code># \u8fd0\u884c\u6240\u6709\u6d4b\u8bd5\npytest\n\n# \u8fd0\u884c\u7279\u5b9a\u6d4b\u8bd5\u6587\u4ef6\npytest tests/test_models.py\n\n# \u8fd0\u884c\u7279\u5b9a\u6d4b\u8bd5\npytest tests/test_models.py::test_rqvae_forward\n\n# \u751f\u6210\u8986\u76d6\u7387\u62a5\u544a\npytest --cov=generative_recommenders\n</code></pre>"},{"location":"zh/contributing/#_14","title":"\u7f16\u5199\u6d4b\u8bd5","text":"<p>\u6d4b\u8bd5\u6587\u4ef6\u7ed3\u6784\uff1a <pre><code>tests/\n\u251c\u2500\u2500 conftest.py                 # \u6d4b\u8bd5\u914d\u7f6e\u548c\u56fa\u4ef6\n\u251c\u2500\u2500 test_models/\n\u2502   \u251c\u2500\u2500 test_rqvae.py          # RQVAE \u6d4b\u8bd5\n\u2502   \u2514\u2500\u2500 test_tiger.py          # TIGER \u6d4b\u8bd5\n\u251c\u2500\u2500 test_data/\n\u2502   \u251c\u2500\u2500 test_datasets.py       # \u6570\u636e\u96c6\u6d4b\u8bd5\n\u2502   \u2514\u2500\u2500 test_processors.py     # \u5904\u7406\u5668\u6d4b\u8bd5\n\u2514\u2500\u2500 test_utils/\n    \u2514\u2500\u2500 test_metrics.py         # \u5de5\u5177\u51fd\u6570\u6d4b\u8bd5\n</code></pre></p> <p>\u6d4b\u8bd5\u793a\u4f8b\uff1a <pre><code>import pytest\nimport torch\nfrom generative_recommenders.models.rqvae import RqVae\n\n\nclass TestRqVae:\n    \"\"\"RQVAE \u6a21\u578b\u6d4b\u8bd5\"\"\"\n\n    @pytest.fixture\n    def model(self):\n        \"\"\"\u521b\u5efa\u6d4b\u8bd5\u6a21\u578b\"\"\"\n        return RqVae(\n            input_dim=768,\n            hidden_dim=256,\n            latent_dim=128,\n            num_embeddings=512\n        )\n\n    @pytest.fixture\n    def sample_input(self):\n        \"\"\"\u521b\u5efa\u6837\u672c\u8f93\u5165\"\"\"\n        return torch.randn(32, 768)\n\n    def test_forward(self, model, sample_input):\n        \"\"\"\u6d4b\u8bd5\u524d\u5411\u4f20\u64ad\"\"\"\n        reconstructed, commitment_loss, embedding_loss, sem_ids = model(sample_input)\n\n        # \u68c0\u67e5\u8f93\u51fa\u5f62\u72b6\n        assert reconstructed.shape == sample_input.shape\n        assert sem_ids.shape == (32,)\n\n        # \u68c0\u67e5\u635f\u5931\u7c7b\u578b\n        assert isinstance(commitment_loss, torch.Tensor)\n        assert isinstance(embedding_loss, torch.Tensor)\n\n    def test_generate_semantic_ids(self, model, sample_input):\n        \"\"\"\u6d4b\u8bd5\u8bed\u4e49ID\u751f\u6210\"\"\"\n        sem_ids = model.generate_semantic_ids(sample_input)\n\n        assert sem_ids.shape == (32,)\n        assert sem_ids.dtype == torch.long\n        assert torch.all(sem_ids &gt;= 0)\n        assert torch.all(sem_ids &lt; model.quantizer.num_embeddings)\n\n    @pytest.mark.parametrize(\"input_dim,hidden_dim\", [\n        (256, 128),\n        (512, 256),\n        (1024, 512),\n    ])\n    def test_different_dimensions(self, input_dim, hidden_dim):\n        \"\"\"\u6d4b\u8bd5\u4e0d\u540c\u7ef4\u5ea6\u914d\u7f6e\"\"\"\n        model = RqVae(\n            input_dim=input_dim,\n            hidden_dim=hidden_dim,\n            latent_dim=64,\n            num_embeddings=256\n        )\n\n        sample_input = torch.randn(16, input_dim)\n        output = model(sample_input)\n\n        assert output[0].shape == (16, input_dim)\n</code></pre></p>"},{"location":"zh/contributing/#_15","title":"\u6587\u6863\u8d21\u732e","text":""},{"location":"zh/contributing/#_16","title":"\u6587\u6863\u7ed3\u6784","text":"<pre><code>docs/\n\u251c\u2500\u2500 zh/                         # \u4e2d\u6587\u6587\u6863\n\u2502   \u251c\u2500\u2500 index.md               # \u9996\u9875\n\u2502   \u251c\u2500\u2500 installation.md        # \u5b89\u88c5\u6307\u5357\n\u2502   \u251c\u2500\u2500 quickstart.md          # \u5feb\u901f\u5f00\u59cb\n\u2502   \u251c\u2500\u2500 models/                # \u6a21\u578b\u6587\u6863\n\u2502   \u251c\u2500\u2500 dataset/               # \u6570\u636e\u96c6\u6587\u6863\n\u2502   \u251c\u2500\u2500 training/              # \u8bad\u7ec3\u6307\u5357\n\u2502   \u2514\u2500\u2500 api/                   # API \u53c2\u8003\n\u2514\u2500\u2500 en/                        # \u82f1\u6587\u6587\u6863\n    \u2514\u2500\u2500 ...                    # \u540c\u6837\u7684\u7ed3\u6784\n</code></pre>"},{"location":"zh/contributing/#_17","title":"\u7f16\u5199\u6587\u6863","text":"<ol> <li>\u4f7f\u7528 Markdown \u683c\u5f0f</li> <li>\u5305\u542b\u4ee3\u7801\u793a\u4f8b</li> <li>\u6dfb\u52a0\u9002\u5f53\u7684\u94fe\u63a5</li> <li>\u4fdd\u6301\u7b80\u6d01\u660e\u4e86</li> </ol> <p>\u6587\u6863\u793a\u4f8b\uff1a <pre><code># \u6a21\u578b\u8bad\u7ec3\n\n\u672c\u6307\u5357\u4ecb\u7ecd\u5982\u4f55\u8bad\u7ec3 GenerativeRecommenders \u6a21\u578b\u3002\n\n## \u5feb\u901f\u5f00\u59cb\n\n\u6700\u7b80\u5355\u7684\u8bad\u7ec3\u65b9\u5f0f\uff1a\n\n```python\nfrom generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\n# \u52a0\u8f7d\u6570\u636e\ndataset = P5AmazonItemDataset(root=\"data\", split=\"beauty\")\n\n# \u521b\u5efa\u6a21\u578b\nmodel = RqVae(input_dim=768, num_embeddings=1024)\n\n# \u8bad\u7ec3\uff08\u8be6\u7ec6\u4ee3\u7801\u89c1\u4e0b\u6587\uff09\ntrainer.fit(model, dataloader)\n</code></pre></p>"},{"location":"zh/contributing/#_18","title":"\u8be6\u7ec6\u914d\u7f6e","text":""},{"location":"zh/contributing/#_19","title":"\u6570\u636e\u914d\u7f6e","text":"\u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>root</code> str - \u6570\u636e\u96c6\u6839\u76ee\u5f55 <code>split</code> str \"beauty\" \u6570\u636e\u5206\u5272 <p>\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003 API \u6587\u6863\u3002 <pre><code>### \u672c\u5730\u9884\u89c8\n\n```bash\n# \u5b89\u88c5 MkDocs\npip install mkdocs mkdocs-material\n\n# \u542f\u52a8\u672c\u5730\u670d\u52a1\u5668\nmkdocs serve\n\n# \u5728\u6d4f\u89c8\u5668\u4e2d\u8bbf\u95ee http://127.0.0.1:8000\n</code></pre></p>"},{"location":"zh/contributing/#_20","title":"\u53d1\u5e03\u6d41\u7a0b","text":""},{"location":"zh/contributing/#_21","title":"\u7248\u672c\u7ba1\u7406","text":"<p>\u6211\u4eec\u4f7f\u7528 \u8bed\u4e49\u5316\u7248\u672c\u63a7\u5236\uff1a</p> <ul> <li>MAJOR: \u4e0d\u517c\u5bb9\u7684 API \u66f4\u6539</li> <li>MINOR: \u5411\u540e\u517c\u5bb9\u7684\u529f\u80fd\u6dfb\u52a0</li> <li>PATCH: \u5411\u540e\u517c\u5bb9\u7684\u9519\u8bef\u4fee\u590d</li> </ul>"},{"location":"zh/contributing/#_22","title":"\u53d1\u5e03\u68c0\u67e5\u6e05\u5355","text":"<p>\u53d1\u5e03\u65b0\u7248\u672c\u524d\uff1a</p> <ul> <li> \u6240\u6709\u6d4b\u8bd5\u901a\u8fc7</li> <li> \u6587\u6863\u5df2\u66f4\u65b0</li> <li> CHANGELOG.md \u5df2\u66f4\u65b0</li> <li> \u7248\u672c\u53f7\u5df2\u66f4\u65b0</li> <li> \u521b\u5efa Git \u6807\u7b7e</li> </ul>"},{"location":"zh/contributing/#_23","title":"\u4ee3\u7801\u5ba1\u67e5","text":""},{"location":"zh/contributing/#_24","title":"\u5ba1\u67e5\u6807\u51c6","text":"<p>\u4ee3\u7801\u5ba1\u67e5\u5173\u6ce8\uff1a</p> <ol> <li>\u6b63\u786e\u6027: \u4ee3\u7801\u662f\u5426\u89e3\u51b3\u4e86\u95ee\u9898</li> <li>\u53ef\u8bfb\u6027: \u4ee3\u7801\u662f\u5426\u6613\u4e8e\u7406\u89e3</li> <li>\u53ef\u7ef4\u62a4\u6027: \u4ee3\u7801\u662f\u5426\u6613\u4e8e\u4fee\u6539</li> <li>\u6027\u80fd: \u662f\u5426\u6709\u660e\u663e\u7684\u6027\u80fd\u95ee\u9898</li> <li>\u6d4b\u8bd5: \u662f\u5426\u6709\u5145\u5206\u7684\u6d4b\u8bd5\u8986\u76d6</li> </ol>"},{"location":"zh/contributing/#_25","title":"\u5ba1\u67e5\u6d41\u7a0b","text":"<ol> <li>\u81ea\u52a8\u68c0\u67e5: CI/CD \u7ba1\u9053\u8fd0\u884c\u6d4b\u8bd5</li> <li>\u4ee3\u7801\u5ba1\u67e5: \u7ef4\u62a4\u8005\u5ba1\u67e5\u4ee3\u7801</li> <li>\u53cd\u9988\u548c\u4fee\u6539: \u6839\u636e\u53cd\u9988\u8fdb\u884c\u8c03\u6574</li> <li>\u6279\u51c6\u548c\u5408\u5e76: \u5ba1\u67e5\u901a\u8fc7\u540e\u5408\u5e76</li> </ol>"},{"location":"zh/contributing/#_26","title":"\u83b7\u5f97\u5e2e\u52a9","text":"<p>\u5982\u679c\u60a8\u5728\u8d21\u732e\u8fc7\u7a0b\u4e2d\u9047\u5230\u95ee\u9898\uff1a</p> <ol> <li>\u67e5\u770b\u73b0\u6709\u6587\u6863\u548c FAQ</li> <li>\u641c\u7d22\u73b0\u6709 issues</li> <li>\u5728 GitHub Discussions \u4e2d\u63d0\u95ee</li> <li>\u521b\u5efa\u65b0\u7684 issue</li> </ol>"},{"location":"zh/contributing/#_27","title":"\u884c\u4e3a\u51c6\u5219","text":"<p>\u6211\u4eec\u81f4\u529b\u4e8e\u4e3a\u6bcf\u4e2a\u4eba\u63d0\u4f9b\u53cb\u597d\u3001\u5b89\u5168\u548c\u6b22\u8fce\u7684\u73af\u5883\u3002\u8bf7\uff1a</p> <ul> <li>\u4fdd\u6301\u5c0a\u91cd: \u5c0a\u91cd\u4e0d\u540c\u7684\u89c2\u70b9\u548c\u7ecf\u9a8c</li> <li>\u4fdd\u6301\u5305\u5bb9: \u6b22\u8fce\u65b0\u624b\u548c\u4e0d\u540c\u80cc\u666f\u7684\u8d21\u732e\u8005</li> <li>\u4fdd\u6301\u5efa\u8bbe\u6027: \u63d0\u4f9b\u6709\u5e2e\u52a9\u7684\u53cd\u9988\u548c\u5efa\u8bae</li> <li>\u4fdd\u6301\u4e13\u4e1a: \u4e13\u6ce8\u4e8e\u6280\u672f\u8ba8\u8bba</li> </ul>"},{"location":"zh/contributing/#_28","title":"\u8054\u7cfb\u65b9\u5f0f","text":"<ul> <li>GitHub Issues: \u6280\u672f\u95ee\u9898\u548c Bug \u62a5\u544a</li> <li>GitHub Discussions: \u4e00\u822c\u8ba8\u8bba\u548c\u95ee\u7b54</li> <li>\u90ae\u4ef6: [\u9879\u76ee\u90ae\u7bb1]\uff08\u5982\u679c\u6709\uff09</li> </ul> <p>\u611f\u8c22\u60a8\u5bf9 GenerativeRecommenders \u9879\u76ee\u7684\u8d21\u732e\uff01\ud83c\udf89</p>"},{"location":"zh/deployment/","title":"\u90e8\u7f72\u6307\u5357","text":"<p>\u672c\u6307\u5357\u6db5\u76d6\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72 GenerativeRecommenders \u6a21\u578b\u7684\u65b9\u6cd5\u3002</p>"},{"location":"zh/deployment/#_2","title":"\u751f\u4ea7\u73af\u5883\u90e8\u7f72","text":""},{"location":"zh/deployment/#fastapi","title":"\u4f7f\u7528 FastAPI \u8fdb\u884c\u6a21\u578b\u670d\u52a1","text":"<p>\u521b\u5efa REST API \u670d\u52a1\u5668\u8fdb\u884c\u6a21\u578b\u63a8\u7406\uff1a</p> <pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport torch\nfrom generative_recommenders.models.tiger import Tiger\nfrom generative_recommenders.models.rqvae import RqVae\n\napp = FastAPI(title=\"GenerativeRecommenders API\", version=\"1.0.0\")\n\nclass RecommendationRequest(BaseModel):\n    user_id: int\n    user_history: List[int]\n    num_recommendations: int = 10\n\nclass RecommendationResponse(BaseModel):\n    user_id: int\n    recommendations: List[int]\n    scores: Optional[List[float]] = None\n\nclass ModelService:\n    def __init__(self, rqvae_path: str, tiger_path: str):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # \u52a0\u8f7d\u6a21\u578b\n        self.rqvae = RqVae.load_from_checkpoint(rqvae_path)\n        self.rqvae.to(self.device)\n        self.rqvae.eval()\n\n        self.tiger = Tiger.load_from_checkpoint(tiger_path)\n        self.tiger.to(self.device)\n        self.tiger.eval()\n\n    def get_recommendations(self, user_history: List[int], k: int) -&gt; List[int]:\n        \"\"\"\u4e3a\u7528\u6237\u751f\u6210\u63a8\u8350\"\"\"\n        with torch.no_grad():\n            # \u5c06\u7269\u54c1ID\u8f6c\u6362\u4e3a\u8bed\u4e49ID\n            semantic_sequence = self.items_to_semantic_ids(user_history)\n\n            # \u751f\u6210\u63a8\u8350\n            input_seq = torch.tensor(semantic_sequence).unsqueeze(0).to(self.device)\n            generated = self.tiger.generate(input_seq, max_length=k*3)  # \u751f\u6210\u66f4\u591a\u4ee5\u5904\u7406\u91cd\u590d\n\n            # \u8f6c\u6362\u56de\u7269\u54c1ID\u5e76\u53bb\u91cd\n            recommendations = self.semantic_ids_to_items(generated.squeeze().tolist())\n\n            # \u79fb\u9664\u7528\u6237\u5386\u53f2\u4e2d\u5df2\u6709\u7684\u7269\u54c1\n            recommendations = [item for item in recommendations if item not in user_history]\n\n            return recommendations[:k]\n\n# \u521d\u59cb\u5316\u6a21\u578b\u670d\u52a1\nmodel_service = ModelService(\n    rqvae_path=\"checkpoints/rqvae.ckpt\",\n    tiger_path=\"checkpoints/tiger.ckpt\"\n)\n\n@app.post(\"/recommend\", response_model=RecommendationResponse)\nasync def recommend(request: RecommendationRequest):\n    \"\"\"\u4e3a\u7528\u6237\u751f\u6210\u63a8\u8350\"\"\"\n    try:\n        recommendations = model_service.get_recommendations(\n            request.user_history,\n            request.num_recommendations\n        )\n\n        return RecommendationResponse(\n            user_id=request.user_id,\n            recommendations=recommendations\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"\u5065\u5eb7\u68c0\u67e5\u7aef\u70b9\"\"\"\n    return {\"status\": \"healthy\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"zh/deployment/#docker","title":"Docker \u90e8\u7f72","text":"<p>\u521b\u5efa Dockerfile\uff1a</p> <pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\n# \u5b89\u88c5\u4f9d\u8d56\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# \u590d\u5236\u5e94\u7528\u4ee3\u7801\nCOPY . .\n\n# \u66b4\u9732\u7aef\u53e3\nEXPOSE 8000\n\n# \u8fd0\u884c\u5e94\u7528\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <p>\u6784\u5efa\u548c\u8fd0\u884c\u5bb9\u5668\uff1a</p> <pre><code># \u6784\u5efa\u955c\u50cf\ndocker build -t generative-recommenders:latest .\n\n# \u8fd0\u884c\u5bb9\u5668\ndocker run -d -p 8000:8000 \\\n    -v /path/to/checkpoints:/app/checkpoints \\\n    generative-recommenders:latest\n</code></pre>"},{"location":"zh/deployment/#kubernetes","title":"Kubernetes \u90e8\u7f72","text":"<p>\u521b\u5efa Kubernetes \u914d\u7f6e\uff1a</p> <pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: generative-recommenders\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: generative-recommenders\n  template:\n    metadata:\n      labels:\n        app: generative-recommenders\n    spec:\n      containers:\n      - name: api\n        image: generative-recommenders:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: MODEL_PATH\n          value: \"/models\"\n        volumeMounts:\n        - name: model-storage\n          mountPath: /models\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2000m\"\n      volumes:\n      - name: model-storage\n        persistentVolumeClaim:\n          claimName: model-pvc\n\n---\n# service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: generative-recommenders-service\nspec:\n  selector:\n    app: generative-recommenders\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8000\n  type: LoadBalancer\n</code></pre> <p>\u90e8\u7f72\u5230 Kubernetes\uff1a</p> <pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre>"},{"location":"zh/deployment/#_3","title":"\u6279\u5904\u7406","text":""},{"location":"zh/deployment/#apache-spark","title":"Apache Spark \u96c6\u6210","text":"<p>\u4f7f\u7528 Spark \u5904\u7406\u5927\u578b\u6570\u636e\u96c6\uff1a</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf, explode\nfrom pyspark.sql.types import ArrayType, IntegerType\nimport torch\n\ndef create_spark_session():\n    return SparkSession.builder \\\n        .appName(\"GenerativeRecommenders\") \\\n        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n        .getOrCreate()\n\ndef broadcast_model(spark, model_path):\n    \"\"\"\u5c06\u6a21\u578b\u5e7f\u64ad\u5230\u6240\u6709\u5de5\u4f5c\u8282\u70b9\"\"\"\n    model = Tiger.load_from_checkpoint(model_path)\n    model.eval()\n    return spark.sparkContext.broadcast(model)\n\ndef batch_recommend_udf(broadcast_model):\n    \"\"\"\u6279\u91cf\u63a8\u8350\u7684 UDF\"\"\"\n    @udf(returnType=ArrayType(IntegerType()))\n    def recommend(user_history):\n        model = broadcast_model.value\n        with torch.no_grad():\n            # \u8f6c\u6362\u4e3a\u5f20\u91cf\n            input_seq = torch.tensor(user_history).unsqueeze(0)\n\n            # \u751f\u6210\u63a8\u8350\n            recommendations = model.generate(input_seq, max_length=20)\n\n            return recommendations.squeeze().tolist()\n\n    return recommend\n\n# \u4e3b\u5904\u7406\u6d41\u7a0b\nspark = create_spark_session()\nmodel_broadcast = broadcast_model(spark, \"checkpoints/tiger.ckpt\")\n\n# \u52a0\u8f7d\u7528\u6237\u6570\u636e\nuser_data = spark.read.parquet(\"s3://data/user_interactions\")\n\n# \u751f\u6210\u63a8\u8350\nrecommend_func = batch_recommend_udf(model_broadcast)\nrecommendations = user_data.withColumn(\n    \"recommendations\", \n    recommend_func(col(\"interaction_history\"))\n)\n\n# \u4fdd\u5b58\u7ed3\u679c\nrecommendations.write.mode(\"overwrite\").parquet(\"s3://output/recommendations\")\n</code></pre>"},{"location":"zh/deployment/#apache-airflow","title":"Apache Airflow \u6d41\u6c34\u7ebf","text":"<p>\u521b\u5efa\u63a8\u8350\u6d41\u6c34\u7ebf\uff1a</p> <pre><code>from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'generative_recommenders_pipeline',\n    default_args=default_args,\n    description='\u6bcf\u65e5\u63a8\u8350\u751f\u6210',\n    schedule_interval='@daily',\n    catchup=False\n)\n\ndef extract_user_data(**context):\n    \"\"\"\u63d0\u53d6\u7528\u6237\u4ea4\u4e92\u6570\u636e\"\"\"\n    # \u5b9e\u73b0\u903b\u8f91\n    pass\n\ndef generate_recommendations(**context):\n    \"\"\"\u4f7f\u7528 TIGER \u6a21\u578b\u751f\u6210\u63a8\u8350\"\"\"\n    # \u5b9e\u73b0\u903b\u8f91\n    pass\n\ndef upload_recommendations(**context):\n    \"\"\"\u5c06\u63a8\u8350\u4e0a\u4f20\u5230\u63a8\u8350\u670d\u52a1\"\"\"\n    # \u5b9e\u73b0\u903b\u8f91\n    pass\n\n# \u5b9a\u4e49\u4efb\u52a1\nextract_task = PythonOperator(\n    task_id='extract_user_data',\n    python_callable=extract_user_data,\n    dag=dag\n)\n\nrecommend_task = PythonOperator(\n    task_id='generate_recommendations',\n    python_callable=generate_recommendations,\n    dag=dag\n)\n\nupload_task = PythonOperator(\n    task_id='upload_recommendations',\n    python_callable=upload_recommendations,\n    dag=dag\n)\n\n# \u8bbe\u7f6e\u4f9d\u8d56\u5173\u7cfb\nextract_task &gt;&gt; recommend_task &gt;&gt; upload_task\n</code></pre>"},{"location":"zh/deployment/#_4","title":"\u76d1\u63a7\u548c\u53ef\u89c2\u6d4b\u6027","text":""},{"location":"zh/deployment/#prometheus","title":"Prometheus \u6307\u6807","text":"<p>\u4e3a FastAPI \u5e94\u7528\u6dfb\u52a0\u6307\u6807\uff1a</p> <pre><code>from prometheus_client import Counter, Histogram, generate_latest\nimport time\n\n# \u6307\u6807\nREQUEST_COUNT = Counter('recommendations_requests_total', '\u63a8\u8350\u8bf7\u6c42\u603b\u6570')\nREQUEST_LATENCY = Histogram('recommendations_request_duration_seconds', '\u8bf7\u6c42\u5ef6\u8fdf')\nERROR_COUNT = Counter('recommendations_errors_total', '\u9519\u8bef\u603b\u6570')\n\n@app.middleware(\"http\")\nasync def add_metrics(request, call_next):\n    start_time = time.time()\n    REQUEST_COUNT.inc()\n\n    try:\n        response = await call_next(request)\n        return response\n    except Exception as e:\n        ERROR_COUNT.inc()\n        raise\n    finally:\n        REQUEST_LATENCY.observe(time.time() - start_time)\n\n@app.get(\"/metrics\")\nasync def metrics():\n    \"\"\"Prometheus \u6307\u6807\u7aef\u70b9\"\"\"\n    return Response(generate_latest(), media_type=\"text/plain\")\n</code></pre>"},{"location":"zh/deployment/#_5","title":"\u65e5\u5fd7\u914d\u7f6e","text":"<p>\u8bbe\u7f6e\u7ed3\u6784\u5316\u65e5\u5fd7\uff1a</p> <pre><code>import logging\nimport json\nfrom datetime import datetime\n\nclass JSONFormatter(logging.Formatter):\n    def format(self, record):\n        log_entry = {\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"level\": record.levelname,\n            \"message\": record.getMessage(),\n            \"module\": record.module,\n            \"function\": record.funcName,\n            \"line\": record.lineno\n        }\n\n        if hasattr(record, 'user_id'):\n            log_entry['user_id'] = record.user_id\n\n        if hasattr(record, 'request_id'):\n            log_entry['request_id'] = record.request_id\n\n        return json.dumps(log_entry)\n\n# \u914d\u7f6e\u65e5\u5fd7\nlogging.basicConfig(\n    level=logging.INFO,\n    handlers=[logging.StreamHandler()],\n    format='%(message)s'\n)\n\nlogger = logging.getLogger(__name__)\nfor handler in logger.handlers:\n    handler.setFormatter(JSONFormatter())\n</code></pre>"},{"location":"zh/deployment/#_6","title":"\u6027\u80fd\u4f18\u5316","text":""},{"location":"zh/deployment/#_7","title":"\u6a21\u578b\u91cf\u5316","text":"<p>\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u65f6\u95f4\uff1a</p> <pre><code>import torch.quantization as quantization\n\ndef quantize_model(model, example_inputs):\n    \"\"\"\u91cf\u5316\u6a21\u578b\u4ee5\u52a0\u5feb\u63a8\u7406\u901f\u5ea6\"\"\"\n    # \u51c6\u5907\u6a21\u578b\u8fdb\u884c\u91cf\u5316\n    model.qconfig = quantization.get_default_qconfig('fbgemm')\n    model_prepared = quantization.prepare(model, inplace=False)\n\n    # \u4f7f\u7528\u793a\u4f8b\u8f93\u5165\u8fdb\u884c\u6821\u51c6\n    model_prepared.eval()\n    with torch.no_grad():\n        model_prepared(example_inputs)\n\n    # \u8f6c\u6362\u4e3a\u91cf\u5316\u6a21\u578b\n    model_quantized = quantization.convert(model_prepared, inplace=False)\n\n    return model_quantized\n\n# \u4f7f\u7528\u793a\u4f8b\nexample_input = torch.randint(0, 1000, (1, 50))\nquantized_tiger = quantize_model(tiger_model, example_input)\n</code></pre>"},{"location":"zh/deployment/#onnx","title":"ONNX \u5bfc\u51fa","text":"<p>\u5c06\u6a21\u578b\u5bfc\u51fa\u4e3a ONNX \u683c\u5f0f\u4ee5\u5b9e\u73b0\u8de8\u5e73\u53f0\u90e8\u7f72\uff1a</p> <pre><code>def export_to_onnx(model, example_input, output_path):\n    \"\"\"\u5c06 PyTorch \u6a21\u578b\u5bfc\u51fa\u4e3a ONNX\"\"\"\n    model.eval()\n\n    torch.onnx.export(\n        model,\n        example_input,\n        output_path,\n        export_params=True,\n        opset_version=11,\n        do_constant_folding=True,\n        input_names=['input_ids'],\n        output_names=['output'],\n        dynamic_axes={\n            'input_ids': {0: 'batch_size', 1: 'sequence'},\n            'output': {0: 'batch_size', 1: 'sequence'}\n        }\n    )\n\n# \u5bfc\u51fa\u6a21\u578b\nexport_to_onnx(tiger_model, example_input, \"models/tiger.onnx\")\n</code></pre>"},{"location":"zh/deployment/#tensorrt","title":"TensorRT \u4f18\u5316","text":"<p>\u4e3a NVIDIA GPU \u4f18\u5316\u6a21\u578b\uff1a</p> <pre><code>import tensorrt as trt\n\ndef convert_onnx_to_tensorrt(onnx_path, engine_path, max_batch_size=32):\n    \"\"\"\u5c06 ONNX \u6a21\u578b\u8f6c\u6362\u4e3a TensorRT \u5f15\u64ce\"\"\"\n    logger = trt.Logger(trt.Logger.WARNING)\n    builder = trt.Builder(logger)\n    network = builder.create_network(1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n    parser = trt.OnnxParser(network, logger)\n\n    # \u89e3\u6790 ONNX \u6a21\u578b\n    with open(onnx_path, 'rb') as model:\n        if not parser.parse(model.read()):\n            for error in range(parser.num_errors):\n                print(parser.get_error(error))\n            return None\n\n    # \u6784\u5efa\u5f15\u64ce\n    config = builder.create_builder_config()\n    config.max_workspace_size = 1 &lt;&lt; 30  # 1GB\n\n    profile = builder.create_optimization_profile()\n    profile.set_shape(\"input_ids\", (1, 1), (max_batch_size, 512), (max_batch_size, 512))\n    config.add_optimization_profile(profile)\n\n    engine = builder.build_engine(network, config)\n\n    # \u4fdd\u5b58\u5f15\u64ce\n    with open(engine_path, 'wb') as f:\n        f.write(engine.serialize())\n\n    return engine\n</code></pre>"},{"location":"zh/deployment/#ab","title":"A/B \u6d4b\u8bd5\u6846\u67b6","text":""},{"location":"zh/deployment/#_8","title":"\u5b9e\u9a8c\u914d\u7f6e","text":"<p>\u8bbe\u7f6e A/B \u6d4b\u8bd5\u8fdb\u884c\u6a21\u578b\u6bd4\u8f83\uff1a</p> <pre><code>import hashlib\nimport random\nfrom typing import Dict, Any\n\nclass ABTestFramework:\n    def __init__(self, experiments: Dict[str, Any]):\n        self.experiments = experiments\n\n    def get_variant(self, user_id: int, experiment_name: str) -&gt; str:\n        \"\"\"\u83b7\u53d6\u7528\u6237\u5728\u5b9e\u9a8c\u4e2d\u7684\u53d8\u4f53\"\"\"\n        if experiment_name not in self.experiments:\n            return \"control\"\n\n        experiment = self.experiments[experiment_name]\n\n        # \u4f7f\u7528\u4e00\u81f4\u6027\u54c8\u5e0c\u8fdb\u884c\u7528\u6237\u5206\u914d\n        hash_input = f\"{user_id}_{experiment_name}_{experiment['salt']}\"\n        hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)\n        bucket = hash_value % 100\n\n        cumulative_traffic = 0\n        for variant, traffic in experiment['variants'].items():\n            cumulative_traffic += traffic\n            if bucket &lt; cumulative_traffic:\n                return variant\n\n        return \"control\"\n\n    def is_user_in_experiment(self, user_id: int, experiment_name: str) -&gt; bool:\n        \"\"\"\u68c0\u67e5\u7528\u6237\u662f\u5426\u5728\u5b9e\u9a8c\u4e2d\"\"\"\n        if experiment_name not in self.experiments:\n            return False\n\n        experiment = self.experiments[experiment_name]\n        if not experiment.get('active', False):\n            return False\n\n        # \u68c0\u67e5\u8d44\u683c\u6761\u4ef6\n        if 'eligibility' in experiment:\n            # \u5b9e\u73b0\u8d44\u683c\u903b\u8f91\n            pass\n\n        return True\n\n# \u5b9e\u9a8c\u914d\u7f6e\u793a\u4f8b\nexperiments_config = {\n    \"model_comparison\": {\n        \"active\": True,\n        \"salt\": \"experiment_salt_123\",\n        \"variants\": {\n            \"control\": 50,  # 50% \u6d41\u91cf\n            \"new_model\": 50  # 50% \u6d41\u91cf\n        },\n        \"eligibility\": {\n            \"min_interactions\": 10\n        }\n    }\n}\n\nab_tester = ABTestFramework(experiments_config)\n\n@app.post(\"/recommend\")\nasync def recommend_with_ab_test(request: RecommendationRequest):\n    \"\"\"\u4f7f\u7528 A/B \u6d4b\u8bd5\u751f\u6210\u63a8\u8350\"\"\"\n    variant = ab_tester.get_variant(request.user_id, \"model_comparison\")\n\n    if variant == \"new_model\":\n        # \u4f7f\u7528\u65b0\u6a21\u578b\n        recommendations = new_model_service.get_recommendations(\n            request.user_history, request.num_recommendations\n        )\n    else:\n        # \u4f7f\u7528\u5bf9\u7167\u6a21\u578b\n        recommendations = model_service.get_recommendations(\n            request.user_history, request.num_recommendations\n        )\n\n    # \u8bb0\u5f55\u5b9e\u9a8c\u6570\u636e\n    logger.info(\"\u63a8\u8350\u5df2\u63d0\u4f9b\", extra={\n        \"user_id\": request.user_id,\n        \"variant\": variant,\n        \"experiment\": \"model_comparison\"\n    })\n\n    return RecommendationResponse(\n        user_id=request.user_id,\n        recommendations=recommendations\n    )\n</code></pre>"},{"location":"zh/deployment/#_9","title":"\u5b89\u5168\u8003\u8651","text":""},{"location":"zh/deployment/#api","title":"API \u8ba4\u8bc1","text":"<p>\u6dfb\u52a0 JWT \u8ba4\u8bc1\uff1a</p> <pre><code>from fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nimport jwt\n\nsecurity = HTTPBearer()\n\ndef verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    \"\"\"\u9a8c\u8bc1 JWT \u4ee4\u724c\"\"\"\n    try:\n        payload = jwt.decode(\n            credentials.credentials,\n            SECRET_KEY,\n            algorithms=[\"HS256\"]\n        )\n        return payload\n    except jwt.PyJWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"\u65e0\u6548\u7684\u8ba4\u8bc1\u51ed\u636e\"\n        )\n\n@app.post(\"/recommend\")\nasync def recommend(\n    request: RecommendationRequest,\n    token_data: dict = Depends(verify_token)\n):\n    \"\"\"\u53d7\u4fdd\u62a4\u7684\u63a8\u8350\u7aef\u70b9\"\"\"\n    # \u9a8c\u8bc1\u7528\u6237\u8bbf\u95ee\u6743\u9650\n    if token_data.get(\"user_id\") != request.user_id:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"\u8bbf\u95ee\u88ab\u62d2\u7edd\"\n        )\n\n    return await model_service.get_recommendations(request)\n</code></pre>"},{"location":"zh/deployment/#_10","title":"\u8f93\u5165\u9a8c\u8bc1","text":"<p>\u9a8c\u8bc1\u548c\u6e05\u7406\u8f93\u5165\uff1a</p> <pre><code>from pydantic import validator\n\nclass RecommendationRequest(BaseModel):\n    user_id: int\n    user_history: List[int]\n    num_recommendations: int = 10\n\n    @validator('user_id')\n    def validate_user_id(cls, v):\n        if v &lt;= 0:\n            raise ValueError('\u7528\u6237ID\u5fc5\u987b\u4e3a\u6b63\u6570')\n        return v\n\n    @validator('user_history')\n    def validate_user_history(cls, v):\n        if len(v) &gt; 1000:  # \u9650\u5236\u5386\u53f2\u957f\u5ea6\n            raise ValueError('\u7528\u6237\u5386\u53f2\u8fc7\u957f')\n        if any(item &lt;= 0 for item in v):\n            raise ValueError('\u5386\u53f2\u4e2d\u5305\u542b\u65e0\u6548\u7684\u7269\u54c1ID')\n        return v\n\n    @validator('num_recommendations')\n    def validate_num_recommendations(cls, v):\n        if not 1 &lt;= v &lt;= 100:\n            raise ValueError('\u63a8\u8350\u6570\u91cf\u5fc5\u987b\u57281\u5230100\u4e4b\u95f4')\n        return v\n</code></pre> <p>\u672c\u90e8\u7f72\u6307\u5357\u6db5\u76d6\u4e86\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72 GenerativeRecommenders \u6a21\u578b\u7684\u91cd\u8981\u65b9\u9762\uff0c\u4ece\u57fa\u672c\u7684 API \u670d\u52a1\u5230\u9ad8\u7ea7\u7684\u4f18\u5316\u548c\u76d1\u63a7\u6280\u672f\u3002</p>"},{"location":"zh/examples/","title":"\u793a\u4f8b\u4ee3\u7801","text":"<p>\u672c\u9875\u9762\u5305\u542b\u4f7f\u7528 GenerativeRecommenders \u7684\u5b9e\u7528\u793a\u4f8b\u3002</p>"},{"location":"zh/examples/#_2","title":"\u57fa\u7840\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/examples/#rqvae","title":"\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3 RQVAE","text":"<pre><code>import torch\nfrom generative_recommenders.models.rqvae import RqVae, QuantizeForwardMode\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\nfrom torch.utils.data import DataLoader\n\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n\n# \u521b\u5efa\u6a21\u578b\nmodel = RqVae(\n    input_dim=768,\n    embed_dim=32,\n    hidden_dims=[512, 256, 128],\n    codebook_size=256,\n    n_layers=3,\n    commitment_weight=0.25,\n    codebook_mode=QuantizeForwardMode.ROTATION_TRICK\n)\n\n# \u8bad\u7ec3\u5faa\u73af\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n\nfor epoch in range(100):\n    for batch in dataloader:\n        optimizer.zero_grad()\n\n        outputs = model(torch.tensor(batch))\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n</code></pre>"},{"location":"zh/examples/#_3","title":"\u4f7f\u7528\u6570\u636e\u96c6\u5de5\u5382","text":"<pre><code>from generative_recommenders.data.dataset_factory import DatasetFactory\n\n# \u521b\u5efa\u7269\u54c1\u6570\u636e\u96c6\nitem_dataset = DatasetFactory.create_item_dataset(\n    \"p5_amazon\",\n    \"dataset/amazon\",\n    split=\"train\"\n)\n\n# \u521b\u5efa\u5e8f\u5217\u6570\u636e\u96c6\nsequence_dataset = DatasetFactory.create_sequence_dataset(\n    \"p5_amazon\", \n    \"dataset/amazon\",\n    split=\"train\",\n    pretrained_rqvae_path=\"./checkpoints/rqvae.pt\"\n)\n</code></pre>"},{"location":"zh/examples/#_4","title":"\u81ea\u5b9a\u4e49\u914d\u7f6e","text":"<pre><code>from generative_recommenders.data.configs import P5AmazonConfig, TextEncodingConfig\n\n# \u81ea\u5b9a\u4e49\u6587\u672c\u7f16\u7801\u914d\u7f6e\ntext_config = TextEncodingConfig(\n    encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    template=\"\u4ea7\u54c1: {title} | \u54c1\u724c: {brand} | \u7c7b\u522b: {categories}\",\n    batch_size=32\n)\n\n# \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u914d\u7f6e\ndataset_config = P5AmazonConfig(\n    root_dir=\"my_data\",\n    split=\"electronics\",\n    text_config=text_config\n)\n</code></pre>"},{"location":"zh/examples/#_5","title":"\u9ad8\u7ea7\u793a\u4f8b","text":""},{"location":"zh/examples/#gpu","title":"\u591a GPU \u8bad\u7ec3","text":"<pre><code>from accelerate import Accelerator\n\ndef train_with_accelerate():\n    accelerator = Accelerator()\n\n    # \u6a21\u578b\u3001\u4f18\u5316\u5668\u3001\u6570\u636e\u52a0\u8f7d\u5668\n    model = RqVae(...)\n    optimizer = torch.optim.AdamW(model.parameters())\n    dataloader = DataLoader(...)\n\n    # \u51c6\u5907\u5206\u5e03\u5f0f\u8bad\u7ec3\n    model, optimizer, dataloader = accelerator.prepare(\n        model, optimizer, dataloader\n    )\n\n    for epoch in range(epochs):\n        for batch in dataloader:\n            optimizer.zero_grad()\n\n            with accelerator.autocast():\n                outputs = model(batch)\n                loss = outputs.loss\n\n            accelerator.backward(loss)\n            optimizer.step()\n</code></pre>"},{"location":"zh/examples/#_6","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u5b9e\u73b0","text":"<pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\n\nclass MyCustomDataset(BaseRecommenderDataset):\n    def download(self):\n        # \u5b9e\u73b0\u6570\u636e\u4e0b\u8f7d\u903b\u8f91\n        pass\n\n    def load_raw_data(self):\n        # \u52a0\u8f7d\u539f\u59cb\u6570\u636e\u6587\u4ef6\n        return {\"items\": items_df, \"interactions\": interactions_df}\n\n    def preprocess_data(self, raw_data):\n        # \u81ea\u5b9a\u4e49\u9884\u5904\u7406\n        return processed_data\n\n    def extract_items(self, processed_data):\n        return processed_data[\"items\"]\n\n    def extract_interactions(self, processed_data):\n        return processed_data[\"interactions\"]\n</code></pre>"},{"location":"zh/examples/#_7","title":"\u96c6\u6210\u793a\u4f8b","text":""},{"location":"zh/examples/#weights-biases","title":"Weights &amp; Biases \u96c6\u6210","text":"<pre><code>import wandb\n\n# \u521d\u59cb\u5316 wandb\nwandb.init(\n    project=\"my-recommendation-project\",\n    config={\n        \"learning_rate\": 0.0005,\n        \"batch_size\": 64,\n        \"model_type\": \"rqvae\"\n    }\n)\n\n# \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8bb0\u5f55\u6307\u6807\nfor epoch in range(epochs):\n    # ... \u8bad\u7ec3\u4ee3\u7801 ...\n\n    wandb.log({\n        \"epoch\": epoch,\n        \"loss\": loss.item(),\n        \"reconstruction_loss\": recon_loss.item(),\n        \"quantization_loss\": quant_loss.item()\n    })\n</code></pre>"},{"location":"zh/examples/#_8","title":"\u8d85\u53c2\u6570\u8c03\u4f18","text":"<pre><code>import optuna\n\ndef objective(trial):\n    # \u5efa\u8bae\u8d85\u53c2\u6570\n    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n    embed_dim = trial.suggest_categorical(\"embed_dim\", [16, 32, 64])\n\n    # \u4f7f\u7528\u5efa\u8bae\u7684\u53c2\u6570\u8bad\u7ec3\u6a21\u578b\n    model = RqVae(embed_dim=embed_dim, ...)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\n    # \u8bad\u7ec3\u5faa\u73af\n    val_loss = train_and_evaluate(model, optimizer, batch_size)\n\n    return val_loss\n\n# \u8fd0\u884c\u4f18\u5316\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=100)\n</code></pre>"},{"location":"zh/examples/#_9","title":"\u8bc4\u4f30\u793a\u4f8b","text":""},{"location":"zh/examples/#_10","title":"\u6a21\u578b\u8bc4\u4f30","text":"<pre><code>def evaluate_model(model, test_dataloader, device):\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            batch = batch.to(device)\n            outputs = model(batch)\n\n            total_loss += outputs.loss.item() * len(batch)\n            total_samples += len(batch)\n\n    return total_loss / total_samples\n\n# \u8bc4\u4f30 RQVAE\ntest_loss = evaluate_model(rqvae_model, test_dataloader, device)\nprint(f\"\u6d4b\u8bd5\u91cd\u6784\u635f\u5931: {test_loss:.4f}\")\n</code></pre>"},{"location":"zh/examples/#_11","title":"\u63a8\u8350\u751f\u6210","text":"<pre><code>def generate_recommendations(tiger_model, user_sequence, top_k=10):\n    \"\"\"\u4e3a\u7528\u6237\u5e8f\u5217\u751f\u6210 Top-K \u63a8\u8350\"\"\"\n    tiger_model.eval()\n\n    with torch.no_grad():\n        # \u7f16\u7801\u7528\u6237\u5e8f\u5217\n        logits = tiger_model.generate(user_sequence, max_length=top_k)\n\n        # \u83b7\u53d6 Top-K \u7269\u54c1\n        top_items = torch.topk(logits, top_k).indices\n\n    return top_items.tolist()\n\n# \u751f\u6210\u63a8\u8350\nuser_seq = [1, 5, 23, 45]  # \u7528\u6237\u4ea4\u4e92\u5386\u53f2\nrecommendations = generate_recommendations(tiger_model, user_seq, top_k=10)\nprint(f\"\u63a8\u8350\u7269\u54c1: {recommendations}\")\n</code></pre>"},{"location":"zh/examples/#_12","title":"\u5b9e\u7528\u5de5\u5177","text":""},{"location":"zh/examples/#_13","title":"\u6570\u636e\u5206\u6790","text":"<pre><code>from generative_recommenders.data.processors.sequence_processor import SequenceStatistics\n\n# \u5206\u6790\u5e8f\u5217\u7edf\u8ba1\u4fe1\u606f\nstats = SequenceStatistics.compute_sequence_stats(sequence_data)\nprint(f\"\u5e73\u5747\u5e8f\u5217\u957f\u5ea6: {stats['avg_seq_length']:.2f}\")\nprint(f\"\u552f\u4e00\u7269\u54c1\u6570\u91cf: {stats['num_unique_items']}\")\n</code></pre>"},{"location":"zh/examples/#_14","title":"\u6a21\u578b\u68c0\u67e5","text":"<pre><code>def inspect_codebook_usage(rqvae_model, dataloader):\n    \"\"\"\u5206\u6790\u7801\u672c\u5229\u7528\u7387\"\"\"\n    used_codes = set()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            outputs = rqvae_model(batch)\n            semantic_ids = outputs.sem_ids\n            used_codes.update(semantic_ids.flatten().tolist())\n\n    usage_rate = len(used_codes) / rqvae_model.codebook_size\n    print(f\"\u7801\u672c\u5229\u7528\u7387: {usage_rate:.2%}\")\n\n    return used_codes\n\nused_codes = inspect_codebook_usage(model, dataloader)\n</code></pre>"},{"location":"zh/examples/#_15","title":"\u6280\u5de7\u548c\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"zh/examples/#_16","title":"\u5185\u5b58\u4f18\u5316","text":"<pre><code># \u4e3a\u5927\u578b\u6a21\u578b\u542f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9\nmodel.gradient_checkpointing_enable()\n\n# \u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor batch in dataloader:\n    optimizer.zero_grad()\n\n    with autocast():\n        outputs = model(batch)\n        loss = outputs.loss\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre>"},{"location":"zh/examples/#_17","title":"\u8c03\u8bd5","text":"<pre><code># \u542f\u7528\u8be6\u7ec6\u65e5\u5fd7\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n# \u8bb0\u5f55\u6a21\u578b\u7edf\u8ba1\u4fe1\u606f\ndef log_model_stats(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    logger.info(f\"\u603b\u53c2\u6570\u91cf: {total_params:,}\")\n    logger.info(f\"\u53ef\u8bad\u7ec3\u53c2\u6570\u91cf: {trainable_params:,}\")\n</code></pre>"},{"location":"zh/faq/","title":"\u5e38\u89c1\u95ee\u9898 (FAQ)","text":"<p>\u672c\u9875\u9762\u6536\u96c6\u4e86\u4f7f\u7528 GenerativeRecommenders \u6846\u67b6\u65f6\u7684\u5e38\u89c1\u95ee\u9898\u548c\u89e3\u7b54\u3002</p>"},{"location":"zh/faq/#_1","title":"\u5b89\u88c5\u548c\u73af\u5883","text":""},{"location":"zh/faq/#q-generativerecommenders","title":"Q: \u5982\u4f55\u5b89\u88c5 GenerativeRecommenders\uff1f","text":"<p>A: \u76ee\u524d\u652f\u6301\u4ece\u6e90\u4ee3\u7801\u5b89\u88c5\uff1a</p> <pre><code>git clone https://github.com/phonism/GenerativeRecommenders.git\ncd GenerativeRecommenders\npip install -e .\n</code></pre>"},{"location":"zh/faq/#q-python","title":"Q: \u652f\u6301\u54ea\u4e9b Python \u7248\u672c\uff1f","text":"<p>A: \u63a8\u8350\u4f7f\u7528 Python 3.8 \u6216\u66f4\u9ad8\u7248\u672c\u3002\u6846\u67b6\u5728 Python 3.8\u30013.9\u30013.10 \u4e0a\u7ecf\u8fc7\u6d4b\u8bd5\u3002</p>"},{"location":"zh/faq/#q","title":"Q: \u9700\u8981\u54ea\u4e9b\u4e3b\u8981\u4f9d\u8d56\uff1f","text":"<p>A: \u4e3b\u8981\u4f9d\u8d56\u5305\u62ec\uff1a - PyTorch &gt;= 1.11.0 - PyTorch Lightning &gt;= 1.6.0 - sentence-transformers &gt;= 2.2.0 - pandas &gt;= 1.3.0 - numpy &gt;= 1.21.0</p>"},{"location":"zh/faq/#q-gpu","title":"Q: \u662f\u5426\u652f\u6301 GPU \u8bad\u7ec3\uff1f","text":"<p>A: \u662f\u7684\uff0c\u6846\u67b6\u5b8c\u5168\u652f\u6301 GPU \u8bad\u7ec3\u3002\u786e\u4fdd\u5b89\u88c5\u4e86\u6b63\u786e\u7684 PyTorch CUDA \u7248\u672c\u3002</p>"},{"location":"zh/faq/#_2","title":"\u6570\u636e\u548c\u6570\u636e\u96c6","text":""},{"location":"zh/faq/#q_1","title":"Q: \u652f\u6301\u54ea\u4e9b\u6570\u636e\u96c6\u683c\u5f0f\uff1f","text":"<p>A: \u6846\u67b6\u652f\u6301\uff1a - JSON \u683c\u5f0f\u7684\u63a8\u8350\u6570\u636e\u96c6 - CSV \u683c\u5f0f\u7684\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u6570\u636e - Parquet \u683c\u5f0f\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6 - \u81ea\u5b9a\u4e49\u683c\u5f0f\uff08\u901a\u8fc7\u7ee7\u627f\u57fa\u7c7b\u5b9e\u73b0\uff09</p>"},{"location":"zh/faq/#q_2","title":"Q: \u5982\u4f55\u6dfb\u52a0\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff1f","text":"<p>A: \u7ee7\u627f <code>BaseRecommenderDataset</code> \u7c7b\u5e76\u5b9e\u73b0\u5fc5\u8981\u65b9\u6cd5\uff1a</p> <pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\n\nclass MyDataset(BaseRecommenderDataset):\n    def download(self):\n        # \u5b9e\u73b0\u6570\u636e\u4e0b\u8f7d\u903b\u8f91\n        pass\n\n    def load_raw_data(self):\n        # \u5b9e\u73b0\u6570\u636e\u52a0\u8f7d\u903b\u8f91\n        return {\"items\": items_df, \"interactions\": interactions_df}\n\n    def preprocess_data(self, raw_data):\n        # \u5b9e\u73b0\u6570\u636e\u9884\u5904\u7406\u903b\u8f91\n        return processed_data\n</code></pre>"},{"location":"zh/faq/#q-p5-amazon","title":"Q: P5 Amazon \u6570\u636e\u96c6\u6709\u591a\u5927\uff1f","text":"<p>A: \u4e0d\u540c\u7c7b\u522b\u7684\u5927\u5c0f\u4e0d\u540c\uff1a - Beauty: ~500MB - Electronics: ~2GB - Sports: ~1GB - \u5b8c\u6574\u6570\u636e\u96c6\u53ef\u80fd\u9700\u8981 10GB+ \u7684\u5b58\u50a8\u7a7a\u95f4</p>"},{"location":"zh/faq/#q_3","title":"Q: \u5982\u4f55\u5904\u7406\u7f3a\u5931\u7684\u7269\u54c1\u7279\u5f81\uff1f","text":"<p>A: \u6846\u67b6\u81ea\u52a8\u5904\u7406\u7f3a\u5931\u7279\u5f81\uff1a - \u6587\u672c\u5b57\u6bb5\u7528 \"Unknown\" \u586b\u5145 - \u6570\u503c\u5b57\u6bb5\u7528\u5747\u503c\u6216 0 \u586b\u5145 - \u53ef\u4ee5\u5728\u914d\u7f6e\u4e2d\u81ea\u5b9a\u4e49\u586b\u5145\u7b56\u7565</p>"},{"location":"zh/faq/#_3","title":"\u6a21\u578b\u8bad\u7ec3","text":""},{"location":"zh/faq/#q-rqvae","title":"Q: RQVAE \u8bad\u7ec3\u9700\u8981\u591a\u957f\u65f6\u95f4\uff1f","text":"<p>A: \u53d6\u51b3\u4e8e\u6570\u636e\u96c6\u5927\u5c0f\u548c\u786c\u4ef6\uff1a - \u5c0f\u6570\u636e\u96c6\uff08&lt;10\u4e07\u7269\u54c1\uff09\uff1a30\u5206\u949f - 2\u5c0f\u65f6 - \u4e2d\u7b49\u6570\u636e\u96c6\uff0810-100\u4e07\u7269\u54c1\uff09\uff1a2-8\u5c0f\u65f6 - \u5927\u6570\u636e\u96c6\uff08&gt;100\u4e07\u7269\u54c1\uff09\uff1a8-24\u5c0f\u65f6</p>"},{"location":"zh/faq/#q-tiger","title":"Q: TIGER \u8bad\u7ec3\u7684\u5185\u5b58\u8981\u6c42\u662f\u4ec0\u4e48\uff1f","text":"<p>A: \u5178\u578b\u5185\u5b58\u4f7f\u7528\uff1a - \u6700\u5c0f\uff1a8GB GPU \u5185\u5b58\uff08\u5c0f\u6279\u91cf\u5927\u5c0f\uff09 - \u63a8\u8350\uff1a16GB GPU \u5185\u5b58\uff08\u4e2d\u7b49\u6279\u91cf\u5927\u5c0f\uff09 - \u5927\u89c4\u6a21\uff1a32GB+ GPU \u5185\u5b58\uff08\u5927\u6279\u91cf\u5927\u5c0f\uff09</p>"},{"location":"zh/faq/#q_4","title":"Q: \u5982\u4f55\u9009\u62e9\u5408\u9002\u7684\u5d4c\u5165\u7ef4\u5ea6\uff1f","text":"<p>A: \u7ecf\u9a8c\u6cd5\u5219\uff1a - \u5c0f\u6570\u636e\u96c6\uff1a256-512 \u7ef4 - \u4e2d\u7b49\u6570\u636e\u96c6\uff1a512-768 \u7ef4 - \u5927\u6570\u636e\u96c6\uff1a768-1024 \u7ef4 - \u5177\u4f53\u9009\u62e9\u5e94\u57fa\u4e8e\u9a8c\u8bc1\u96c6\u6027\u80fd</p>"},{"location":"zh/faq/#q-cuda","title":"Q: \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0 CUDA \u5185\u5b58\u4e0d\u8db3\u600e\u4e48\u529e\uff1f","text":"<p>A: \u89e3\u51b3\u65b9\u6cd5\uff1a 1. \u51cf\u5c11\u6279\u91cf\u5927\u5c0f 2. \u4f7f\u7528\u68af\u5ea6\u7d2f\u79ef 3. \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 4. \u51cf\u5c11\u6a21\u578b\u5c3a\u5bf8</p> <pre><code># \u51cf\u5c11\u6279\u91cf\u5927\u5c0f\nconfig.batch_size = 16\n\n# \u68af\u5ea6\u7d2f\u79ef\nconfig.accumulate_grad_batches = 4\n\n# \u6df7\u5408\u7cbe\u5ea6\nconfig.precision = 16\n</code></pre>"},{"location":"zh/faq/#_4","title":"\u6a21\u578b\u4f7f\u7528","text":""},{"location":"zh/faq/#q_5","title":"Q: \u5982\u4f55\u751f\u6210\u63a8\u8350\uff1f","text":"<p>A: \u57fa\u672c\u63a8\u8350\u751f\u6210\uff1a</p> <pre><code># \u52a0\u8f7d\u6a21\u578b\nrqvae = RqVae.load_from_checkpoint(\"rqvae.ckpt\")\ntiger = Tiger.load_from_checkpoint(\"tiger.ckpt\")\n\n# \u7528\u6237\u5386\u53f2\uff08\u7269\u54c1ID\uff09\nuser_history = [1, 5, 23, 67]\n\n# \u8f6c\u6362\u4e3a\u8bed\u4e49ID\nsemantic_ids = rqvae.encode_items(user_history)\n\n# \u751f\u6210\u63a8\u8350\nrecommendations = tiger.generate(semantic_ids, max_length=10)\n</code></pre>"},{"location":"zh/faq/#q_6","title":"Q: \u5982\u4f55\u5904\u7406\u51b7\u542f\u52a8\u7528\u6237\uff1f","text":"<p>A: \u5bf9\u4e8e\u65b0\u7528\u6237\uff1a 1. \u4f7f\u7528\u6d41\u884c\u7269\u54c1\u63a8\u8350 2. \u57fa\u4e8e\u7528\u6237\u753b\u50cf\u7684\u5185\u5bb9\u63a8\u8350 3. \u57fa\u4e8e\u7269\u54c1\u7279\u5f81\u7684\u76f8\u4f3c\u5ea6\u63a8\u8350</p> <pre><code>def recommend_for_new_user(user_profile, k=10):\n    # \u57fa\u4e8e\u7528\u6237\u753b\u50cf\u627e\u76f8\u4f3c\u7269\u54c1\n    similar_items = find_similar_items_by_profile(user_profile)\n    return similar_items[:k]\n</code></pre>"},{"location":"zh/faq/#q_7","title":"Q: \u63a8\u8350\u7ed3\u679c\u7684\u591a\u6837\u6027\u5982\u4f55\u4fdd\u8bc1\uff1f","text":"<p>A: \u63d0\u9ad8\u591a\u6837\u6027\u7684\u65b9\u6cd5\uff1a 1. \u4f7f\u7528 Top-p \u91c7\u6837\u800c\u4e0d\u662f\u8d2a\u5fc3\u91c7\u6837 2. \u540e\u5904\u7406\u53bb\u91cd\u548c\u591a\u6837\u5316 3. \u5728\u8bad\u7ec3\u65f6\u52a0\u5165\u591a\u6837\u6027\u635f\u5931</p> <pre><code># \u4f7f\u7528\u91c7\u6837\u751f\u6210\nrecommendations = tiger.generate(\n    input_seq, \n    temperature=0.8,\n    top_p=0.9,\n    do_sample=True\n)\n</code></pre>"},{"location":"zh/faq/#_5","title":"\u6027\u80fd\u548c\u4f18\u5316","text":""},{"location":"zh/faq/#q_8","title":"Q: \u5982\u4f55\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\uff1f","text":"<p>A: \u4f18\u5316\u65b9\u6cd5\uff1a 1. \u6a21\u578b\u91cf\u5316 2. ONNX \u5bfc\u51fa 3. TensorRT \u4f18\u5316 4. \u6279\u91cf\u63a8\u7406</p> <pre><code># \u6a21\u578b\u91cf\u5316\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\n\n# \u6279\u91cf\u63a8\u7406\ndef batch_recommend(user_histories, batch_size=32):\n    results = []\n    for i in range(0, len(user_histories), batch_size):\n        batch = user_histories[i:i+batch_size]\n        batch_results = model.batch_generate(batch)\n        results.extend(batch_results)\n    return results\n</code></pre>"},{"location":"zh/faq/#q_9","title":"Q: \u6a21\u578b\u5927\u5c0f\u53ef\u4ee5\u538b\u7f29\u5417\uff1f","text":"<p>A: \u538b\u7f29\u6280\u672f\uff1a - \u91cf\u5316\uff1a\u51cf\u5c11 50-75% \u5927\u5c0f - \u526a\u679d\uff1a\u79fb\u9664\u4e0d\u91cd\u8981\u7684\u53c2\u6570 - \u77e5\u8bc6\u84b8\u998f\uff1a\u8bad\u7ec3\u5c0f\u6a21\u578b\u6a21\u4eff\u5927\u6a21\u578b</p>"},{"location":"zh/faq/#q_10","title":"Q: \u5982\u4f55\u76d1\u63a7\u6a21\u578b\u6027\u80fd\uff1f","text":"<p>A: \u76d1\u63a7\u6307\u6807\uff1a - \u63a8\u7406\u5ef6\u8fdf - \u5185\u5b58\u4f7f\u7528 - GPU \u5229\u7528\u7387 - \u63a8\u8350\u8d28\u91cf\u6307\u6807\uff08Recall\u3001NDCG\uff09</p>"},{"location":"zh/faq/#_6","title":"\u9519\u8bef\u548c\u8c03\u8bd5","text":""},{"location":"zh/faq/#q-runtimeerror-cuda-out-of-memory","title":"Q: \u8bad\u7ec3\u65f6\u51fa\u73b0 \"RuntimeError: CUDA out of memory\" \u9519\u8bef\uff1f","text":"<p>A: \u89e3\u51b3\u6b65\u9aa4\uff1a 1. \u51cf\u5c11 batch_size 2. \u542f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9 3. \u6e05\u7406 GPU \u7f13\u5b58</p> <pre><code>import torch\ntorch.cuda.empty_cache()\n\n# \u6216\u5728\u8bad\u7ec3\u914d\u7f6e\u4e2d\nconfig.gradient_checkpointing = True\n</code></pre>"},{"location":"zh/faq/#q_11","title":"Q: \u6a21\u578b\u52a0\u8f7d\u5931\u8d25\u600e\u4e48\u529e\uff1f","text":"<p>A: \u68c0\u67e5\u9879\uff1a 1. \u68c0\u67e5\u70b9\u6587\u4ef6\u662f\u5426\u5b8c\u6574 2. PyTorch \u7248\u672c\u517c\u5bb9\u6027 3. \u6a21\u578b\u67b6\u6784\u662f\u5426\u5339\u914d</p> <pre><code>try:\n    model = Model.load_from_checkpoint(checkpoint_path)\nexcept Exception as e:\n    print(f\"\u52a0\u8f7d\u5931\u8d25: {e}\")\n    # \u5c1d\u8bd5\u52a0\u8f7d\u72b6\u6001\u5b57\u5178\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['state_dict'])\n</code></pre>"},{"location":"zh/faq/#q_12","title":"Q: \u8bad\u7ec3\u635f\u5931\u4e0d\u6536\u655b\u600e\u4e48\u529e\uff1f","text":"<p>A: \u8c03\u8bd5\u65b9\u6cd5\uff1a 1. \u68c0\u67e5\u5b66\u4e60\u7387\uff08\u53ef\u80fd\u8fc7\u5927\u6216\u8fc7\u5c0f\uff09 2. \u68c0\u67e5\u6570\u636e\u9884\u5904\u7406 3. \u589e\u52a0\u8bad\u7ec3\u6570\u636e\u91cf 4. \u8c03\u6574\u6a21\u578b\u67b6\u6784</p> <pre><code># \u5b66\u4e60\u7387\u8c03\u5ea6\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, 'min', patience=5, factor=0.5\n)\n</code></pre>"},{"location":"zh/faq/#_7","title":"\u90e8\u7f72\u548c\u751f\u4ea7","text":""},{"location":"zh/faq/#q_13","title":"Q: \u5982\u4f55\u90e8\u7f72\u5230\u751f\u4ea7\u73af\u5883\uff1f","text":"<p>A: \u90e8\u7f72\u9009\u9879\uff1a 1. REST API \u670d\u52a1\uff08FastAPI/Flask\uff09 2. Docker \u5bb9\u5668\u5316 3. Kubernetes \u96c6\u7fa4 4. \u4e91\u670d\u52a1\u5e73\u53f0</p>"},{"location":"zh/faq/#q_14","title":"Q: \u652f\u6301\u5b9e\u65f6\u63a8\u8350\u5417\uff1f","text":"<p>A: \u662f\u7684\uff0c\u6846\u67b6\u652f\u6301\uff1a - \u5728\u7ebf\u63a8\u7406 API - \u6279\u91cf\u9884\u8ba1\u7b97 - \u6d41\u5f0f\u5904\u7406\u96c6\u6210</p>"},{"location":"zh/faq/#q-ab","title":"Q: \u5982\u4f55\u8fdb\u884c A/B \u6d4b\u8bd5\uff1f","text":"<p>A: A/B \u6d4b\u8bd5\u6846\u67b6\uff1a</p> <pre><code>class ABTestFramework:\n    def get_variant(self, user_id, experiment_name):\n        # \u57fa\u4e8e\u7528\u6237ID\u7684\u4e00\u81f4\u6027\u54c8\u5e0c\u5206\u7ec4\n        hash_value = hash(f\"{user_id}_{experiment_name}\")\n        return \"A\" if hash_value % 2 == 0 else \"B\"\n\n    def recommend_with_test(self, user_id, user_history):\n        variant = self.get_variant(user_id, \"model_test\")\n        if variant == \"A\":\n            return model_a.recommend(user_history)\n        else:\n            return model_b.recommend(user_history)\n</code></pre>"},{"location":"zh/faq/#_8","title":"\u9ad8\u7ea7\u4f7f\u7528","text":""},{"location":"zh/faq/#q_15","title":"Q: \u5982\u4f55\u5b9e\u73b0\u591a\u4efb\u52a1\u5b66\u4e60\uff1f","text":"<p>A: \u6269\u5c55\u6a21\u578b\u4ee5\u652f\u6301\u591a\u4e2a\u76ee\u6807\uff1a</p> <pre><code>class MultiTaskTiger(Tiger):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.rating_head = nn.Linear(self.embedding_dim, 1)\n        self.category_head = nn.Linear(self.embedding_dim, num_categories)\n\n    def forward(self, x):\n        hidden = super().forward(x)\n\n        # \u591a\u4e2a\u8f93\u51fa\u5934\n        recommendations = self.recommendation_head(hidden)\n        ratings = self.rating_head(hidden)\n        categories = self.category_head(hidden)\n\n        return recommendations, ratings, categories\n</code></pre>"},{"location":"zh/faq/#q_16","title":"Q: \u5982\u4f55\u96c6\u6210\u5916\u90e8\u7279\u5f81\uff1f","text":"<p>A: \u7279\u5f81\u878d\u5408\u65b9\u6cd5\uff1a</p> <pre><code>class FeatureEnhancedModel(Tiger):\n    def __init__(self, user_feature_dim, item_feature_dim, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.user_feature_proj = nn.Linear(user_feature_dim, self.embedding_dim)\n        self.item_feature_proj = nn.Linear(item_feature_dim, self.embedding_dim)\n\n    def forward(self, item_seq, user_features=None, item_features=None):\n        seq_emb = super().forward(item_seq)\n\n        if user_features is not None:\n            user_emb = self.user_feature_proj(user_features)\n            seq_emb = seq_emb + user_emb.unsqueeze(1)\n\n        return seq_emb\n</code></pre>"},{"location":"zh/faq/#q_17","title":"Q: \u5982\u4f55\u5904\u7406\u5e8f\u5217\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\uff1f","text":"<p>A: \u65f6\u95f4\u611f\u77e5\u7684\u63a8\u8350\uff1a</p> <pre><code>class TimeAwareTiger(Tiger):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.time_emb = nn.Embedding(24 * 7, self.embedding_dim)  # \u5c0f\u65f6*\u5929\n\n    def forward(self, item_seq, time_seq=None):\n        seq_emb = self.item_embedding(item_seq)\n\n        if time_seq is not None:\n            time_emb = self.time_emb(time_seq)\n            seq_emb = seq_emb + time_emb\n\n        return self.transformer(seq_emb)\n</code></pre> <p>\u5982\u679c\u60a8\u6709\u5176\u4ed6\u95ee\u9898\uff0c\u8bf7\u67e5\u9605 API \u6587\u6863 \u6216\u5728 GitHub \u4e0a\u63d0\u4ea4 issue\u3002</p>"},{"location":"zh/getting-started/","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>\u672c\u6307\u5357\u5c06\u5e2e\u52a9\u60a8\u5feb\u901f\u4e0a\u624b GenerativeRecommenders \u6846\u67b6\u3002</p>"},{"location":"zh/getting-started/#_2","title":"\u524d\u7f6e\u8981\u6c42","text":"<ul> <li>Python 3.8 \u6216\u66f4\u9ad8\u7248\u672c</li> <li>CUDA 11.0+ (\u5982\u679c\u4f7f\u7528 GPU)</li> <li>8GB+ GPU \u663e\u5b58\uff08\u63a8\u8350\uff09</li> </ul>"},{"location":"zh/getting-started/#_3","title":"\u5b89\u88c5","text":""},{"location":"zh/getting-started/#1","title":"1. \u514b\u9686\u4ed3\u5e93","text":"<pre><code>git clone https://github.com/phonism/GenerativeRecommenders.git\ncd GenerativeRecommenders\n</code></pre>"},{"location":"zh/getting-started/#2","title":"2. \u5b89\u88c5\u4f9d\u8d56","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"zh/getting-started/#3","title":"3. \u51c6\u5907\u6570\u636e","text":"<p>\u4e0b\u8f7d P5 Amazon \u6570\u636e\u96c6\uff1a</p> <pre><code># \u6570\u636e\u4f1a\u81ea\u52a8\u4e0b\u8f7d\u5230 dataset/amazon \u76ee\u5f55\nmkdir -p dataset/amazon\n</code></pre>"},{"location":"zh/getting-started/#rqvae","title":"\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\uff1a\u8bad\u7ec3 RQVAE","text":""},{"location":"zh/getting-started/#1_1","title":"1. \u67e5\u770b\u914d\u7f6e\u6587\u4ef6","text":"<pre><code>cat config/rqvae/p5_amazon.gin\n</code></pre> <p>\u4e3b\u8981\u914d\u7f6e\u53c2\u6570\uff1a - <code>train.iterations=400000</code>: \u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570 - <code>train.batch_size=64</code>: \u6279\u91cf\u5927\u5c0f - <code>train.learning_rate=0.0005</code>: \u5b66\u4e60\u7387 - <code>train.dataset_folder=\"dataset/amazon\"</code>: \u6570\u636e\u96c6\u8def\u5f84</p>"},{"location":"zh/getting-started/#2_1","title":"2. \u5f00\u59cb\u8bad\u7ec3","text":"<pre><code>python generative_recommenders/trainers/rqvae_trainer.py config/rqvae/p5_amazon.gin\n</code></pre> <p>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u60a8\u4f1a\u770b\u5230\uff1a - \u81ea\u52a8\u6570\u636e\u4e0b\u8f7d\u548c\u9884\u5904\u7406 - \u6587\u672c\u7279\u5f81\u7f16\u7801\u8fdb\u5ea6 - \u8bad\u7ec3\u635f\u5931\u548c\u6307\u6807 - \u6a21\u578b\u68c0\u67e5\u70b9\u4fdd\u5b58</p>"},{"location":"zh/getting-started/#3_1","title":"3. \u76d1\u63a7\u8bad\u7ec3","text":"<p>\u5982\u679c\u542f\u7528\u4e86 Weights &amp; Biases \u65e5\u5fd7\uff1a</p> <pre><code># \u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e\ntrain.wandb_logging=True\ntrain.wandb_project=\"your_project_name\"\n</code></pre> <p>\u8bbf\u95ee wandb.ai \u67e5\u770b\u8bad\u7ec3\u8fdb\u5ea6\u3002</p>"},{"location":"zh/getting-started/#tiger","title":"\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\uff1a\u8bad\u7ec3 TIGER","text":""},{"location":"zh/getting-started/#1-rqvae","title":"1. \u786e\u4fdd RQVAE \u5df2\u8bad\u7ec3","text":"<p>TIGER \u9700\u8981\u9884\u8bad\u7ec3\u7684 RQVAE \u6a21\u578b\u6765\u751f\u6210\u8bed\u4e49 ID\uff1a</p> <pre><code># \u68c0\u67e5 RQVAE \u68c0\u67e5\u70b9\u662f\u5426\u5b58\u5728\nls out/rqvae/p5_amazon/beauty/checkpoint_*.pt\n</code></pre>"},{"location":"zh/getting-started/#2-tiger","title":"2. \u914d\u7f6e TIGER","text":"<p>\u7f16\u8f91 <code>config/tiger/p5_amazon.gin</code>\uff1a</p> <pre><code>train.pretrained_rqvae_path=\"./out/rqvae/p5_amazon/beauty/checkpoint_299999.pt\"\n</code></pre>"},{"location":"zh/getting-started/#3_2","title":"3. \u5f00\u59cb\u8bad\u7ec3","text":"<pre><code>python generative_recommenders/trainers/tiger_trainer.py config/tiger/p5_amazon.gin\n</code></pre>"},{"location":"zh/getting-started/#_4","title":"\u7406\u89e3\u6846\u67b6\u7ed3\u6784","text":""},{"location":"zh/getting-started/#_5","title":"\u6570\u636e\u5904\u7406\u6d41\u6c34\u7ebf","text":"<pre><code>graph TD\n    A[\u539f\u59cb\u6570\u636e] --&gt; B[\u6570\u636e\u4e0b\u8f7d]\n    B --&gt; C[\u9884\u5904\u7406]\n    C --&gt; D[\u6587\u672c\u7f16\u7801]\n    D --&gt; E[\u5e8f\u5217\u751f\u6210]\n    E --&gt; F[\u6570\u636e\u96c6]</code></pre>"},{"location":"zh/getting-started/#_6","title":"\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b","text":"<pre><code>graph TD\n    A[\u914d\u7f6e\u6587\u4ef6] --&gt; B[\u6570\u636e\u96c6\u52a0\u8f7d]\n    B --&gt; C[\u6a21\u578b\u521d\u59cb\u5316]\n    C --&gt; D[\u8bad\u7ec3\u5faa\u73af]\n    D --&gt; E[\u8bc4\u4f30]\n    E --&gt; F[\u68c0\u67e5\u70b9\u4fdd\u5b58]\n    F --&gt; D</code></pre>"},{"location":"zh/getting-started/#_7","title":"\u81ea\u5b9a\u4e49\u914d\u7f6e","text":""},{"location":"zh/getting-started/#_8","title":"\u521b\u5efa\u81ea\u5b9a\u4e49\u914d\u7f6e","text":"<pre><code># my_config.gin\nimport generative_recommenders.data.p5_amazon\nimport generative_recommenders.models.rqvae\n\n# \u81ea\u5b9a\u4e49\u53c2\u6570\ntrain.batch_size=32\ntrain.learning_rate=0.001\ntrain.vae_hidden_dims=[256, 128, 64]\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u8def\u5f84\ntrain.dataset_folder=\"path/to/my/data\"\n</code></pre>"},{"location":"zh/getting-started/#_9","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49\u914d\u7f6e","text":"<pre><code>python generative_recommenders/trainers/rqvae_trainer.py my_config.gin\n</code></pre>"},{"location":"zh/getting-started/#_10","title":"\u8bc4\u4f30\u6a21\u578b","text":""},{"location":"zh/getting-started/#rqvae_1","title":"RQVAE \u8bc4\u4f30","text":"<pre><code>from generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = RqVae.load_from_checkpoint(\"path/to/checkpoint.pt\")\n\n# \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e\ntest_dataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\", \n    train_test_split=\"eval\"\n)\n\n# \u8bc4\u4f30\u91cd\u6784\u8d28\u91cf\nreconstruction_loss = model.evaluate(test_dataset)\n</code></pre>"},{"location":"zh/getting-started/#tiger_1","title":"TIGER \u8bc4\u4f30","text":"<pre><code>from generative_recommenders.models.tiger import Tiger\nfrom generative_recommenders.modules.metrics import TopKAccumulator\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = Tiger.load_from_checkpoint(\"path/to/checkpoint.pt\")\n\n# \u8ba1\u7b97 Recall@K\nmetrics = TopKAccumulator(k=10)\nrecall = metrics.compute_recall(model, test_dataloader)\n</code></pre>"},{"location":"zh/getting-started/#_11","title":"\u5e38\u89c1\u95ee\u9898","text":""},{"location":"zh/getting-started/#q","title":"Q: \u5185\u5b58\u4e0d\u8db3\u600e\u4e48\u529e\uff1f","text":"<p>A: \u8c03\u6574\u4ee5\u4e0b\u53c2\u6570\uff1a <pre><code>train.batch_size=16  # \u51cf\u5c0f\u6279\u91cf\u5927\u5c0f\ntrain.vae_hidden_dims=[256, 128]  # \u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\n</code></pre></p>"},{"location":"zh/getting-started/#q_1","title":"Q: \u8bad\u7ec3\u901f\u5ea6\u6162\uff1f","text":"<p>A: \u4f18\u5316\u5efa\u8bae\uff1a - \u4f7f\u7528\u66f4\u5927\u7684\u6279\u91cf\u5927\u5c0f - \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 - \u4f7f\u7528\u591a GPU \u8bad\u7ec3</p>"},{"location":"zh/getting-started/#q_2","title":"Q: \u5982\u4f55\u6dfb\u52a0\u65b0\u6570\u636e\u96c6\uff1f","text":"<p>A: \u53c2\u8003\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u6307\u5357</p>"},{"location":"zh/getting-started/#_12","title":"\u4e0b\u4e00\u6b65","text":"<ul> <li>\u4e86\u89e3\u6a21\u578b\u67b6\u6784</li> <li>\u5b66\u4e60\u6570\u636e\u96c6\u5904\u7406</li> <li>\u67e5\u770bAPI \u6587\u6863</li> <li>\u63a2\u7d22\u9ad8\u7ea7\u793a\u4f8b</li> </ul>"},{"location":"zh/installation/","title":"\u5b89\u88c5\u6307\u5357","text":"<p>\u672c\u6307\u5357\u63d0\u4f9b\u4e86 GenerativeRecommenders \u6846\u67b6\u7684\u8be6\u7ec6\u5b89\u88c5\u8bf4\u660e\u3002</p>"},{"location":"zh/installation/#_2","title":"\u7cfb\u7edf\u8981\u6c42","text":""},{"location":"zh/installation/#_3","title":"\u786c\u4ef6\u8981\u6c42","text":"<p>\u6700\u4f4e\u914d\u7f6e\uff1a - CPU: 4 \u6838\u5fc3 - RAM: 8 GB - \u5b58\u50a8: 20 GB \u53ef\u7528\u7a7a\u95f4</p> <p>\u63a8\u8350\u914d\u7f6e\uff1a - CPU: 8+ \u6838\u5fc3 - RAM: 16+ GB - GPU: NVIDIA GPU (8GB+ VRAM) - \u5b58\u50a8: 50+ GB SSD</p>"},{"location":"zh/installation/#_4","title":"\u8f6f\u4ef6\u8981\u6c42","text":"<ul> <li>Python 3.8 - 3.11</li> <li>CUDA 11.0+ (\u5982\u679c\u4f7f\u7528 GPU)</li> <li>Git</li> </ul>"},{"location":"zh/installation/#_5","title":"\u5b89\u88c5\u65b9\u6cd5","text":""},{"location":"zh/installation/#_6","title":"\u65b9\u6cd5\u4e00\uff1a\u4ece\u6e90\u7801\u5b89\u88c5\uff08\u63a8\u8350\uff09","text":""},{"location":"zh/installation/#1","title":"1. \u514b\u9686\u4ed3\u5e93","text":"<pre><code>git clone https://github.com/phonism/GenerativeRecommenders.git\ncd GenerativeRecommenders\n</code></pre>"},{"location":"zh/installation/#2","title":"2. \u521b\u5efa\u865a\u62df\u73af\u5883","text":"<p>\u4f7f\u7528 conda: <pre><code>conda create -n genrec python=3.10\nconda activate genrec\n</code></pre></p> <p>\u4f7f\u7528 venv: <pre><code>python -m venv genrec_env\nsource genrec_env/bin/activate  # Linux/Mac\n# \u6216\ngenrec_env\\Scripts\\activate     # Windows\n</code></pre></p>"},{"location":"zh/installation/#3","title":"3. \u5b89\u88c5\u4f9d\u8d56","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"zh/installation/#_7","title":"\u65b9\u6cd5\u4e8c\uff1a\u5f00\u53d1\u73af\u5883\u5b89\u88c5","text":"<p>\u5982\u679c\u60a8\u8ba1\u5212\u4fee\u6539\u4ee3\u7801\u6216\u8d21\u732e\u4ee3\u7801\uff1a</p> <pre><code>git clone https://github.com/phonism/GenerativeRecommenders.git\ncd GenerativeRecommenders\n\n# \u521b\u5efa\u5f00\u53d1\u73af\u5883\npip install -r requirements.txt\npip install -r requirements-dev.txt  # \u5f00\u53d1\u4f9d\u8d56\n\n# \u5b89\u88c5\u9884\u63d0\u4ea4\u94a9\u5b50\npre-commit install\n</code></pre>"},{"location":"zh/installation/#_8","title":"\u4f9d\u8d56\u5305\u8bf4\u660e","text":""},{"location":"zh/installation/#_9","title":"\u6838\u5fc3\u4f9d\u8d56","text":"<pre><code># \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\ntorch==2.6.0\ntorchvision==0.21.0\ntorch_geometric==2.6.1\n\n# \u5206\u5e03\u5f0f\u8bad\u7ec3\naccelerate==0.31.0\n\n# \u914d\u7f6e\u7ba1\u7406\ngin_config==0.5.0\n\n# \u6570\u636e\u5904\u7406\npandas==1.5.3\npolars==1.9.0\nnumpy==1.24.3\n\n# \u6587\u672c\u5904\u7406\nsentence_transformers==3.3.1\n\n# \u5b9e\u9a8c\u8ddf\u8e2a\nwandb==0.19.0\n\n# \u5de5\u5177\u5e93\neinops==0.8.0\ntqdm==4.65.0\n</code></pre>"},{"location":"zh/installation/#_10","title":"\u53ef\u9009\u4f9d\u8d56","text":"<pre><code># \u63a8\u8350\u7cfb\u7edf\u4e13\u7528\u5e93\uff08\u53ef\u9009\uff09\npip install fbgemm_gpu==1.1.0\npip install torchrec==1.1.0\n\n# \u5f00\u53d1\u5de5\u5177\uff08\u53ef\u9009\uff09\npip install black isort flake8 pytest\n</code></pre>"},{"location":"zh/installation/#gpu","title":"GPU \u652f\u6301","text":""},{"location":"zh/installation/#cuda","title":"CUDA \u5b89\u88c5\u68c0\u67e5","text":"<pre><code>python -c \"import torch; print(f'CUDA Available: {torch.cuda.is_available()}')\"\npython -c \"import torch; print(f'CUDA Version: {torch.version.cuda}')\"\n</code></pre>"},{"location":"zh/installation/#cuda-pytorch","title":"\u5b89\u88c5 CUDA \u7248\u672c\u7684 PyTorch","text":"<p>\u5982\u679c\u81ea\u52a8\u5b89\u88c5\u7684\u4e0d\u662f CUDA \u7248\u672c\uff1a</p> <pre><code># CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# CUDA 12.1  \npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n</code></pre>"},{"location":"zh/installation/#_11","title":"\u9a8c\u8bc1\u5b89\u88c5","text":""},{"location":"zh/installation/#_12","title":"\u57fa\u672c\u9a8c\u8bc1","text":"<pre><code>python -c \"\nimport torch\nimport pandas as pd\nimport sentence_transformers\nprint('\u2713 \u57fa\u7840\u4f9d\u8d56\u5b89\u88c5\u6210\u529f')\n\"\n</code></pre>"},{"location":"zh/installation/#_13","title":"\u6846\u67b6\u9a8c\u8bc1","text":"<pre><code>python -c \"\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\nfrom generative_recommenders.models.rqvae import RqVae\nprint('\u2713 GenerativeRecommenders \u5b89\u88c5\u6210\u529f')\n\"\n</code></pre>"},{"location":"zh/installation/#gpu_1","title":"GPU \u9a8c\u8bc1","text":"<pre><code>python -c \"\nimport torch\nprint(f'GPU \u6570\u91cf: {torch.cuda.device_count()}')\nif torch.cuda.is_available():\n    print(f'GPU \u578b\u53f7: {torch.cuda.get_device_name(0)}')\n    print('\u2713 GPU \u652f\u6301\u53ef\u7528')\nelse:\n    print('\u26a0 GPU \u4e0d\u53ef\u7528\uff0c\u5c06\u4f7f\u7528 CPU')\n\"\n</code></pre>"},{"location":"zh/installation/#_14","title":"\u5e38\u89c1\u95ee\u9898","text":""},{"location":"zh/installation/#q-importerror-no-module-named-torch","title":"Q: ImportError: No module named 'torch'","text":"<p>\u89e3\u51b3\u65b9\u6848: <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n# \u6216\u8005\u5b89\u88c5 CUDA \u7248\u672c\uff08\u89c1\u4e0a\u6587\uff09\n</code></pre></p>"},{"location":"zh/installation/#q-cuda-out-of-memory","title":"Q: CUDA out of memory","text":"<p>\u89e3\u51b3\u65b9\u6848: - \u51cf\u5c0f\u6279\u91cf\u5927\u5c0f: <code>train.batch_size=16</code> - \u542f\u7528\u68af\u5ea6\u7d2f\u79ef: <code>train.gradient_accumulate_every=4</code> - \u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6: <code>train.mixed_precision_type=\"fp16\"</code></p>"},{"location":"zh/installation/#q-sentence-transformers","title":"Q: sentence-transformers \u4e0b\u8f7d\u6162","text":"<p>\u89e3\u51b3\u65b9\u6848: <pre><code># \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u4f7f\u7528\u955c\u50cf\nexport HF_ENDPOINT=https://hf-mirror.com\n\n# \u6216\u8005\u9884\u4e0b\u8f7d\u6a21\u578b\npython -c \"\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('sentence-transformers/sentence-t5-xl')\n\"\n</code></pre></p>"},{"location":"zh/installation/#q","title":"Q: \u6570\u636e\u96c6\u4e0b\u8f7d\u5931\u8d25","text":"<p>\u89e3\u51b3\u65b9\u6848: <pre><code># \u624b\u52a8\u8bbe\u7f6e\u4ee3\u7406\nexport HTTP_PROXY=http://your-proxy:port\nexport HTTPS_PROXY=http://your-proxy:port\n\n# \u6216\u8005\u624b\u52a8\u4e0b\u8f7d\u6570\u636e\u96c6\u5230 dataset/ \u76ee\u5f55\n</code></pre></p>"},{"location":"zh/installation/#q-windows","title":"Q: Windows \u4e0b\u8def\u5f84\u95ee\u9898","text":"<p>\u89e3\u51b3\u65b9\u6848: <pre><code># \u4f7f\u7528\u6b63\u659c\u6760\u6216\u539f\u59cb\u5b57\u7b26\u4e32\ntrain.dataset_folder=\"dataset/amazon\"\n# \u6216\ntrain.dataset_folder=r\"dataset\\amazon\"\n</code></pre></p>"},{"location":"zh/installation/#_15","title":"\u6027\u80fd\u4f18\u5316","text":""},{"location":"zh/installation/#_16","title":"\u7cfb\u7edf\u7ea7\u4f18\u5316","text":"<pre><code># Linux: \u589e\u52a0\u5171\u4eab\u5185\u5b58\necho 'vm.overcommit_memory=1' &gt;&gt; /etc/sysctl.conf\n\n# \u8bbe\u7f6e PyTorch \u7ebf\u7a0b\u6570\nexport OMP_NUM_THREADS=4\nexport MKL_NUM_THREADS=4\n</code></pre>"},{"location":"zh/installation/#_17","title":"\u5185\u5b58\u4f18\u5316","text":"<pre><code># \u5728\u8bad\u7ec3\u524d\u8bbe\u7f6e\nimport torch\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n</code></pre>"},{"location":"zh/installation/#docker","title":"Docker \u5b89\u88c5\uff08\u53ef\u9009\uff09","text":""},{"location":"zh/installation/#_18","title":"\u4f7f\u7528\u9884\u6784\u5efa\u955c\u50cf","text":"<pre><code>docker pull pytorch/pytorch:2.6.0-cuda12.1-cudnn9-devel\n\ndocker run -it --gpus all -v $(pwd):/workspace pytorch/pytorch:2.6.0-cuda12.1-cudnn9-devel\ncd /workspace\npip install -r requirements.txt\n</code></pre>"},{"location":"zh/installation/#dockerfile","title":"\u81ea\u5b9a\u4e49 Dockerfile","text":"<pre><code>FROM pytorch/pytorch:2.6.0-cuda12.1-cudnn9-devel\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\nCMD [\"python\", \"-c\", \"print('GenerativeRecommenders ready!')\"]\n</code></pre>"},{"location":"zh/installation/#_19","title":"\u4e0b\u4e00\u6b65","text":"<p>\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\uff1a</p> <ol> <li>\u9605\u8bfb\u5feb\u901f\u5f00\u59cb\u6307\u5357</li> <li>\u4e86\u89e3\u6570\u636e\u96c6\u51c6\u5907 </li> <li>\u5f00\u59cb\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u5b9e\u9a8c</li> <li>\u67e5\u770bAPI \u6587\u6863</li> </ol> <p>\u5982\u679c\u9047\u5230\u5176\u4ed6\u95ee\u9898\uff0c\u8bf7\u67e5\u770b\u6211\u4eec\u7684 FAQ \u6216\u5728 GitHub \u4e0a\u63d0\u4ea4 Issue\u3002</p>"},{"location":"zh/api/","title":"API \u53c2\u8003","text":"<p>\u672c\u8282\u63d0\u4f9b GenerativeRecommenders \u6846\u67b6\u7684\u8be6\u7ec6 API \u6587\u6863\u3002</p>"},{"location":"zh/api/#_1","title":"\u6838\u5fc3\u6a21\u5757","text":""},{"location":"zh/api/#_2","title":"\u6a21\u578b","text":"<ul> <li>RQVAE: \u6b8b\u5dee\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668</li> <li>TIGER: \u57fa\u4e8e Transformer \u7684\u751f\u6210\u5f0f\u68c0\u7d22\u6a21\u578b</li> </ul>"},{"location":"zh/api/#_3","title":"\u6570\u636e\u5904\u7406","text":"<ul> <li>\u57fa\u7840\u6570\u636e\u96c6: \u6570\u636e\u96c6\u62bd\u8c61\u57fa\u7c7b</li> <li>\u914d\u7f6e\u7ba1\u7406: \u914d\u7f6e\u7ba1\u7406\u7c7b</li> <li>\u5904\u7406\u5668: \u6587\u672c\u548c\u5e8f\u5217\u5904\u7406\u5de5\u5177</li> <li>\u6570\u636e\u96c6\u5de5\u5382: \u6570\u636e\u96c6\u521b\u5efa\u5de5\u5382\u6a21\u5f0f</li> </ul>"},{"location":"zh/api/#_4","title":"\u8bad\u7ec3","text":"<ul> <li>\u8bad\u7ec3\u5668: \u8bad\u7ec3\u5de5\u5177\u548c\u811a\u672c</li> <li>\u6a21\u5757: \u6838\u5fc3\u6784\u5efa\u5757\uff08\u7f16\u7801\u5668\u3001\u635f\u5931\u51fd\u6570\u3001\u6307\u6807\uff09</li> </ul>"},{"location":"zh/api/#_5","title":"\u5feb\u901f\u5bfc\u822a","text":""},{"location":"zh/api/#_6","title":"\u6838\u5fc3\u7ec4\u4ef6","text":"<p>\u6a21\u578b: - RQVAE \u6a21\u578b\u7c7b - \u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668 - TIGER \u6a21\u578b\u7c7b - \u57fa\u4e8e Transformer \u7684\u751f\u6210\u5f0f\u68c0\u7d22</p> <p>\u6570\u636e\u5904\u7406: - \u6570\u636e\u96c6\u7c7b - \u6570\u636e\u52a0\u8f7d\u548c\u9884\u5904\u7406 - \u914d\u7f6e\u7cfb\u7edf - \u53c2\u6570\u7ba1\u7406 - \u5904\u7406\u5668 - \u6587\u672c\u548c\u5e8f\u5217\u5904\u7406\u5de5\u5177</p> <p>\u8bad\u7ec3: - RQVAE \u8bad\u7ec3 - \u8bad\u7ec3\u6d41\u7a0b\u548c\u914d\u7f6e - TIGER \u8bad\u7ec3 - \u9ad8\u7ea7\u8bad\u7ec3\u5de5\u4f5c\u6d41</p>"},{"location":"zh/api/#_7","title":"\u4ee3\u7801\u793a\u4f8b","text":"<p>\u67e5\u770b\u793a\u4f8b\u9875\u9762\u4e86\u89e3\u5b9e\u7528\u7684\u4f7f\u7528\u6a21\u5f0f\u548c\u4ee3\u7801\u7247\u6bb5\u3002</p>"},{"location":"zh/api/base-dataset/","title":"\u57fa\u7840\u6570\u636e\u96c6 API \u53c2\u8003","text":"<p>\u6570\u636e\u96c6\u62bd\u8c61\u57fa\u7c7b\u548c\u901a\u7528\u6570\u636e\u5904\u7406\u63a5\u53e3\u7684\u8be6\u7ec6\u6587\u6863\u3002</p>"},{"location":"zh/api/base-dataset/#_1","title":"\u62bd\u8c61\u57fa\u7c7b","text":""},{"location":"zh/api/base-dataset/#baserecommenderdataset","title":"BaseRecommenderDataset","text":"<p>\u6240\u6709\u63a8\u8350\u6570\u636e\u96c6\u7684\u62bd\u8c61\u57fa\u7c7b\u3002</p> <pre><code>class BaseRecommenderDataset(ABC):\n    def __init__(self, config: DatasetConfig):\n        self.config = config\n        self.root_path = Path(config.root_dir)\n        self.text_processor = TextProcessor(config.text_config)\n</code></pre> <p>\u53c2\u6570: - <code>config</code>: \u6570\u636e\u96c6\u914d\u7f6e\u5bf9\u8c61</p> <p>\u62bd\u8c61\u65b9\u6cd5:</p>"},{"location":"zh/api/base-dataset/#download","title":"download()","text":"<p>\u4e0b\u8f7d\u6570\u636e\u96c6\u5230\u672c\u5730\u3002</p> <pre><code>@abstractmethod\ndef download(self) -&gt; None:\n    \"\"\"\u4e0b\u8f7d\u6570\u636e\u96c6\u5230\u672c\u5730\u5b58\u50a8\"\"\"\n    pass\n</code></pre>"},{"location":"zh/api/base-dataset/#load_raw_data","title":"load_raw_data()","text":"<p>\u52a0\u8f7d\u539f\u59cb\u6570\u636e\u6587\u4ef6\u3002</p> <pre><code>@abstractmethod\ndef load_raw_data(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    \u52a0\u8f7d\u539f\u59cb\u6570\u636e\u6587\u4ef6\n\n    Returns:\n        \u5305\u542b\u539f\u59cb\u6570\u636e\u7684\u5b57\u5178\uff0c\u901a\u5e38\u5305\u542b 'items' \u548c 'interactions' \u952e\n    \"\"\"\n    pass\n</code></pre>"},{"location":"zh/api/base-dataset/#preprocess_dataraw_data","title":"preprocess_data(raw_data)","text":"<p>\u9884\u5904\u7406\u539f\u59cb\u6570\u636e\u3002</p> <pre><code>@abstractmethod\ndef preprocess_data(self, raw_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    \u9884\u5904\u7406\u539f\u59cb\u6570\u636e\n\n    Args:\n        raw_data: \u539f\u59cb\u6570\u636e\u5b57\u5178\n\n    Returns:\n        \u9884\u5904\u7406\u540e\u7684\u6570\u636e\u5b57\u5178\n    \"\"\"\n    pass\n</code></pre>"},{"location":"zh/api/base-dataset/#extract_itemsprocessed_data","title":"extract_items(processed_data)","text":"<p>\u63d0\u53d6\u7269\u54c1\u4fe1\u606f\u3002</p> <pre><code>@abstractmethod\ndef extract_items(self, processed_data: Dict[str, Any]) -&gt; pd.DataFrame:\n    \"\"\"\n    \u63d0\u53d6\u7269\u54c1\u4fe1\u606f\n\n    Args:\n        processed_data: \u9884\u5904\u7406\u540e\u7684\u6570\u636e\n\n    Returns:\n        \u7269\u54c1\u4fe1\u606f DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"zh/api/base-dataset/#extract_interactionsprocessed_data","title":"extract_interactions(processed_data)","text":"<p>\u63d0\u53d6\u7528\u6237\u4ea4\u4e92\u4fe1\u606f\u3002</p> <pre><code>@abstractmethod\ndef extract_interactions(self, processed_data: Dict[str, Any]) -&gt; pd.DataFrame:\n    \"\"\"\n    \u63d0\u53d6\u7528\u6237\u4ea4\u4e92\u4fe1\u606f\n\n    Args:\n        processed_data: \u9884\u5904\u7406\u540e\u7684\u6570\u636e\n\n    Returns:\n        \u4ea4\u4e92\u4fe1\u606f DataFrame\n    \"\"\"\n    pass\n</code></pre> <p>\u516c\u5171\u65b9\u6cd5:</p>"},{"location":"zh/api/base-dataset/#get_dataset","title":"get_dataset()","text":"<p>\u83b7\u53d6\u5b8c\u6574\u7684\u6570\u636e\u96c6\u3002</p> <pre><code>def get_dataset(self) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    \u83b7\u53d6\u5b8c\u6574\u7684\u6570\u636e\u96c6\n\n    Returns:\n        (items_df, interactions_df): \u7269\u54c1\u548c\u4ea4\u4e92\u6570\u636e\u6846\n    \"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#filter_low_interactionsinteractions_df-min_user_interactions-min_item_interactions","title":"filter_low_interactions(interactions_df, min_user_interactions, min_item_interactions)","text":"<p>\u8fc7\u6ee4\u4f4e\u9891\u7528\u6237\u548c\u7269\u54c1\u3002</p> <pre><code>def filter_low_interactions(\n    self,\n    interactions_df: pd.DataFrame,\n    min_user_interactions: int = 5,\n    min_item_interactions: int = 5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    \u8fc7\u6ee4\u4f4e\u9891\u7528\u6237\u548c\u7269\u54c1\n\n    Args:\n        interactions_df: \u4ea4\u4e92\u6570\u636e\u6846\n        min_user_interactions: \u6700\u5c11\u7528\u6237\u4ea4\u4e92\u6570\n        min_item_interactions: \u6700\u5c11\u7269\u54c1\u4ea4\u4e92\u6570\n\n    Returns:\n        \u8fc7\u6ee4\u540e\u7684\u4ea4\u4e92\u6570\u636e\u6846\n    \"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#_2","title":"\u6570\u636e\u96c6\u5305\u88c5\u5668","text":""},{"location":"zh/api/base-dataset/#itemdataset","title":"ItemDataset","text":"<p>\u7528\u4e8e\u7269\u54c1\u7ea7\u6570\u636e\u7684\u6570\u636e\u96c6\u5305\u88c5\u5668\uff0c\u4e3b\u8981\u7528\u4e8e\u8bad\u7ec3 RQVAE\u3002</p> <pre><code>class ItemDataset(Dataset):\n    def __init__(\n        self,\n        base_dataset: BaseRecommenderDataset,\n        split: str = \"all\",\n        return_text: bool = False\n    ):\n</code></pre> <p>\u53c2\u6570: - <code>base_dataset</code>: \u57fa\u7840\u6570\u636e\u96c6\u5b9e\u4f8b - <code>split</code>: \u6570\u636e\u5206\u5272 (\"all\", \"train\", \"val\", \"test\") - <code>return_text</code>: \u662f\u5426\u8fd4\u56de\u6587\u672c\u4fe1\u606f</p> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/base-dataset/#len","title":"len()","text":"<p>\u8fd4\u56de\u6570\u636e\u96c6\u5927\u5c0f\u3002</p> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\u8fd4\u56de\u6570\u636e\u96c6\u4e2d\u7269\u54c1\u6570\u91cf\"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#getitemidx","title":"getitem(idx)","text":"<p>\u83b7\u53d6\u5355\u4e2a\u6570\u636e\u6837\u672c\u3002</p> <pre><code>def __getitem__(self, idx: int) -&gt; Union[torch.Tensor, Dict[str, Any]]:\n    \"\"\"\n    \u83b7\u53d6\u5355\u4e2a\u7269\u54c1\u6570\u636e\n\n    Args:\n        idx: \u7269\u54c1\u7d22\u5f15\n\n    Returns:\n        \u5982\u679c return_text=False: \u7269\u54c1\u7279\u5f81\u5411\u91cf (torch.Tensor)\n        \u5982\u679c return_text=True: \u5305\u542b\u7279\u5f81\u548c\u6587\u672c\u7684\u5b57\u5178\n    \"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#get_item_featuresitem_id","title":"get_item_features(item_id)","text":"<p>\u6839\u636e\u7269\u54c1 ID \u83b7\u53d6\u7279\u5f81\u3002</p> <pre><code>def get_item_features(self, item_id: int) -&gt; torch.Tensor:\n    \"\"\"\n    \u6839\u636e\u7269\u54c1 ID \u83b7\u53d6\u7279\u5f81\n\n    Args:\n        item_id: \u7269\u54c1 ID\n\n    Returns:\n        \u7269\u54c1\u7279\u5f81\u5411\u91cf\n    \"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#sequencedataset","title":"SequenceDataset","text":"<p>\u7528\u4e8e\u5e8f\u5217\u7ea7\u6570\u636e\u7684\u6570\u636e\u96c6\u5305\u88c5\u5668\uff0c\u4e3b\u8981\u7528\u4e8e\u8bad\u7ec3 TIGER\u3002</p> <pre><code>class SequenceDataset(Dataset):\n    def __init__(\n        self,\n        base_dataset: BaseRecommenderDataset,\n        split: str = \"train\",\n        semantic_encoder: Optional[torch.nn.Module] = None,\n        sequence_config: Optional[SequenceConfig] = None\n    ):\n</code></pre> <p>\u53c2\u6570: - <code>base_dataset</code>: \u57fa\u7840\u6570\u636e\u96c6\u5b9e\u4f8b - <code>split</code>: \u6570\u636e\u5206\u5272 (\"train\", \"val\", \"test\") - <code>semantic_encoder</code>: \u8bed\u4e49\u7f16\u7801\u5668 (RQVAE) - <code>sequence_config</code>: \u5e8f\u5217\u914d\u7f6e</p> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/base-dataset/#len_1","title":"len()","text":"<p>\u8fd4\u56de\u6570\u636e\u96c6\u5927\u5c0f\u3002</p> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\u8fd4\u56de\u6570\u636e\u96c6\u4e2d\u7528\u6237\u5e8f\u5217\u6570\u91cf\"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#getitemidx_1","title":"getitem(idx)","text":"<p>\u83b7\u53d6\u5355\u4e2a\u5e8f\u5217\u6570\u636e\u3002</p> <pre><code>def __getitem__(self, idx: int) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    \u83b7\u53d6\u5355\u4e2a\u7528\u6237\u5e8f\u5217\u6570\u636e\n\n    Args:\n        idx: \u5e8f\u5217\u7d22\u5f15\n\n    Returns:\n        \u5305\u542b\u8f93\u5165\u5e8f\u5217\u548c\u76ee\u6807\u5e8f\u5217\u7684\u5b57\u5178:\n        - 'input_ids': \u8f93\u5165\u5e8f\u5217 (torch.Tensor)\n        - 'labels': \u76ee\u6807\u5e8f\u5217 (torch.Tensor)  \n        - 'attention_mask': \u6ce8\u610f\u529b\u63a9\u7801 (torch.Tensor)\n    \"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#build_sequences","title":"build_sequences()","text":"<p>\u6784\u5efa\u7528\u6237\u4ea4\u4e92\u5e8f\u5217\u3002</p> <pre><code>def build_sequences(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    \u6784\u5efa\u7528\u6237\u4ea4\u4e92\u5e8f\u5217\n\n    Returns:\n        \u5e8f\u5217\u5217\u8868\uff0c\u6bcf\u4e2a\u5e8f\u5217\u5305\u542b\u7528\u6237 ID \u548c\u7269\u54c1 ID \u5217\u8868\n    \"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#encode_sequences_to_semantic_idssequences","title":"encode_sequences_to_semantic_ids(sequences)","text":"<p>\u5c06\u5e8f\u5217\u7f16\u7801\u4e3a\u8bed\u4e49 ID\u3002</p> <pre><code>def encode_sequences_to_semantic_ids(\n    self, \n    sequences: List[Dict[str, Any]]\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    \u5c06\u7269\u54c1\u5e8f\u5217\u7f16\u7801\u4e3a\u8bed\u4e49 ID \u5e8f\u5217\n\n    Args:\n        sequences: \u539f\u59cb\u5e8f\u5217\u5217\u8868\n\n    Returns:\n        \u7f16\u7801\u540e\u7684\u5e8f\u5217\u5217\u8868\n    \"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#_3","title":"\u6570\u636e\u5904\u7406\u5de5\u5177","text":""},{"location":"zh/api/base-dataset/#train_test_splitinteractions_df-test_ratio-val_ratio","title":"train_test_split(interactions_df, test_ratio, val_ratio)","text":"<p>\u5206\u5272\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u3002</p> <pre><code>def train_test_split(\n    interactions_df: pd.DataFrame,\n    test_ratio: float = 0.2,\n    val_ratio: float = 0.1,\n    time_based: bool = True\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    \u5206\u5272\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\n\n    Args:\n        interactions_df: \u4ea4\u4e92\u6570\u636e\u6846\n        test_ratio: \u6d4b\u8bd5\u96c6\u6bd4\u4f8b\n        val_ratio: \u9a8c\u8bc1\u96c6\u6bd4\u4f8b\n        time_based: \u662f\u5426\u57fa\u4e8e\u65f6\u95f4\u5206\u5272\n\n    Returns:\n        (train_df, val_df, test_df): \u5206\u5272\u540e\u7684\u6570\u636e\u6846\n    \"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#create_item_mappingitems_df","title":"create_item_mapping(items_df)","text":"<p>\u521b\u5efa\u7269\u54c1 ID \u6620\u5c04\u3002</p> <pre><code>def create_item_mapping(items_df: pd.DataFrame) -&gt; Tuple[Dict[int, int], Dict[int, int]]:\n    \"\"\"\n    \u521b\u5efa\u7269\u54c1 ID \u6620\u5c04\n\n    Args:\n        items_df: \u7269\u54c1\u6570\u636e\u6846\n\n    Returns:\n        (id_to_index, index_to_id): ID \u6620\u5c04\u5b57\u5178\n    \"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#normalize_featuresfeatures","title":"normalize_features(features)","text":"<p>\u6807\u51c6\u5316\u7279\u5f81\u5411\u91cf\u3002</p> <pre><code>def normalize_features(features: np.ndarray, method: str = \"l2\") -&gt; np.ndarray:\n    \"\"\"\n    \u6807\u51c6\u5316\u7279\u5f81\u5411\u91cf\n\n    Args:\n        features: \u7279\u5f81\u77e9\u9635\n        method: \u6807\u51c6\u5316\u65b9\u6cd5 (\"l2\", \"minmax\", \"zscore\")\n\n    Returns:\n        \u6807\u51c6\u5316\u540e\u7684\u7279\u5f81\u77e9\u9635\n    \"\"\"\n</code></pre>"},{"location":"zh/api/base-dataset/#_4","title":"\u7f13\u5b58\u673a\u5236","text":""},{"location":"zh/api/base-dataset/#cachemanager","title":"CacheManager","text":"<p>\u7ba1\u7406\u6570\u636e\u5904\u7406\u7f13\u5b58\u3002</p> <pre><code>class CacheManager:\n    def __init__(self, cache_dir: str):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n    def get_cache_path(self, cache_key: str) -&gt; Path:\n        \"\"\"\u83b7\u53d6\u7f13\u5b58\u6587\u4ef6\u8def\u5f84\"\"\"\n        return self.cache_dir / f\"{cache_key}.pkl\"\n\n    def exists(self, cache_key: str) -&gt; bool:\n        \"\"\"\u68c0\u67e5\u7f13\u5b58\u662f\u5426\u5b58\u5728\"\"\"\n        return self.get_cache_path(cache_key).exists()\n\n    def save(self, cache_key: str, data: Any) -&gt; None:\n        \"\"\"\u4fdd\u5b58\u6570\u636e\u5230\u7f13\u5b58\"\"\"\n        cache_path = self.get_cache_path(cache_key)\n        with open(cache_path, 'wb') as f:\n            pickle.dump(data, f)\n\n    def load(self, cache_key: str) -&gt; Any:\n        \"\"\"\u4ece\u7f13\u5b58\u52a0\u8f7d\u6570\u636e\"\"\"\n        cache_path = self.get_cache_path(cache_key)\n        with open(cache_path, 'rb') as f:\n            return pickle.load(f)\n</code></pre>"},{"location":"zh/api/base-dataset/#_5","title":"\u6570\u636e\u9a8c\u8bc1","text":""},{"location":"zh/api/base-dataset/#validate_datasetdataset","title":"validate_dataset(dataset)","text":"<p>\u9a8c\u8bc1\u6570\u636e\u96c6\u5b8c\u6574\u6027\u3002</p> <pre><code>def validate_dataset(dataset: Dataset) -&gt; Dict[str, Any]:\n    \"\"\"\n    \u9a8c\u8bc1\u6570\u636e\u96c6\u5b8c\u6574\u6027\n\n    Args:\n        dataset: \u6570\u636e\u96c6\u5b9e\u4f8b\n\n    Returns:\n        \u9a8c\u8bc1\u7ed3\u679c\u5b57\u5178\n    \"\"\"\n    results = {\n        'size': len(dataset),\n        'sample_shapes': [],\n        'data_types': [],\n        'errors': []\n    }\n\n    try:\n        # \u68c0\u67e5\u6570\u636e\u96c6\u5927\u5c0f\n        if len(dataset) == 0:\n            results['errors'].append(\"Dataset is empty\")\n\n        # \u68c0\u67e5\u6837\u672c\u5f62\u72b6\u548c\u7c7b\u578b\n        for i in range(min(5, len(dataset))):\n            sample = dataset[i]\n            if isinstance(sample, torch.Tensor):\n                results['sample_shapes'].append(sample.shape)\n                results['data_types'].append(sample.dtype)\n            elif isinstance(sample, dict):\n                for key, value in sample.items():\n                    if isinstance(value, torch.Tensor):\n                        results['sample_shapes'].append((key, value.shape))\n                        results['data_types'].append((key, value.dtype))\n\n    except Exception as e:\n        results['errors'].append(f\"Validation error: {str(e)}\")\n\n    return results\n</code></pre>"},{"location":"zh/api/base-dataset/#_6","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/api/base-dataset/#_7","title":"\u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6","text":"<pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\nfrom generative_recommenders.data.configs import DatasetConfig\n\nclass MyDataset(BaseRecommenderDataset):\n    def download(self):\n        # \u5b9e\u73b0\u4e0b\u8f7d\u903b\u8f91\n        pass\n\n    def load_raw_data(self):\n        # \u52a0\u8f7d\u6570\u636e\n        return {\"items\": items_df, \"interactions\": interactions_df}\n\n    def preprocess_data(self, raw_data):\n        # \u9884\u5904\u7406\n        return raw_data\n\n    def extract_items(self, processed_data):\n        return processed_data[\"items\"]\n\n    def extract_interactions(self, processed_data):\n        return processed_data[\"interactions\"]\n\n# \u4f7f\u7528\u6570\u636e\u96c6\nconfig = DatasetConfig(root_dir=\"data\", split=\"default\")\ndataset = MyDataset(config)\nitems_df, interactions_df = dataset.get_dataset()\n</code></pre>"},{"location":"zh/api/base-dataset/#_8","title":"\u521b\u5efa\u7269\u54c1\u6570\u636e\u96c6","text":"<pre><code>from generative_recommenders.data.base_dataset import ItemDataset\n\n# \u521b\u5efa\u7269\u54c1\u6570\u636e\u96c6\nitem_dataset = ItemDataset(\n    base_dataset=dataset,\n    split=\"train\",\n    return_text=False\n)\n\n# \u4f7f\u7528 DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(item_dataset, batch_size=32, shuffle=True)\n\nfor batch in dataloader:\n    features = batch  # (batch_size, feature_dim)\n    # \u8bad\u7ec3\u903b\u8f91\n</code></pre>"},{"location":"zh/api/base-dataset/#_9","title":"\u521b\u5efa\u5e8f\u5217\u6570\u636e\u96c6","text":"<pre><code>from generative_recommenders.data.base_dataset import SequenceDataset\nfrom generative_recommenders.models.rqvae import RqVae\n\n# \u52a0\u8f7d\u8bed\u4e49\u7f16\u7801\u5668\nsemantic_encoder = RqVae.load_from_checkpoint(\"rqvae.ckpt\")\n\n# \u521b\u5efa\u5e8f\u5217\u6570\u636e\u96c6\nsequence_dataset = SequenceDataset(\n    base_dataset=dataset,\n    split=\"train\",\n    semantic_encoder=semantic_encoder\n)\n\n# \u4f7f\u7528 DataLoader\ndataloader = DataLoader(sequence_dataset, batch_size=16, shuffle=True)\n\nfor batch in dataloader:\n    input_ids = batch['input_ids']  # (batch_size, seq_len)\n    labels = batch['labels']        # (batch_size, seq_len)\n    # \u8bad\u7ec3\u903b\u8f91\n</code></pre>"},{"location":"zh/api/configs/","title":"\u914d\u7f6e\u7ba1\u7406 API \u53c2\u8003","text":"<p>\u914d\u7f6e\u7ba1\u7406\u7c7b\u7684\u8be6\u7ec6\u6587\u6863\uff0c\u7528\u4e8e\u7ba1\u7406\u6570\u636e\u5904\u7406\u548c\u6a21\u578b\u8bad\u7ec3\u53c2\u6570\u3002</p>"},{"location":"zh/api/configs/#_1","title":"\u57fa\u7840\u914d\u7f6e\u7c7b","text":""},{"location":"zh/api/configs/#datasetconfig","title":"DatasetConfig","text":"<p>\u6570\u636e\u96c6\u57fa\u7840\u914d\u7f6e\u7c7b\u3002</p> <pre><code>@dataclass\nclass DatasetConfig:\n    root_dir: str\n    split: str = \"default\"\n    force_reload: bool = False\n    text_config: Optional[TextEncodingConfig] = None\n    sequence_config: Optional[SequenceConfig] = None\n    processing_config: Optional[DataProcessingConfig] = None\n\n    def __post_init__(self):\n        \"\"\"\u521d\u59cb\u5316\u540e\u5904\u7406\"\"\"\n        if self.text_config is None:\n            self.text_config = TextEncodingConfig()\n        if self.sequence_config is None:\n            self.sequence_config = SequenceConfig()\n        if self.processing_config is None:\n            self.processing_config = DataProcessingConfig()\n</code></pre> <p>\u53c2\u6570: - <code>root_dir</code>: \u6570\u636e\u96c6\u6839\u76ee\u5f55 - <code>split</code>: \u6570\u636e\u5206\u5272\u6807\u8bc6 - <code>force_reload</code>: \u662f\u5426\u5f3a\u5236\u91cd\u65b0\u52a0\u8f7d - <code>text_config</code>: \u6587\u672c\u7f16\u7801\u914d\u7f6e - <code>sequence_config</code>: \u5e8f\u5217\u5904\u7406\u914d\u7f6e - <code>processing_config</code>: \u6570\u636e\u5904\u7406\u914d\u7f6e</p>"},{"location":"zh/api/configs/#_2","title":"\u6587\u672c\u7f16\u7801\u914d\u7f6e","text":""},{"location":"zh/api/configs/#textencodingconfig","title":"TextEncodingConfig","text":"<p>\u6587\u672c\u7f16\u7801\u76f8\u5173\u914d\u7f6e\u3002</p> <pre><code>@dataclass\nclass TextEncodingConfig:\n    encoder_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    template: str = \"Title: {title}; Brand: {brand}; Category: {category}; Price: {price}\"\n    batch_size: int = 32\n    max_length: int = 512\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    cache_dir: str = \"cache/text_embeddings\"\n    normalize_embeddings: bool = True\n\n    def __post_init__(self):\n        \"\"\"\u9a8c\u8bc1\u914d\u7f6e\u53c2\u6570\"\"\"\n        if self.batch_size &lt;= 0:\n            raise ValueError(\"batch_size must be positive\")\n        if self.max_length &lt;= 0:\n            raise ValueError(\"max_length must be positive\")\n</code></pre> <p>\u53c2\u6570: - <code>encoder_model</code>: \u6587\u672c\u7f16\u7801\u5668\u6a21\u578b\u540d\u79f0 - <code>template</code>: \u6587\u672c\u6a21\u677f\u683c\u5f0f - <code>batch_size</code>: \u6279\u5904\u7406\u5927\u5c0f - <code>max_length</code>: \u6700\u5927\u6587\u672c\u957f\u5ea6 - <code>device</code>: \u8ba1\u7b97\u8bbe\u5907 - <code>cache_dir</code>: \u7f13\u5b58\u76ee\u5f55 - <code>normalize_embeddings</code>: \u662f\u5426\u6807\u51c6\u5316\u5d4c\u5165</p> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/configs/#get_cache_keysplit-model_name","title":"get_cache_key(split, model_name)","text":"<p>\u751f\u6210\u7f13\u5b58\u952e\u3002</p> <pre><code>def get_cache_key(self, split: str, model_name: str = None) -&gt; str:\n    \"\"\"\n    \u751f\u6210\u7f13\u5b58\u952e\n\n    Args:\n        split: \u6570\u636e\u5206\u5272\n        model_name: \u6a21\u578b\u540d\u79f0\n\n    Returns:\n        \u7f13\u5b58\u952e\u5b57\u7b26\u4e32\n    \"\"\"\n    if model_name is None:\n        model_name = self.encoder_model\n    return f\"{model_name}_{split}_{hash(self.template)}\"\n</code></pre>"},{"location":"zh/api/configs/#format_textitem_data","title":"format_text(item_data)","text":"<p>\u683c\u5f0f\u5316\u7269\u54c1\u6587\u672c\u3002</p> <pre><code>def format_text(self, item_data: Dict[str, Any]) -&gt; str:\n    \"\"\"\n    \u4f7f\u7528\u6a21\u677f\u683c\u5f0f\u5316\u7269\u54c1\u6587\u672c\n\n    Args:\n        item_data: \u7269\u54c1\u6570\u636e\u5b57\u5178\n\n    Returns:\n        \u683c\u5f0f\u5316\u540e\u7684\u6587\u672c\n    \"\"\"\n    try:\n        return self.template.format(**item_data)\n    except KeyError as e:\n        # \u5904\u7406\u7f3a\u5931\u5b57\u6bb5\n        available_fields = set(item_data.keys())\n        template_fields = set(re.findall(r'\\{(\\w+)\\}', self.template))\n        missing_fields = template_fields - available_fields\n\n        # \u7528\u9ed8\u8ba4\u503c\u586b\u5145\u7f3a\u5931\u5b57\u6bb5\n        filled_data = item_data.copy()\n        for field in missing_fields:\n            filled_data[field] = \"Unknown\"\n\n        return self.template.format(**filled_data)\n</code></pre>"},{"location":"zh/api/configs/#_3","title":"\u5e8f\u5217\u5904\u7406\u914d\u7f6e","text":""},{"location":"zh/api/configs/#sequenceconfig","title":"SequenceConfig","text":"<p>\u5e8f\u5217\u5904\u7406\u76f8\u5173\u914d\u7f6e\u3002</p> <pre><code>@dataclass\nclass SequenceConfig:\n    max_seq_length: int = 50\n    min_seq_length: int = 3\n    padding_token: int = 0\n    truncate_strategy: str = \"recent\"  # \"recent\", \"random\", \"oldest\"\n    sequence_stride: int = 1\n    target_offset: int = 1\n    include_timestamps: bool = False\n    time_encoding_dim: int = 32\n\n    def __post_init__(self):\n        \"\"\"\u9a8c\u8bc1\u914d\u7f6e\u53c2\u6570\"\"\"\n        if self.max_seq_length &lt;= self.min_seq_length:\n            raise ValueError(\"max_seq_length must be greater than min_seq_length\")\n        if self.truncate_strategy not in [\"recent\", \"random\", \"oldest\"]:\n            raise ValueError(\"Invalid truncate_strategy\")\n        if self.target_offset &lt;= 0:\n            raise ValueError(\"target_offset must be positive\")\n</code></pre> <p>\u53c2\u6570: - <code>max_seq_length</code>: \u6700\u5927\u5e8f\u5217\u957f\u5ea6 - <code>min_seq_length</code>: \u6700\u5c0f\u5e8f\u5217\u957f\u5ea6 - <code>padding_token</code>: \u586b\u5145\u6807\u8bb0 - <code>truncate_strategy</code>: \u622a\u65ad\u7b56\u7565 - <code>sequence_stride</code>: \u5e8f\u5217\u6b65\u957f - <code>target_offset</code>: \u76ee\u6807\u504f\u79fb - <code>include_timestamps</code>: \u662f\u5426\u5305\u542b\u65f6\u95f4\u6233 - <code>time_encoding_dim</code>: \u65f6\u95f4\u7f16\u7801\u7ef4\u5ea6</p> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/configs/#truncate_sequencesequence-strategy","title":"truncate_sequence(sequence, strategy)","text":"<p>\u622a\u65ad\u5e8f\u5217\u3002</p> <pre><code>def truncate_sequence(\n    self, \n    sequence: List[Any], \n    strategy: str = None\n) -&gt; List[Any]:\n    \"\"\"\n    \u6839\u636e\u7b56\u7565\u622a\u65ad\u5e8f\u5217\n\n    Args:\n        sequence: \u8f93\u5165\u5e8f\u5217\n        strategy: \u622a\u65ad\u7b56\u7565\uff0c\u5982\u4e3a None \u5219\u4f7f\u7528\u914d\u7f6e\u4e2d\u7684\u7b56\u7565\n\n    Returns:\n        \u622a\u65ad\u540e\u7684\u5e8f\u5217\n    \"\"\"\n    if len(sequence) &lt;= self.max_seq_length:\n        return sequence\n\n    strategy = strategy or self.truncate_strategy\n\n    if strategy == \"recent\":\n        return sequence[-self.max_seq_length:]\n    elif strategy == \"oldest\":\n        return sequence[:self.max_seq_length]\n    elif strategy == \"random\":\n        start_idx = random.randint(0, len(sequence) - self.max_seq_length)\n        return sequence[start_idx:start_idx + self.max_seq_length]\n    else:\n        raise ValueError(f\"Unknown truncate strategy: {strategy}\")\n</code></pre>"},{"location":"zh/api/configs/#pad_sequencesequence","title":"pad_sequence(sequence)","text":"<p>\u586b\u5145\u5e8f\u5217\u3002</p> <pre><code>def pad_sequence(self, sequence: List[Any]) -&gt; List[Any]:\n    \"\"\"\n    \u586b\u5145\u5e8f\u5217\u5230\u6700\u5927\u957f\u5ea6\n\n    Args:\n        sequence: \u8f93\u5165\u5e8f\u5217\n\n    Returns:\n        \u586b\u5145\u540e\u7684\u5e8f\u5217\n    \"\"\"\n    if len(sequence) &gt;= self.max_seq_length:\n        return sequence[:self.max_seq_length]\n\n    pad_length = self.max_seq_length - len(sequence)\n    return sequence + [self.padding_token] * pad_length\n</code></pre>"},{"location":"zh/api/configs/#_4","title":"\u6570\u636e\u5904\u7406\u914d\u7f6e","text":""},{"location":"zh/api/configs/#dataprocessingconfig","title":"DataProcessingConfig","text":"<p>\u6570\u636e\u5904\u7406\u76f8\u5173\u914d\u7f6e\u3002</p> <pre><code>@dataclass\nclass DataProcessingConfig:\n    min_user_interactions: int = 5\n    min_item_interactions: int = 5\n    remove_duplicates: bool = True\n    normalize_ratings: bool = False\n    rating_scale: Tuple[float, float] = (1.0, 5.0)\n    train_ratio: float = 0.7\n    val_ratio: float = 0.15\n    test_ratio: float = 0.15\n    random_seed: int = 42\n\n    def __post_init__(self):\n        \"\"\"\u9a8c\u8bc1\u914d\u7f6e\u53c2\u6570\"\"\"\n        if abs(self.train_ratio + self.val_ratio + self.test_ratio - 1.0) &gt; 1e-6:\n            raise ValueError(\"train_ratio + val_ratio + test_ratio must equal 1.0\")\n        if any(ratio &lt;= 0 for ratio in [self.train_ratio, self.val_ratio, self.test_ratio]):\n            raise ValueError(\"All ratios must be positive\")\n        if self.min_user_interactions &lt;= 0 or self.min_item_interactions &lt;= 0:\n            raise ValueError(\"Minimum interactions must be positive\")\n</code></pre> <p>\u53c2\u6570: - <code>min_user_interactions</code>: \u6700\u5c11\u7528\u6237\u4ea4\u4e92\u6570 - <code>min_item_interactions</code>: \u6700\u5c11\u7269\u54c1\u4ea4\u4e92\u6570 - <code>remove_duplicates</code>: \u662f\u5426\u79fb\u9664\u91cd\u590d\u4ea4\u4e92 - <code>normalize_ratings</code>: \u662f\u5426\u6807\u51c6\u5316\u8bc4\u5206 - <code>rating_scale</code>: \u8bc4\u5206\u8303\u56f4 - <code>train_ratio</code>: \u8bad\u7ec3\u96c6\u6bd4\u4f8b - <code>val_ratio</code>: \u9a8c\u8bc1\u96c6\u6bd4\u4f8b - <code>test_ratio</code>: \u6d4b\u8bd5\u96c6\u6bd4\u4f8b - <code>random_seed</code>: \u968f\u673a\u79cd\u5b50</p> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/configs/#get_split_indicestotal_size","title":"get_split_indices(total_size)","text":"<p>\u83b7\u53d6\u6570\u636e\u5206\u5272\u7d22\u5f15\u3002</p> <pre><code>def get_split_indices(self, total_size: int) -&gt; Tuple[List[int], List[int], List[int]]:\n    \"\"\"\n    \u6839\u636e\u914d\u7f6e\u6bd4\u4f8b\u83b7\u53d6\u6570\u636e\u5206\u5272\u7d22\u5f15\n\n    Args:\n        total_size: \u603b\u6570\u636e\u91cf\n\n    Returns:\n        (train_indices, val_indices, test_indices): \u5206\u5272\u7d22\u5f15\u5217\u8868\n    \"\"\"\n    indices = list(range(total_size))\n    random.Random(self.random_seed).shuffle(indices)\n\n    train_size = int(total_size * self.train_ratio)\n    val_size = int(total_size * self.val_ratio)\n\n    train_indices = indices[:train_size]\n    val_indices = indices[train_size:train_size + val_size]\n    test_indices = indices[train_size + val_size:]\n\n    return train_indices, val_indices, test_indices\n</code></pre>"},{"location":"zh/api/configs/#normalize_ratingrating","title":"normalize_rating(rating)","text":"<p>\u6807\u51c6\u5316\u8bc4\u5206\u3002</p> <pre><code>def normalize_rating(self, rating: float) -&gt; float:\n    \"\"\"\n    \u6807\u51c6\u5316\u8bc4\u5206\u5230 [0, 1] \u8303\u56f4\n\n    Args:\n        rating: \u539f\u59cb\u8bc4\u5206\n\n    Returns:\n        \u6807\u51c6\u5316\u540e\u7684\u8bc4\u5206\n    \"\"\"\n    if not self.normalize_ratings:\n        return rating\n\n    min_rating, max_rating = self.rating_scale\n    return (rating - min_rating) / (max_rating - min_rating)\n</code></pre>"},{"location":"zh/api/configs/#_5","title":"\u7279\u5b9a\u6570\u636e\u96c6\u914d\u7f6e","text":""},{"location":"zh/api/configs/#p5amazonconfig","title":"P5AmazonConfig","text":"<p>P5 Amazon \u6570\u636e\u96c6\u4e13\u7528\u914d\u7f6e\u3002</p> <pre><code>@dataclass\nclass P5AmazonConfig(DatasetConfig):\n    category: str = \"beauty\"\n    min_rating: float = 4.0\n    include_price: bool = True\n    include_brand: bool = True\n    download_url: str = \"https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/\"\n\n    def __post_init__(self):\n        super().__post_init__()\n\n        # \u8bbe\u7f6e\u7279\u5b9a\u7684\u6587\u672c\u6a21\u677f\n        if self.include_price and self.include_brand:\n            template = \"Title: {title}; Brand: {brand}; Category: {category}; Price: {price}\"\n        elif self.include_brand:\n            template = \"Title: {title}; Brand: {brand}; Category: {category}\"\n        else:\n            template = \"Title: {title}; Category: {category}\"\n\n        self.text_config.template = template\n\n    def get_category_url(self) -&gt; str:\n        \"\"\"\u83b7\u53d6\u7279\u5b9a\u7c7b\u522b\u7684\u4e0b\u8f7d URL\"\"\"\n        return f\"{self.download_url}{self.category}.json.gz\"\n</code></pre> <p>\u989d\u5916\u53c2\u6570: - <code>category</code>: \u4ea7\u54c1\u7c7b\u522b - <code>min_rating</code>: \u6700\u4f4e\u8bc4\u5206\u9608\u503c - <code>include_price</code>: \u662f\u5426\u5305\u542b\u4ef7\u683c\u4fe1\u606f - <code>include_brand</code>: \u662f\u5426\u5305\u542b\u54c1\u724c\u4fe1\u606f - <code>download_url</code>: \u4e0b\u8f7d\u57fa\u7840 URL</p>"},{"location":"zh/api/configs/#_6","title":"\u914d\u7f6e\u9a8c\u8bc1\u548c\u5de5\u5177","text":""},{"location":"zh/api/configs/#validate_configconfig","title":"validate_config(config)","text":"<p>\u9a8c\u8bc1\u914d\u7f6e\u5b8c\u6574\u6027\u3002</p> <pre><code>def validate_config(config: DatasetConfig) -&gt; List[str]:\n    \"\"\"\n    \u9a8c\u8bc1\u914d\u7f6e\u7684\u6709\u6548\u6027\n\n    Args:\n        config: \u6570\u636e\u96c6\u914d\u7f6e\n\n    Returns:\n        \u9519\u8bef\u4fe1\u606f\u5217\u8868\uff0c\u7a7a\u5217\u8868\u8868\u793a\u914d\u7f6e\u6709\u6548\n    \"\"\"\n    errors = []\n\n    # \u68c0\u67e5\u6839\u76ee\u5f55\n    if not config.root_dir:\n        errors.append(\"root_dir cannot be empty\")\n\n    # \u68c0\u67e5\u6587\u672c\u914d\u7f6e\n    if config.text_config:\n        if not config.text_config.encoder_model:\n            errors.append(\"encoder_model cannot be empty\")\n        if config.text_config.batch_size &lt;= 0:\n            errors.append(\"batch_size must be positive\")\n\n    # \u68c0\u67e5\u5e8f\u5217\u914d\u7f6e\n    if config.sequence_config:\n        if config.sequence_config.max_seq_length &lt;= config.sequence_config.min_seq_length:\n            errors.append(\"max_seq_length must be greater than min_seq_length\")\n\n    # \u68c0\u67e5\u5904\u7406\u914d\u7f6e\n    if config.processing_config:\n        ratios_sum = (\n            config.processing_config.train_ratio + \n            config.processing_config.val_ratio + \n            config.processing_config.test_ratio\n        )\n        if abs(ratios_sum - 1.0) &gt; 1e-6:\n            errors.append(\"Split ratios must sum to 1.0\")\n\n    return errors\n</code></pre>"},{"location":"zh/api/configs/#load_config_from_fileconfig_path","title":"load_config_from_file(config_path)","text":"<p>\u4ece\u6587\u4ef6\u52a0\u8f7d\u914d\u7f6e\u3002</p> <pre><code>def load_config_from_file(config_path: str) -&gt; DatasetConfig:\n    \"\"\"\n    \u4ece YAML \u6216 JSON \u6587\u4ef6\u52a0\u8f7d\u914d\u7f6e\n\n    Args:\n        config_path: \u914d\u7f6e\u6587\u4ef6\u8def\u5f84\n\n    Returns:\n        \u6570\u636e\u96c6\u914d\u7f6e\u5bf9\u8c61\n    \"\"\"\n    config_path = Path(config_path)\n\n    if config_path.suffix.lower() in ['.yaml', '.yml']:\n        import yaml\n        with open(config_path, 'r') as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.suffix.lower() == '.json':\n        with open(config_path, 'r') as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unsupported config file format: {config_path.suffix}\")\n\n    # \u6839\u636e\u914d\u7f6e\u7c7b\u578b\u521b\u5efa\u76f8\u5e94\u5bf9\u8c61\n    config_type = config_dict.pop('config_type', 'DatasetConfig')\n\n    if config_type == 'P5AmazonConfig':\n        return P5AmazonConfig(**config_dict)\n    else:\n        return DatasetConfig(**config_dict)\n</code></pre>"},{"location":"zh/api/configs/#save_config_to_fileconfig-config_path","title":"save_config_to_file(config, config_path)","text":"<p>\u4fdd\u5b58\u914d\u7f6e\u5230\u6587\u4ef6\u3002</p> <pre><code>def save_config_to_file(config: DatasetConfig, config_path: str) -&gt; None:\n    \"\"\"\n    \u4fdd\u5b58\u914d\u7f6e\u5230 YAML \u6216 JSON \u6587\u4ef6\n\n    Args:\n        config: \u6570\u636e\u96c6\u914d\u7f6e\u5bf9\u8c61\n        config_path: \u914d\u7f6e\u6587\u4ef6\u8def\u5f84\n    \"\"\"\n    config_path = Path(config_path)\n    config_dict = asdict(config)\n\n    # \u6dfb\u52a0\u914d\u7f6e\u7c7b\u578b\u4fe1\u606f\n    config_dict['config_type'] = config.__class__.__name__\n\n    if config_path.suffix.lower() in ['.yaml', '.yml']:\n        import yaml\n        with open(config_path, 'w') as f:\n            yaml.dump(config_dict, f, default_flow_style=False)\n    elif config_path.suffix.lower() == '.json':\n        with open(config_path, 'w') as f:\n            json.dump(config_dict, f, indent=2)\n    else:\n        raise ValueError(f\"Unsupported config file format: {config_path.suffix}\")\n</code></pre>"},{"location":"zh/api/configs/#_7","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/api/configs/#_8","title":"\u57fa\u672c\u914d\u7f6e\u521b\u5efa","text":"<pre><code>from generative_recommenders.data.configs import (\n    DatasetConfig, TextEncodingConfig, SequenceConfig, DataProcessingConfig\n)\n\n# \u521b\u5efa\u57fa\u672c\u914d\u7f6e\nconfig = DatasetConfig(\n    root_dir=\"dataset/amazon\",\n    split=\"beauty\",\n    text_config=TextEncodingConfig(\n        encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n        batch_size=64\n    ),\n    sequence_config=SequenceConfig(\n        max_seq_length=100,\n        min_seq_length=5\n    ),\n    processing_config=DataProcessingConfig(\n        min_user_interactions=10,\n        train_ratio=0.8,\n        val_ratio=0.1,\n        test_ratio=0.1\n    )\n)\n</code></pre>"},{"location":"zh/api/configs/#p5-amazon","title":"P5 Amazon \u914d\u7f6e","text":"<pre><code>from generative_recommenders.data.configs import P5AmazonConfig\n\n# \u521b\u5efa P5 Amazon \u914d\u7f6e\nconfig = P5AmazonConfig(\n    root_dir=\"dataset/amazon\",\n    category=\"beauty\",\n    min_rating=4.0,\n    include_price=True,\n    include_brand=True\n)\n</code></pre>"},{"location":"zh/api/configs/#_9","title":"\u914d\u7f6e\u6587\u4ef6\u64cd\u4f5c","text":"<pre><code># \u4fdd\u5b58\u914d\u7f6e\u5230\u6587\u4ef6\nsave_config_to_file(config, \"config/dataset_config.yaml\")\n\n# \u4ece\u6587\u4ef6\u52a0\u8f7d\u914d\u7f6e\nloaded_config = load_config_from_file(\"config/dataset_config.yaml\")\n\n# \u9a8c\u8bc1\u914d\u7f6e\nerrors = validate_config(loaded_config)\nif errors:\n    print(\"Configuration errors:\")\n    for error in errors:\n        print(f\"  - {error}\")\nelse:\n    print(\"Configuration is valid\")\n</code></pre>"},{"location":"zh/api/dataset-factory/","title":"\u6570\u636e\u96c6\u5de5\u5382 API \u53c2\u8003","text":"<p>\u6570\u636e\u96c6\u521b\u5efa\u5de5\u5382\u6a21\u5f0f\u7684\u8be6\u7ec6\u6587\u6863\uff0c\u7528\u4e8e\u7edf\u4e00\u7ba1\u7406\u548c\u521b\u5efa\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\u96c6\u3002</p>"},{"location":"zh/api/dataset-factory/#_1","title":"\u6838\u5fc3\u5de5\u5382\u7c7b","text":""},{"location":"zh/api/dataset-factory/#datasetfactory","title":"DatasetFactory","text":"<p>\u6570\u636e\u96c6\u5de5\u5382\u7684\u6838\u5fc3\u7c7b\uff0c\u7ba1\u7406\u6570\u636e\u96c6\u6ce8\u518c\u548c\u521b\u5efa\u3002</p> <pre><code>class DatasetFactory:\n    \"\"\"\u6570\u636e\u96c6\u5de5\u5382\u7c7b\"\"\"\n\n    _registered_datasets = {}\n\n    @classmethod\n    def register_dataset(\n        cls,\n        name: str,\n        base_class: Type[BaseRecommenderDataset],\n        item_class: Type[ItemDataset],\n        sequence_class: Type[SequenceDataset]\n    ) -&gt; None:\n        \"\"\"\n        \u6ce8\u518c\u6570\u636e\u96c6\u7c7b\n\n        Args:\n            name: \u6570\u636e\u96c6\u540d\u79f0\n            base_class: \u57fa\u7840\u6570\u636e\u96c6\u7c7b\n            item_class: \u7269\u54c1\u6570\u636e\u96c6\u7c7b\n            sequence_class: \u5e8f\u5217\u6570\u636e\u96c6\u7c7b\n        \"\"\"\n        cls._registered_datasets[name] = {\n            'base': base_class,\n            'item': item_class,\n            'sequence': sequence_class\n        }\n        print(f\"Registered dataset: {name}\")\n</code></pre> <p>\u7c7b\u65b9\u6cd5:</p>"},{"location":"zh/api/dataset-factory/#list_datasets","title":"list_datasets()","text":"<p>\u5217\u51fa\u6240\u6709\u5df2\u6ce8\u518c\u7684\u6570\u636e\u96c6\u3002</p> <pre><code>@classmethod\ndef list_datasets(cls) -&gt; List[str]:\n    \"\"\"\n    \u5217\u51fa\u6240\u6709\u5df2\u6ce8\u518c\u7684\u6570\u636e\u96c6\n\n    Returns:\n        \u6570\u636e\u96c6\u540d\u79f0\u5217\u8868\n    \"\"\"\n    return list(cls._registered_datasets.keys())\n</code></pre>"},{"location":"zh/api/dataset-factory/#get_dataset_infoname","title":"get_dataset_info(name)","text":"<p>\u83b7\u53d6\u6570\u636e\u96c6\u4fe1\u606f\u3002</p> <pre><code>@classmethod\ndef get_dataset_info(cls, name: str) -&gt; Dict[str, Type]:\n    \"\"\"\n    \u83b7\u53d6\u6307\u5b9a\u6570\u636e\u96c6\u7684\u7c7b\u4fe1\u606f\n\n    Args:\n        name: \u6570\u636e\u96c6\u540d\u79f0\n\n    Returns:\n        \u5305\u542b\u6570\u636e\u96c6\u7c7b\u7684\u5b57\u5178\n\n    Raises:\n        ValueError: \u5982\u679c\u6570\u636e\u96c6\u672a\u6ce8\u518c\n    \"\"\"\n    if name not in cls._registered_datasets:\n        available = \", \".join(cls.list_datasets())\n        raise ValueError(f\"Dataset '{name}' not registered. Available: {available}\")\n\n    return cls._registered_datasets[name]\n</code></pre>"},{"location":"zh/api/dataset-factory/#create_base_datasetname-kwargs","title":"create_base_dataset(name, **kwargs)","text":"<p>\u521b\u5efa\u57fa\u7840\u6570\u636e\u96c6\u5b9e\u4f8b\u3002</p> <pre><code>@classmethod\ndef create_base_dataset(cls, name: str, **kwargs) -&gt; BaseRecommenderDataset:\n    \"\"\"\n    \u521b\u5efa\u57fa\u7840\u6570\u636e\u96c6\u5b9e\u4f8b\n\n    Args:\n        name: \u6570\u636e\u96c6\u540d\u79f0\n        **kwargs: \u4f20\u9012\u7ed9\u6570\u636e\u96c6\u6784\u9020\u51fd\u6570\u7684\u53c2\u6570\n\n    Returns:\n        \u57fa\u7840\u6570\u636e\u96c6\u5b9e\u4f8b\n    \"\"\"\n    dataset_info = cls.get_dataset_info(name)\n    base_class = dataset_info['base']\n\n    # \u521b\u5efa\u914d\u7f6e\u5bf9\u8c61\n    if 'config' not in kwargs:\n        config_class = cls._get_config_class(base_class)\n        kwargs['config'] = config_class(**kwargs)\n\n    return base_class(kwargs['config'])\n</code></pre>"},{"location":"zh/api/dataset-factory/#create_item_datasetname-kwargs","title":"create_item_dataset(name, **kwargs)","text":"<p>\u521b\u5efa\u7269\u54c1\u6570\u636e\u96c6\u5b9e\u4f8b\u3002</p> <pre><code>@classmethod\ndef create_item_dataset(cls, name: str, **kwargs) -&gt; ItemDataset:\n    \"\"\"\n    \u521b\u5efa\u7269\u54c1\u6570\u636e\u96c6\u5b9e\u4f8b\n\n    Args:\n        name: \u6570\u636e\u96c6\u540d\u79f0\n        **kwargs: \u4f20\u9012\u7ed9\u6570\u636e\u96c6\u6784\u9020\u51fd\u6570\u7684\u53c2\u6570\n\n    Returns:\n        \u7269\u54c1\u6570\u636e\u96c6\u5b9e\u4f8b\n    \"\"\"\n    dataset_info = cls.get_dataset_info(name)\n    item_class = dataset_info['item']\n\n    return item_class(**kwargs)\n</code></pre>"},{"location":"zh/api/dataset-factory/#create_sequence_datasetname-kwargs","title":"create_sequence_dataset(name, **kwargs)","text":"<p>\u521b\u5efa\u5e8f\u5217\u6570\u636e\u96c6\u5b9e\u4f8b\u3002</p> <pre><code>@classmethod\ndef create_sequence_dataset(cls, name: str, **kwargs) -&gt; SequenceDataset:\n    \"\"\"\n    \u521b\u5efa\u5e8f\u5217\u6570\u636e\u96c6\u5b9e\u4f8b\n\n    Args:\n        name: \u6570\u636e\u96c6\u540d\u79f0\n        **kwargs: \u4f20\u9012\u7ed9\u6570\u636e\u96c6\u6784\u9020\u51fd\u6570\u7684\u53c2\u6570\n\n    Returns:\n        \u5e8f\u5217\u6570\u636e\u96c6\u5b9e\u4f8b\n    \"\"\"\n    dataset_info = cls.get_dataset_info(name)\n    sequence_class = dataset_info['sequence']\n\n    return sequence_class(**kwargs)\n</code></pre>"},{"location":"zh/api/dataset-factory/#_get_config_classbase_class","title":"_get_config_class(base_class)","text":"<p>\u83b7\u53d6\u6570\u636e\u96c6\u5bf9\u5e94\u7684\u914d\u7f6e\u7c7b\u3002</p> <pre><code>@classmethod\ndef _get_config_class(cls, base_class: Type[BaseRecommenderDataset]) -&gt; Type[DatasetConfig]:\n    \"\"\"\n    \u6839\u636e\u57fa\u7840\u6570\u636e\u96c6\u7c7b\u83b7\u53d6\u5bf9\u5e94\u7684\u914d\u7f6e\u7c7b\n\n    Args:\n        base_class: \u57fa\u7840\u6570\u636e\u96c6\u7c7b\n\n    Returns:\n        \u914d\u7f6e\u7c7b\n    \"\"\"\n    # \u901a\u8fc7\u7c7b\u540d\u6216\u6ce8\u89e3\u63a8\u65ad\u914d\u7f6e\u7c7b\n    if hasattr(base_class, '_config_class'):\n        return base_class._config_class\n\n    # \u9ed8\u8ba4\u914d\u7f6e\u7c7b\u6620\u5c04\n    config_mapping = {\n        'P5AmazonDataset': P5AmazonConfig,\n        'MovieLensDataset': MovieLensConfig,\n        # \u53ef\u4ee5\u7ee7\u7eed\u6dfb\u52a0\u5176\u4ed6\u6620\u5c04\n    }\n\n    class_name = base_class.__name__\n    return config_mapping.get(class_name, DatasetConfig)\n</code></pre>"},{"location":"zh/api/dataset-factory/#_2","title":"\u6570\u636e\u96c6\u6ce8\u518c\u5668","text":""},{"location":"zh/api/dataset-factory/#datasetregistry","title":"DatasetRegistry","text":"<p>\u6570\u636e\u96c6\u6ce8\u518c\u7ba1\u7406\u5668\u3002</p> <pre><code>class DatasetRegistry:\n    \"\"\"\u6570\u636e\u96c6\u6ce8\u518c\u7ba1\u7406\u5668\"\"\"\n\n    def __init__(self):\n        self.datasets = {}\n        self.auto_register_builtin_datasets()\n\n    def register(\n        self,\n        name: str,\n        base_class: Type[BaseRecommenderDataset],\n        item_class: Type[ItemDataset] = None,\n        sequence_class: Type[SequenceDataset] = None,\n        config_class: Type[DatasetConfig] = None\n    ) -&gt; None:\n        \"\"\"\n        \u6ce8\u518c\u6570\u636e\u96c6\n\n        Args:\n            name: \u6570\u636e\u96c6\u540d\u79f0\n            base_class: \u57fa\u7840\u6570\u636e\u96c6\u7c7b\n            item_class: \u7269\u54c1\u6570\u636e\u96c6\u7c7b\n            sequence_class: \u5e8f\u5217\u6570\u636e\u96c6\u7c7b\n            config_class: \u914d\u7f6e\u7c7b\n        \"\"\"\n        # \u81ea\u52a8\u751f\u6210\u5305\u88c5\u7c7b\uff08\u5982\u679c\u672a\u63d0\u4f9b\uff09\n        if item_class is None:\n            item_class = self._create_item_wrapper(name, base_class)\n\n        if sequence_class is None:\n            sequence_class = self._create_sequence_wrapper(name, base_class)\n\n        # \u8bbe\u7f6e\u914d\u7f6e\u7c7b\n        if config_class:\n            base_class._config_class = config_class\n\n        self.datasets[name] = {\n            'base': base_class,\n            'item': item_class,\n            'sequence': sequence_class,\n            'config': config_class or DatasetConfig\n        }\n\n    def _create_item_wrapper(\n        self, \n        name: str, \n        base_class: Type[BaseRecommenderDataset]\n    ) -&gt; Type[ItemDataset]:\n        \"\"\"\u52a8\u6001\u521b\u5efa\u7269\u54c1\u6570\u636e\u96c6\u5305\u88c5\u7c7b\"\"\"\n\n        class DynamicItemDataset(ItemDataset):\n            def __init__(self, **kwargs):\n                # \u521b\u5efa\u57fa\u7840\u6570\u636e\u96c6\n                config_class = getattr(base_class, '_config_class', DatasetConfig)\n                config = config_class(**kwargs)\n                base_dataset = base_class(config)\n\n                # \u521d\u59cb\u5316\u7269\u54c1\u6570\u636e\u96c6\n                super().__init__(\n                    base_dataset=base_dataset,\n                    split=kwargs.get('train_test_split', 'all'),\n                    return_text=kwargs.get('return_text', False)\n                )\n\n        DynamicItemDataset.__name__ = f\"{name.title()}ItemDataset\"\n        return DynamicItemDataset\n\n    def _create_sequence_wrapper(\n        self, \n        name: str, \n        base_class: Type[BaseRecommenderDataset]\n    ) -&gt; Type[SequenceDataset]:\n        \"\"\"\u52a8\u6001\u521b\u5efa\u5e8f\u5217\u6570\u636e\u96c6\u5305\u88c5\u7c7b\"\"\"\n\n        class DynamicSequenceDataset(SequenceDataset):\n            def __init__(self, **kwargs):\n                # \u521b\u5efa\u57fa\u7840\u6570\u636e\u96c6\n                config_class = getattr(base_class, '_config_class', DatasetConfig)\n                config = config_class(**kwargs)\n                base_dataset = base_class(config)\n\n                # \u52a0\u8f7d\u8bed\u4e49\u7f16\u7801\u5668\n                semantic_encoder = None\n                if 'pretrained_rqvae_path' in kwargs:\n                    from generative_recommenders.models.rqvae import RqVae\n                    semantic_encoder = RqVae.load_from_checkpoint(kwargs['pretrained_rqvae_path'])\n                    semantic_encoder.eval()\n\n                # \u521d\u59cb\u5316\u5e8f\u5217\u6570\u636e\u96c6\n                super().__init__(\n                    base_dataset=base_dataset,\n                    split=kwargs.get('train_test_split', 'train'),\n                    semantic_encoder=semantic_encoder\n                )\n\n        DynamicSequenceDataset.__name__ = f\"{name.title()}SequenceDataset\"\n        return DynamicSequenceDataset\n\n    def auto_register_builtin_datasets(self) -&gt; None:\n        \"\"\"\u81ea\u52a8\u6ce8\u518c\u5185\u7f6e\u6570\u636e\u96c6\"\"\"\n        try:\n            from generative_recommenders.data.p5_amazon import (\n                P5AmazonDataset, P5AmazonItemDataset, P5AmazonSequenceDataset\n            )\n            from generative_recommenders.data.configs import P5AmazonConfig\n\n            self.register(\n                name=\"p5_amazon\",\n                base_class=P5AmazonDataset,\n                item_class=P5AmazonItemDataset,\n                sequence_class=P5AmazonSequenceDataset,\n                config_class=P5AmazonConfig\n            )\n        except ImportError:\n            pass\n\n        # \u53ef\u4ee5\u7ee7\u7eed\u6dfb\u52a0\u5176\u4ed6\u5185\u7f6e\u6570\u636e\u96c6\n</code></pre>"},{"location":"zh/api/dataset-factory/#_3","title":"\u914d\u7f6e\u6784\u5efa\u5668","text":""},{"location":"zh/api/dataset-factory/#configbuilder","title":"ConfigBuilder","text":"<p>\u914d\u7f6e\u5bf9\u8c61\u6784\u5efa\u5668\u3002</p> <pre><code>class ConfigBuilder:\n    \"\"\"\u914d\u7f6e\u6784\u5efa\u5668\"\"\"\n\n    def __init__(self, config_class: Type[DatasetConfig]):\n        self.config_class = config_class\n        self.params = {}\n\n    def set_root_dir(self, root_dir: str) -&gt; 'ConfigBuilder':\n        \"\"\"\u8bbe\u7f6e\u6839\u76ee\u5f55\"\"\"\n        self.params['root_dir'] = root_dir\n        return self\n\n    def set_split(self, split: str) -&gt; 'ConfigBuilder':\n        \"\"\"\u8bbe\u7f6e\u6570\u636e\u5206\u5272\"\"\"\n        self.params['split'] = split\n        return self\n\n    def set_text_config(self, **kwargs) -&gt; 'ConfigBuilder':\n        \"\"\"\u8bbe\u7f6e\u6587\u672c\u914d\u7f6e\"\"\"\n        self.params['text_config'] = TextEncodingConfig(**kwargs)\n        return self\n\n    def set_sequence_config(self, **kwargs) -&gt; 'ConfigBuilder':\n        \"\"\"\u8bbe\u7f6e\u5e8f\u5217\u914d\u7f6e\"\"\"\n        self.params['sequence_config'] = SequenceConfig(**kwargs)\n        return self\n\n    def set_processing_config(self, **kwargs) -&gt; 'ConfigBuilder':\n        \"\"\"\u8bbe\u7f6e\u5904\u7406\u914d\u7f6e\"\"\"\n        self.params['processing_config'] = DataProcessingConfig(**kwargs)\n        return self\n\n    def build(self) -&gt; DatasetConfig:\n        \"\"\"\u6784\u5efa\u914d\u7f6e\u5bf9\u8c61\"\"\"\n        return self.config_class(**self.params)\n</code></pre>"},{"location":"zh/api/dataset-factory/#_4","title":"\u6570\u636e\u96c6\u7ba1\u7406\u5668","text":""},{"location":"zh/api/dataset-factory/#datasetmanager","title":"DatasetManager","text":"<p>\u6570\u636e\u96c6\u751f\u547d\u5468\u671f\u7ba1\u7406\u5668\u3002</p> <pre><code>class DatasetManager:\n    \"\"\"\u6570\u636e\u96c6\u7ba1\u7406\u5668\"\"\"\n\n    def __init__(self):\n        self.registry = DatasetRegistry()\n        self.cache = {}\n\n    def create_dataset(\n        self,\n        name: str,\n        dataset_type: str = \"item\",\n        cache_key: str = None,\n        **kwargs\n    ) -&gt; Union[ItemDataset, SequenceDataset]:\n        \"\"\"\n        \u521b\u5efa\u6570\u636e\u96c6\u5b9e\u4f8b\n\n        Args:\n            name: \u6570\u636e\u96c6\u540d\u79f0\n            dataset_type: \u6570\u636e\u96c6\u7c7b\u578b (\"item\" \u6216 \"sequence\")\n            cache_key: \u7f13\u5b58\u952e\n            **kwargs: \u6570\u636e\u96c6\u53c2\u6570\n\n        Returns:\n            \u6570\u636e\u96c6\u5b9e\u4f8b\n        \"\"\"\n        # \u68c0\u67e5\u7f13\u5b58\n        if cache_key and cache_key in self.cache:\n            return self.cache[cache_key]\n\n        # \u83b7\u53d6\u6570\u636e\u96c6\u4fe1\u606f\n        if name not in self.registry.datasets:\n            raise ValueError(f\"Dataset '{name}' not registered\")\n\n        dataset_info = self.registry.datasets[name]\n\n        # \u521b\u5efa\u6570\u636e\u96c6\n        if dataset_type == \"item\":\n            dataset = dataset_info['item'](**kwargs)\n        elif dataset_type == \"sequence\":\n            dataset = dataset_info['sequence'](**kwargs)\n        else:\n            raise ValueError(f\"Invalid dataset_type: {dataset_type}\")\n\n        # \u7f13\u5b58\u7ed3\u679c\n        if cache_key:\n            self.cache[cache_key] = dataset\n\n        return dataset\n\n    def get_dataset_config(self, name: str, **kwargs) -&gt; DatasetConfig:\n        \"\"\"\n        \u83b7\u53d6\u6570\u636e\u96c6\u914d\u7f6e\n\n        Args:\n            name: \u6570\u636e\u96c6\u540d\u79f0\n            **kwargs: \u914d\u7f6e\u53c2\u6570\n\n        Returns:\n            \u914d\u7f6e\u5bf9\u8c61\n        \"\"\"\n        if name not in self.registry.datasets:\n            raise ValueError(f\"Dataset '{name}' not registered\")\n\n        config_class = self.registry.datasets[name]['config']\n        return config_class(**kwargs)\n\n    def list_datasets(self) -&gt; List[str]:\n        \"\"\"\u5217\u51fa\u6240\u6709\u53ef\u7528\u6570\u636e\u96c6\"\"\"\n        return list(self.registry.datasets.keys())\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"\u6e05\u7a7a\u7f13\u5b58\"\"\"\n        self.cache.clear()\n</code></pre>"},{"location":"zh/api/dataset-factory/#_5","title":"\u5de5\u5177\u51fd\u6570","text":""},{"location":"zh/api/dataset-factory/#register_dataset_from_modulemodule_path","title":"register_dataset_from_module(module_path)","text":"<p>\u4ece\u6a21\u5757\u6ce8\u518c\u6570\u636e\u96c6\u3002</p> <pre><code>def register_dataset_from_module(module_path: str) -&gt; None:\n    \"\"\"\n    \u4ece\u6a21\u5757\u81ea\u52a8\u6ce8\u518c\u6570\u636e\u96c6\n\n    Args:\n        module_path: \u6a21\u5757\u8def\u5f84\uff0c\u5982 \"my_package.my_dataset\"\n    \"\"\"\n    import importlib\n\n    module = importlib.import_module(module_path)\n\n    # \u67e5\u627e\u6570\u636e\u96c6\u7c7b\n    base_classes = []\n    item_classes = []\n    sequence_classes = []\n\n    for name in dir(module):\n        obj = getattr(module, name)\n        if isinstance(obj, type):\n            if issubclass(obj, BaseRecommenderDataset) and obj != BaseRecommenderDataset:\n                base_classes.append(obj)\n            elif issubclass(obj, ItemDataset) and obj != ItemDataset:\n                item_classes.append(obj)\n            elif issubclass(obj, SequenceDataset) and obj != SequenceDataset:\n                sequence_classes.append(obj)\n\n    # \u81ea\u52a8\u5339\u914d\u548c\u6ce8\u518c\n    for base_class in base_classes:\n        dataset_name = base_class.__name__.lower().replace('dataset', '')\n\n        # \u67e5\u627e\u5bf9\u5e94\u7684\u5305\u88c5\u7c7b\n        item_class = None\n        sequence_class = None\n\n        for cls in item_classes:\n            if dataset_name in cls.__name__.lower():\n                item_class = cls\n                break\n\n        for cls in sequence_classes:\n            if dataset_name in cls.__name__.lower():\n                sequence_class = cls\n                break\n\n        # \u6ce8\u518c\u6570\u636e\u96c6\n        if item_class or sequence_class:\n            DatasetFactory.register_dataset(\n                name=dataset_name,\n                base_class=base_class,\n                item_class=item_class,\n                sequence_class=sequence_class\n            )\n</code></pre>"},{"location":"zh/api/dataset-factory/#create_dataset_from_configconfig_path","title":"create_dataset_from_config(config_path)","text":"<p>\u4ece\u914d\u7f6e\u6587\u4ef6\u521b\u5efa\u6570\u636e\u96c6\u3002</p> <pre><code>def create_dataset_from_config(config_path: str) -&gt; Union[ItemDataset, SequenceDataset]:\n    \"\"\"\n    \u4ece\u914d\u7f6e\u6587\u4ef6\u521b\u5efa\u6570\u636e\u96c6\n\n    Args:\n        config_path: \u914d\u7f6e\u6587\u4ef6\u8def\u5f84\n\n    Returns:\n        \u6570\u636e\u96c6\u5b9e\u4f8b\n    \"\"\"\n    import yaml\n\n    with open(config_path, 'r') as f:\n        config_data = yaml.safe_load(f)\n\n    # \u63d0\u53d6\u6570\u636e\u96c6\u4fe1\u606f\n    dataset_name = config_data['dataset']['name']\n    dataset_type = config_data['dataset']['type']\n    dataset_params = config_data['dataset'].get('params', {})\n\n    # \u521b\u5efa\u6570\u636e\u96c6\n    manager = DatasetManager()\n    return manager.create_dataset(\n        name=dataset_name,\n        dataset_type=dataset_type,\n        **dataset_params\n    )\n</code></pre>"},{"location":"zh/api/dataset-factory/#_6","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/api/dataset-factory/#_7","title":"\u6ce8\u518c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6","text":"<pre><code>from generative_recommenders.data.dataset_factory import DatasetFactory\nfrom my_package.my_dataset import MyDataset, MyItemDataset, MySequenceDataset\n\n# \u65b9\u5f0f1\uff1a\u624b\u52a8\u6ce8\u518c\nDatasetFactory.register_dataset(\n    name=\"my_dataset\",\n    base_class=MyDataset,\n    item_class=MyItemDataset,\n    sequence_class=MySequenceDataset\n)\n\n# \u65b9\u5f0f2\uff1a\u4f7f\u7528\u6ce8\u518c\u5668\nregistry = DatasetRegistry()\nregistry.register(\n    name=\"my_dataset\",\n    base_class=MyDataset,\n    item_class=MyItemDataset,\n    sequence_class=MySequenceDataset\n)\n</code></pre>"},{"location":"zh/api/dataset-factory/#_8","title":"\u521b\u5efa\u6570\u636e\u96c6\u5b9e\u4f8b","text":"<pre><code># \u4f7f\u7528\u5de5\u5382\u65b9\u6cd5\nitem_dataset = DatasetFactory.create_item_dataset(\n    \"p5_amazon\",\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n\nsequence_dataset = DatasetFactory.create_sequence_dataset(\n    \"p5_amazon\",\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\",\n    pretrained_rqvae_path=\"checkpoints/rqvae.ckpt\"\n)\n</code></pre>"},{"location":"zh/api/dataset-factory/#_9","title":"\u4f7f\u7528\u6570\u636e\u96c6\u7ba1\u7406\u5668","text":"<pre><code># \u521b\u5efa\u7ba1\u7406\u5668\nmanager = DatasetManager()\n\n# \u5217\u51fa\u53ef\u7528\u6570\u636e\u96c6\nprint(\"Available datasets:\", manager.list_datasets())\n\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset = manager.create_dataset(\n    name=\"p5_amazon\",\n    dataset_type=\"item\",\n    cache_key=\"amazon_beauty_train\",\n    root=\"dataset/amazon\",\n    split=\"beauty\"\n)\n</code></pre>"},{"location":"zh/api/dataset-factory/#_10","title":"\u914d\u7f6e\u6784\u5efa\u5668","text":"<pre><code># \u4f7f\u7528\u6784\u5efa\u5668\u521b\u5efa\u914d\u7f6e\nconfig = (ConfigBuilder(P5AmazonConfig)\n    .set_root_dir(\"dataset/amazon\")\n    .set_split(\"beauty\")\n    .set_text_config(\n        encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n        batch_size=64\n    )\n    .set_sequence_config(\n        max_seq_length=100,\n        min_seq_length=5\n    )\n    .build())\n\n# \u4f7f\u7528\u914d\u7f6e\u521b\u5efa\u6570\u636e\u96c6\nbase_dataset = P5AmazonDataset(config)\n</code></pre>"},{"location":"zh/api/dataset-factory/#_11","title":"\u4ece\u914d\u7f6e\u6587\u4ef6\u521b\u5efa","text":"<pre><code># dataset_config.yaml\ndataset:\n  name: p5_amazon\n  type: item\n  params:\n    root: \"dataset/amazon\"\n    split: \"beauty\"\n    train_test_split: \"train\"\n    encoder_model_name: \"sentence-transformers/all-MiniLM-L6-v2\"\n</code></pre> <pre><code># \u4ece\u914d\u7f6e\u6587\u4ef6\u52a0\u8f7d\ndataset = create_dataset_from_config(\"dataset_config.yaml\")\n</code></pre>"},{"location":"zh/api/datasets/","title":"\u6570\u636e\u96c6 API \u53c2\u8003","text":"<p>GenerativeRecommenders \u6570\u636e\u96c6\u6a21\u5757\u7684\u8be6\u7ec6 API \u6587\u6863\u3002</p>"},{"location":"zh/api/datasets/#_1","title":"\u57fa\u7840\u6570\u636e\u96c6\u7c7b","text":""},{"location":"zh/api/datasets/#baserecommenderdataset","title":"BaseRecommenderDataset","text":"<p>\u6240\u6709\u63a8\u8350\u7cfb\u7edf\u6570\u636e\u96c6\u7684\u62bd\u8c61\u57fa\u7c7b\u3002</p> <pre><code>class BaseRecommenderDataset:\n    \"\"\"\u63a8\u8350\u7cfb\u7edf\u6570\u636e\u96c6\u57fa\u7c7b\"\"\"\n\n    def __init__(self, config: DatasetConfig):\n        self.config = config\n        self.root_dir = Path(config.root_dir)\n        self._items_df = None\n        self._interactions_df = None\n\n    @abstractmethod\n    def download(self) -&gt; None:\n        \"\"\"\u4e0b\u8f7d\u6570\u636e\u96c6\"\"\"\n        pass\n\n    @abstractmethod\n    def load_raw_data(self) -&gt; Dict[str, pd.DataFrame]:\n        \"\"\"\u52a0\u8f7d\u539f\u59cb\u6570\u636e\"\"\"\n        pass\n\n    @abstractmethod\n    def preprocess_data(self, raw_data: Dict[str, pd.DataFrame]) -&gt; Dict[str, pd.DataFrame]:\n        \"\"\"\u9884\u5904\u7406\u6570\u636e\"\"\"\n        pass\n</code></pre> <p>\u4e3b\u8981\u65b9\u6cd5\uff1a</p>"},{"location":"zh/api/datasets/#load_dataset","title":"load_dataset()","text":"<p>\u52a0\u8f7d\u6570\u636e\u96c6\u3002</p> <pre><code>def load_dataset(self, force_reload: bool = False) -&gt; None:\n    \"\"\"\n    \u52a0\u8f7d\u6570\u636e\u96c6\n\n    Args:\n        force_reload: \u662f\u5426\u5f3a\u5236\u91cd\u65b0\u52a0\u8f7d\n    \"\"\"\n</code></pre>"},{"location":"zh/api/datasets/#get_items","title":"get_items()","text":"<p>\u83b7\u53d6\u7269\u54c1\u6570\u636e\u3002</p> <pre><code>def get_items(self) -&gt; pd.DataFrame:\n    \"\"\"\n    \u83b7\u53d6\u7269\u54c1\u6570\u636e\n\n    Returns:\n        \u7269\u54c1 DataFrame\n    \"\"\"\n</code></pre>"},{"location":"zh/api/datasets/#get_interactions","title":"get_interactions()","text":"<p>\u83b7\u53d6\u4ea4\u4e92\u6570\u636e\u3002</p> <pre><code>def get_interactions(self) -&gt; pd.DataFrame:\n    \"\"\"\n    \u83b7\u53d6\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u6570\u636e\n\n    Returns:\n        \u4ea4\u4e92 DataFrame\n    \"\"\"\n</code></pre>"},{"location":"zh/api/datasets/#_2","title":"\u7269\u54c1\u6570\u636e\u96c6\u7c7b","text":""},{"location":"zh/api/datasets/#itemdataset","title":"ItemDataset","text":"<p>\u7528\u4e8e\u7269\u54c1\u7f16\u7801\u548c\u7279\u5f81\u5b66\u4e60\u7684\u6570\u636e\u96c6\u7c7b\u3002</p> <pre><code>class ItemDataset(Dataset):\n    \"\"\"\u7269\u54c1\u6570\u636e\u96c6\u7c7b\"\"\"\n\n    def __init__(\n        self,\n        base_dataset: BaseRecommenderDataset,\n        split: str = \"all\",\n        return_text: bool = False\n    ):\n        self.base_dataset = base_dataset\n        self.split = split\n        self.return_text = return_text\n</code></pre> <p>\u53c2\u6570\uff1a - <code>base_dataset</code>: \u57fa\u7840\u6570\u636e\u96c6\u5b9e\u4f8b - <code>split</code>: \u6570\u636e\u5206\u5272\uff08\"train\", \"val\", \"test\", \"all\"\uff09 - <code>return_text</code>: \u662f\u5426\u8fd4\u56de\u6587\u672c\u7279\u5f81</p> <p>\u65b9\u6cd5\uff1a</p>"},{"location":"zh/api/datasets/#getitemidx","title":"getitem(idx)","text":"<p>\u83b7\u53d6\u6570\u636e\u9879\u3002</p> <pre><code>def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    \u83b7\u53d6\u6307\u5b9a\u7d22\u5f15\u7684\u6570\u636e\u9879\n\n    Args:\n        idx: \u6570\u636e\u7d22\u5f15\n\n    Returns:\n        \u5305\u542b\u7269\u54c1\u4fe1\u606f\u7684\u5b57\u5178\n    \"\"\"\n</code></pre>"},{"location":"zh/api/datasets/#_3","title":"\u5e8f\u5217\u6570\u636e\u96c6\u7c7b","text":""},{"location":"zh/api/datasets/#sequencedataset","title":"SequenceDataset","text":"<p>\u7528\u4e8e\u5e8f\u5217\u751f\u6210\u8bad\u7ec3\u7684\u6570\u636e\u96c6\u7c7b\u3002</p> <pre><code>class SequenceDataset(Dataset):\n    \"\"\"\u5e8f\u5217\u6570\u636e\u96c6\u7c7b\"\"\"\n\n    def __init__(\n        self,\n        base_dataset: BaseRecommenderDataset,\n        split: str = \"train\",\n        semantic_encoder: Optional[nn.Module] = None\n    ):\n        self.base_dataset = base_dataset\n        self.split = split\n        self.semantic_encoder = semantic_encoder\n</code></pre> <p>\u53c2\u6570\uff1a - <code>base_dataset</code>: \u57fa\u7840\u6570\u636e\u96c6\u5b9e\u4f8b - <code>split</code>: \u6570\u636e\u5206\u5272 - <code>semantic_encoder</code>: \u8bed\u4e49\u7f16\u7801\u5668\uff08\u5982 RQVAE\uff09</p> <p>\u65b9\u6cd5\uff1a</p>"},{"location":"zh/api/datasets/#create_sequences","title":"create_sequences()","text":"<p>\u521b\u5efa\u7528\u6237\u5e8f\u5217\u3002</p> <pre><code>def create_sequences(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    \u521b\u5efa\u7528\u6237\u4ea4\u4e92\u5e8f\u5217\n\n    Returns:\n        \u5e8f\u5217\u5217\u8868\n    \"\"\"\n</code></pre>"},{"location":"zh/api/datasets/#encode_sequence","title":"encode_sequence()","text":"<p>\u7f16\u7801\u5e8f\u5217\u3002</p> <pre><code>def encode_sequence(self, item_ids: List[int]) -&gt; torch.Tensor:\n    \"\"\"\n    \u5c06\u7269\u54c1 ID \u5e8f\u5217\u7f16\u7801\u4e3a\u8bed\u4e49\u8868\u793a\n\n    Args:\n        item_ids: \u7269\u54c1 ID \u5217\u8868\n\n    Returns:\n        \u7f16\u7801\u540e\u7684\u5e8f\u5217\u5f20\u91cf\n    \"\"\"\n</code></pre>"},{"location":"zh/api/datasets/#_4","title":"\u5177\u4f53\u6570\u636e\u96c6\u5b9e\u73b0","text":""},{"location":"zh/api/datasets/#p5amazondataset","title":"P5AmazonDataset","text":"<p>P5 Amazon \u6570\u636e\u96c6\u5b9e\u73b0\u3002</p> <pre><code>@gin.configurable\nclass P5AmazonDataset(BaseRecommenderDataset):\n    \"\"\"P5 Amazon \u6570\u636e\u96c6\"\"\"\n\n    def __init__(self, config: P5AmazonConfig):\n        super().__init__(config)\n        self.category = config.category\n        self.min_rating = config.min_rating\n</code></pre> <p>\u7279\u8272\u529f\u80fd\uff1a - \u652f\u6301\u591a\u4e2a\u4ea7\u54c1\u7c7b\u522b - \u81ea\u52a8\u4e0b\u8f7d\u548c\u9884\u5904\u7406 - \u6587\u672c\u7279\u5f81\u63d0\u53d6 - \u8bc4\u5206\u8fc7\u6ee4</p>"},{"location":"zh/api/datasets/#p5amazonitemdataset","title":"P5AmazonItemDataset","text":"<p>P5 Amazon \u7269\u54c1\u6570\u636e\u96c6\u5c01\u88c5\u3002</p> <pre><code>@gin.configurable\nclass P5AmazonItemDataset(ItemDataset):\n    \"\"\"P5 Amazon \u7269\u54c1\u6570\u636e\u96c6\"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        split: str = \"beauty\",\n        train_test_split: str = \"all\",\n        return_text: bool = False,\n        **kwargs\n    ):\n</code></pre>"},{"location":"zh/api/datasets/#p5amazonsequencedataset","title":"P5AmazonSequenceDataset","text":"<p>P5 Amazon \u5e8f\u5217\u6570\u636e\u96c6\u5c01\u88c5\u3002</p> <pre><code>@gin.configurable\nclass P5AmazonSequenceDataset(SequenceDataset):\n    \"\"\"P5 Amazon \u5e8f\u5217\u6570\u636e\u96c6\"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        split: str = \"beauty\", \n        train_test_split: str = \"train\",\n        pretrained_rqvae_path: str = None,\n        **kwargs\n    ):\n</code></pre>"},{"location":"zh/api/datasets/#_5","title":"\u6570\u636e\u96c6\u5de5\u5382","text":""},{"location":"zh/api/datasets/#datasetfactory","title":"DatasetFactory","text":"<p>\u6570\u636e\u96c6\u5de5\u5382\u7c7b\uff0c\u7528\u4e8e\u7edf\u4e00\u7ba1\u7406\u548c\u521b\u5efa\u6570\u636e\u96c6\u3002</p> <pre><code>class DatasetFactory:\n    \"\"\"\u6570\u636e\u96c6\u5de5\u5382\"\"\"\n\n    _registered_datasets = {}\n\n    @classmethod\n    def register_dataset(\n        cls,\n        name: str,\n        base_class: Type[BaseRecommenderDataset],\n        item_class: Type[ItemDataset],\n        sequence_class: Type[SequenceDataset]\n    ) -&gt; None:\n        \"\"\"\u6ce8\u518c\u6570\u636e\u96c6\u7c7b\"\"\"\n</code></pre> <p>\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code># \u6ce8\u518c\u6570\u636e\u96c6\nDatasetFactory.register_dataset(\n    \"p5_amazon\",\n    P5AmazonDataset,\n    P5AmazonItemDataset, \n    P5AmazonSequenceDataset\n)\n\n# \u521b\u5efa\u6570\u636e\u96c6\nitem_dataset = DatasetFactory.create_item_dataset(\n    \"p5_amazon\",\n    root=\"data/amazon\",\n    split=\"beauty\"\n)\n</code></pre>"},{"location":"zh/api/datasets/#_6","title":"\u6570\u636e\u5904\u7406\u5668","text":""},{"location":"zh/api/datasets/#textprocessor","title":"TextProcessor","text":"<p>\u6587\u672c\u5904\u7406\u5668\uff0c\u7528\u4e8e\u7269\u54c1\u6587\u672c\u7279\u5f81\u7f16\u7801\u3002</p> <pre><code>class TextProcessor:\n    \"\"\"\u6587\u672c\u5904\u7406\u5668\"\"\"\n\n    def __init__(self, config: TextEncodingConfig):\n        self.config = config\n        self.encoder = SentenceTransformer(config.encoder_model)\n</code></pre> <p>\u65b9\u6cd5\uff1a</p>"},{"location":"zh/api/datasets/#encode_item_features","title":"encode_item_features()","text":"<p>\u7f16\u7801\u7269\u54c1\u6587\u672c\u7279\u5f81\u3002</p> <pre><code>def encode_item_features(\n    self,\n    items_df: pd.DataFrame,\n    cache_key: str = None,\n    force_reload: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"\n    \u7f16\u7801\u7269\u54c1\u6587\u672c\u7279\u5f81\n\n    Args:\n        items_df: \u7269\u54c1\u6570\u636e\u6846\n        cache_key: \u7f13\u5b58\u952e\n        force_reload: \u662f\u5426\u5f3a\u5236\u91cd\u65b0\u8ba1\u7b97\n\n    Returns:\n        \u7269\u54c1\u6587\u672c\u7f16\u7801\u5f20\u91cf\n    \"\"\"\n</code></pre>"},{"location":"zh/api/datasets/#sequenceprocessor","title":"SequenceProcessor","text":"<p>\u5e8f\u5217\u5904\u7406\u5668\uff0c\u7528\u4e8e\u5e8f\u5217\u6570\u636e\u9884\u5904\u7406\u3002</p> <pre><code>class SequenceProcessor:\n    \"\"\"\u5e8f\u5217\u5904\u7406\u5668\"\"\"\n\n    def __init__(self, config: SequenceConfig):\n        self.config = config\n</code></pre> <p>\u65b9\u6cd5\uff1a</p>"},{"location":"zh/api/datasets/#process_user_sequence","title":"process_user_sequence()","text":"<p>\u5904\u7406\u7528\u6237\u5e8f\u5217\u3002</p> <pre><code>def process_user_sequence(\n    self,\n    sequence: List[int],\n    target_offset: int = 1\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    \u5904\u7406\u7528\u6237\u4ea4\u4e92\u5e8f\u5217\n\n    Args:\n        sequence: \u539f\u59cb\u5e8f\u5217\n        target_offset: \u76ee\u6807\u504f\u79fb\u91cf\n\n    Returns:\n        \u5904\u7406\u540e\u7684\u5e8f\u5217\u6570\u636e\n    \"\"\"\n</code></pre>"},{"location":"zh/api/datasets/#_7","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/api/datasets/#_8","title":"\u57fa\u7840\u4f7f\u7528","text":"<pre><code>from generative_recommenders.data import P5AmazonDataset, P5AmazonConfig\n\n# \u521b\u5efa\u914d\u7f6e\nconfig = P5AmazonConfig(\n    root_dir=\"data/amazon\",\n    split=\"beauty\"\n)\n\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset = P5AmazonDataset(config)\ndataset.load_dataset()\n\n# \u83b7\u53d6\u6570\u636e\nitems = dataset.get_items()\ninteractions = dataset.get_interactions()\n</code></pre>"},{"location":"zh/api/datasets/#_9","title":"\u7269\u54c1\u6570\u636e\u96c6\u4f7f\u7528","text":"<pre><code>from generative_recommenders.data import P5AmazonItemDataset\n\n# \u521b\u5efa\u7269\u54c1\u6570\u636e\u96c6\nitem_dataset = P5AmazonItemDataset(\n    root=\"data/amazon\",\n    split=\"beauty\",\n    return_text=True\n)\n\n# \u4f7f\u7528 DataLoader\ndataloader = DataLoader(item_dataset, batch_size=32, shuffle=True)\nfor batch in dataloader:\n    item_ids = batch['item_id']\n    text_features = batch['text_features']\n    # \u8bad\u7ec3\u7269\u54c1\u7f16\u7801\u5668...\n</code></pre>"},{"location":"zh/api/datasets/#_10","title":"\u5e8f\u5217\u6570\u636e\u96c6\u4f7f\u7528","text":"<pre><code>from generative_recommenders.data import P5AmazonSequenceDataset\nfrom generative_recommenders.models import RqVae\n\n# \u52a0\u8f7d\u9884\u8bad\u7ec3\u7684 RQVAE\nrqvae = RqVae.load_from_checkpoint(\"checkpoints/rqvae.ckpt\")\n\n# \u521b\u5efa\u5e8f\u5217\u6570\u636e\u96c6\nseq_dataset = P5AmazonSequenceDataset(\n    root=\"data/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\",\n    pretrained_rqvae_path=\"checkpoints/rqvae.ckpt\"\n)\n\n# \u4f7f\u7528 DataLoader\ndataloader = DataLoader(seq_dataset, batch_size=16, shuffle=True)\nfor batch in dataloader:\n    input_ids = batch['input_ids']\n    target_ids = batch['target_ids']\n    # \u8bad\u7ec3\u5e8f\u5217\u751f\u6210\u6a21\u578b...\n</code></pre>"},{"location":"zh/api/datasets/#_11","title":"\u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u914d\u7f6e\u7ba1\u7406 - \u6570\u636e\u96c6\u914d\u7f6e\u7cfb\u7edf</li> <li>\u5904\u7406\u5668 - \u6570\u636e\u5904\u7406\u5de5\u5177</li> <li>\u6570\u636e\u96c6\u5de5\u5382 - \u5de5\u5382\u6a21\u5f0f\u521b\u5efa\u6570\u636e\u96c6</li> <li>\u8bad\u7ec3\u5668 - \u6a21\u578b\u8bad\u7ec3\u5de5\u5177</li> </ul>"},{"location":"zh/api/modules/","title":"\u6a21\u5757 API \u53c2\u8003","text":"<p>\u6838\u5fc3\u6784\u5efa\u6a21\u5757\u7684\u8be6\u7ec6\u6587\u6863\uff0c\u5305\u62ec\u7f16\u7801\u5668\u3001\u635f\u5931\u51fd\u6570\u3001\u6307\u6807\u7b49\u3002</p>"},{"location":"zh/api/modules/#_1","title":"\u7f16\u7801\u5668\u6a21\u5757","text":""},{"location":"zh/api/modules/#transformerencoder","title":"TransformerEncoder","text":"<p>\u57fa\u4e8e Transformer \u7684\u7f16\u7801\u5668\u3002</p> <pre><code>class TransformerEncoder(nn.Module):\n    \"\"\"Transformer \u7f16\u7801\u5668\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        embedding_dim: int,\n        num_heads: int,\n        num_layers: int,\n        attn_dim: int,\n        dropout: float = 0.1,\n        max_seq_length: int = 1024\n    ):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.pos_encoding = PositionalEncoding(embedding_dim, max_seq_length)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embedding_dim,\n            nhead=num_heads,\n            dim_feedforward=attn_dim,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.dropout = nn.Dropout(dropout)\n</code></pre> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/modules/#forwardinput_ids-attention_mask","title":"forward(input_ids, attention_mask)","text":"<p>\u524d\u5411\u4f20\u64ad\u3002</p> <pre><code>def forward(\n    self, \n    input_ids: torch.Tensor, \n    attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    \u524d\u5411\u4f20\u64ad\n\n    Args:\n        input_ids: \u8f93\u5165\u5e8f\u5217 (batch_size, seq_len)\n        attention_mask: \u6ce8\u610f\u529b\u63a9\u7801 (batch_size, seq_len)\n\n    Returns:\n        \u7f16\u7801\u540e\u7684\u5e8f\u5217 (batch_size, seq_len, embedding_dim)\n    \"\"\"\n    # \u5d4c\u5165\u548c\u4f4d\u7f6e\u7f16\u7801\n    embeddings = self.embedding(input_ids)\n    embeddings = self.pos_encoding(embeddings)\n    embeddings = self.dropout(embeddings)\n\n    # \u521b\u5efa\u586b\u5145\u63a9\u7801\n    if attention_mask is not None:\n        # \u8f6c\u6362\u4e3a Transformer \u671f\u671b\u7684\u683c\u5f0f\n        src_key_padding_mask = (attention_mask == 0)\n    else:\n        src_key_padding_mask = None\n\n    # Transformer \u7f16\u7801\n    encoded = self.transformer(\n        embeddings,\n        src_key_padding_mask=src_key_padding_mask\n    )\n\n    return encoded\n</code></pre>"},{"location":"zh/api/modules/#positionalencoding","title":"PositionalEncoding","text":"<p>\u4f4d\u7f6e\u7f16\u7801\u6a21\u5757\u3002</p> <pre><code>class PositionalEncoding(nn.Module):\n    \"\"\"\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801\"\"\"\n\n    def __init__(self, embedding_dim: int, max_seq_length: int = 5000):\n        super().__init__()\n\n        pe = torch.zeros(max_seq_length, embedding_dim)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n\n        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * \n                           (-math.log(10000.0) / embedding_dim))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        \u6dfb\u52a0\u4f4d\u7f6e\u7f16\u7801\n\n        Args:\n            x: \u8f93\u5165\u5d4c\u5165 (batch_size, seq_len, embedding_dim)\n\n        Returns:\n            \u6dfb\u52a0\u4f4d\u7f6e\u7f16\u7801\u540e\u7684\u5d4c\u5165\n        \"\"\"\n        return x + self.pe[:, :x.size(1)]\n</code></pre>"},{"location":"zh/api/modules/#multiheadattention","title":"MultiHeadAttention","text":"<p>\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3002</p> <pre><code>class MultiHeadAttention(nn.Module):\n    \"\"\"\u591a\u5934\u6ce8\u610f\u529b\"\"\"\n\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_heads: int,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        assert embedding_dim % num_heads == 0\n\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.head_dim = embedding_dim // num_heads\n\n        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n        self.w_o = nn.Linear(embedding_dim, embedding_dim)\n\n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(self.head_dim)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        mask: Optional[torch.Tensor] = None\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        \u591a\u5934\u6ce8\u610f\u529b\u8ba1\u7b97\n\n        Args:\n            query: \u67e5\u8be2\u5411\u91cf (batch_size, seq_len, embedding_dim)\n            key: \u952e\u5411\u91cf (batch_size, seq_len, embedding_dim)\n            value: \u503c\u5411\u91cf (batch_size, seq_len, embedding_dim)\n            mask: \u6ce8\u610f\u529b\u63a9\u7801 (batch_size, seq_len, seq_len)\n\n        Returns:\n            (attention_output, attention_weights): \u6ce8\u610f\u529b\u8f93\u51fa\u548c\u6743\u91cd\n        \"\"\"\n        batch_size, seq_len, _ = query.size()\n\n        # \u7ebf\u6027\u53d8\u6362\n        Q = self.w_q(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.w_k(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # \u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n\n        # \u5e94\u7528\u63a9\u7801\n        if mask is not None:\n            mask = mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n            scores.masked_fill_(mask == 0, -1e9)\n\n        # \u6ce8\u610f\u529b\u6743\u91cd\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n\n        # \u6ce8\u610f\u529b\u8f93\u51fa\n        attention_output = torch.matmul(attention_weights, V)\n        attention_output = attention_output.transpose(1, 2).contiguous().view(\n            batch_size, seq_len, self.embedding_dim\n        )\n\n        output = self.w_o(attention_output)\n\n        return output, attention_weights\n</code></pre>"},{"location":"zh/api/modules/#_2","title":"\u635f\u5931\u51fd\u6570","text":""},{"location":"zh/api/modules/#vqvaeloss","title":"VQVAELoss","text":"<p>VQVAE \u635f\u5931\u51fd\u6570\u3002</p> <pre><code>class VQVAELoss(nn.Module):\n    \"\"\"VQVAE \u635f\u5931\u51fd\u6570\"\"\"\n\n    def __init__(\n        self,\n        commitment_cost: float = 0.25,\n        beta: float = 1.0\n    ):\n        super().__init__()\n        self.commitment_cost = commitment_cost\n        self.beta = beta\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        x_recon: torch.Tensor,\n        commitment_loss: torch.Tensor,\n        embedding_loss: torch.Tensor\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        \u8ba1\u7b97 VQVAE \u635f\u5931\n\n        Args:\n            x: \u539f\u59cb\u8f93\u5165\n            x_recon: \u91cd\u6784\u8f93\u51fa\n            commitment_loss: \u627f\u8bfa\u635f\u5931\n            embedding_loss: \u5d4c\u5165\u635f\u5931\n\n        Returns:\n            \u635f\u5931\u5b57\u5178\n        \"\"\"\n        # \u91cd\u6784\u635f\u5931\n        recon_loss = F.mse_loss(x_recon, x, reduction='mean')\n\n        # \u603b\u635f\u5931\n        total_loss = (\n            recon_loss + \n            self.commitment_cost * commitment_loss + \n            self.beta * embedding_loss\n        )\n\n        return {\n            'total_loss': total_loss,\n            'reconstruction_loss': recon_loss,\n            'commitment_loss': commitment_loss,\n            'embedding_loss': embedding_loss\n        }\n</code></pre>"},{"location":"zh/api/modules/#sequenceloss","title":"SequenceLoss","text":"<p>\u5e8f\u5217\u5efa\u6a21\u635f\u5931\u51fd\u6570\u3002</p> <pre><code>class SequenceLoss(nn.Module):\n    \"\"\"\u5e8f\u5217\u5efa\u6a21\u635f\u5931\u51fd\u6570\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        ignore_index: int = -100,\n        label_smoothing: float = 0.0\n    ):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.ignore_index = ignore_index\n        self.label_smoothing = label_smoothing\n\n        if label_smoothing &gt; 0:\n            self.criterion = nn.CrossEntropyLoss(\n                ignore_index=ignore_index,\n                label_smoothing=label_smoothing\n            )\n        else:\n            self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n\n    def forward(\n        self,\n        logits: torch.Tensor,\n        labels: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        \u8ba1\u7b97\u5e8f\u5217\u5efa\u6a21\u635f\u5931\n\n        Args:\n            logits: \u6a21\u578b\u8f93\u51fa (batch_size, seq_len, vocab_size)\n            labels: \u76ee\u6807\u6807\u7b7e (batch_size, seq_len)\n            attention_mask: \u6ce8\u610f\u529b\u63a9\u7801 (batch_size, seq_len)\n\n        Returns:\n            \u635f\u5931\u5b57\u5178\n        \"\"\"\n        # \u5c55\u5e73\u5f20\u91cf\n        flat_logits = logits.view(-1, self.vocab_size)\n        flat_labels = labels.view(-1)\n\n        # \u8ba1\u7b97\u635f\u5931\n        loss = self.criterion(flat_logits, flat_labels)\n\n        # \u8ba1\u7b97\u51c6\u786e\u7387\n        with torch.no_grad():\n            predictions = torch.argmax(flat_logits, dim=-1)\n            mask = (flat_labels != self.ignore_index)\n            correct = (predictions == flat_labels) &amp; mask\n            accuracy = correct.sum().float() / mask.sum().float()\n\n        return {\n            'loss': loss,\n            'accuracy': accuracy\n        }\n</code></pre>"},{"location":"zh/api/modules/#contrastiveloss","title":"ContrastiveLoss","text":"<p>\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\u51fd\u6570\u3002</p> <pre><code>class ContrastiveLoss(nn.Module):\n    \"\"\"\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\u51fd\u6570\"\"\"\n\n    def __init__(\n        self,\n        temperature: float = 0.1,\n        margin: float = 0.2\n    ):\n        super().__init__()\n        self.temperature = temperature\n        self.margin = margin\n\n    def forward(\n        self,\n        anchor: torch.Tensor,\n        positive: torch.Tensor,\n        negative: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        \u8ba1\u7b97\u5bf9\u6bd4\u635f\u5931\n\n        Args:\n            anchor: \u951a\u70b9\u5d4c\u5165 (batch_size, embedding_dim)\n            positive: \u6b63\u6837\u672c\u5d4c\u5165 (batch_size, embedding_dim)\n            negative: \u8d1f\u6837\u672c\u5d4c\u5165 (batch_size, num_negatives, embedding_dim)\n\n        Returns:\n            \u5bf9\u6bd4\u635f\u5931\n        \"\"\"\n        # \u6807\u51c6\u5316\u5d4c\u5165\n        anchor = F.normalize(anchor, dim=-1)\n        positive = F.normalize(positive, dim=-1)\n        negative = F.normalize(negative, dim=-1)\n\n        # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\n        pos_sim = torch.sum(anchor * positive, dim=-1) / self.temperature\n        neg_sim = torch.bmm(negative, anchor.unsqueeze(-1)).squeeze(-1) / self.temperature\n\n        # \u8ba1\u7b97\u5bf9\u6bd4\u635f\u5931\n        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)\n        labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n\n        loss = F.cross_entropy(logits, labels)\n\n        return loss\n</code></pre>"},{"location":"zh/api/modules/#_3","title":"\u8bc4\u4f30\u6307\u6807","text":""},{"location":"zh/api/modules/#recommendationmetrics","title":"RecommendationMetrics","text":"<p>\u63a8\u8350\u7cfb\u7edf\u8bc4\u4f30\u6307\u6807\u3002</p> <pre><code>class RecommendationMetrics:\n    \"\"\"\u63a8\u8350\u7cfb\u7edf\u8bc4\u4f30\u6307\u6807\"\"\"\n\n    @staticmethod\n    def recall_at_k(predictions: List[List[int]], targets: List[List[int]], k: int) -&gt; float:\n        \"\"\"\n        \u8ba1\u7b97 Recall@K\n\n        Args:\n            predictions: \u9884\u6d4b\u5217\u8868\n            targets: \u76ee\u6807\u5217\u8868\n            k: Top-K\n\n        Returns:\n            Recall@K \u503c\n        \"\"\"\n        recall_scores = []\n\n        for pred, target in zip(predictions, targets):\n            if len(target) == 0:\n                continue\n\n            top_k_pred = set(pred[:k])\n            target_set = set(target)\n\n            recall = len(top_k_pred &amp; target_set) / len(target_set)\n            recall_scores.append(recall)\n\n        return np.mean(recall_scores) if recall_scores else 0.0\n\n    @staticmethod\n    def precision_at_k(predictions: List[List[int]], targets: List[List[int]], k: int) -&gt; float:\n        \"\"\"\u8ba1\u7b97 Precision@K\"\"\"\n        precision_scores = []\n\n        for pred, target in zip(predictions, targets):\n            if k == 0:\n                continue\n\n            top_k_pred = set(pred[:k])\n            target_set = set(target)\n\n            precision = len(top_k_pred &amp; target_set) / k\n            precision_scores.append(precision)\n\n        return np.mean(precision_scores) if precision_scores else 0.0\n\n    @staticmethod\n    def ndcg_at_k(predictions: List[List[int]], targets: List[List[int]], k: int) -&gt; float:\n        \"\"\"\u8ba1\u7b97 NDCG@K\"\"\"\n        ndcg_scores = []\n\n        for pred, target in zip(predictions, targets):\n            if len(target) == 0:\n                continue\n\n            # \u8ba1\u7b97 DCG\n            dcg = 0\n            for i, item in enumerate(pred[:k]):\n                if item in target:\n                    dcg += 1 / np.log2(i + 2)\n\n            # \u8ba1\u7b97 IDCG\n            idcg = sum(1 / np.log2(i + 2) for i in range(min(len(target), k)))\n\n            # \u8ba1\u7b97 NDCG\n            ndcg = dcg / idcg if idcg &gt; 0 else 0\n            ndcg_scores.append(ndcg)\n\n        return np.mean(ndcg_scores) if ndcg_scores else 0.0\n\n    @staticmethod\n    def hit_rate_at_k(predictions: List[List[int]], targets: List[List[int]], k: int) -&gt; float:\n        \"\"\"\u8ba1\u7b97 Hit Rate@K\"\"\"\n        hits = 0\n        total = 0\n\n        for pred, target in zip(predictions, targets):\n            if len(target) == 0:\n                continue\n\n            top_k_pred = set(pred[:k])\n            target_set = set(target)\n\n            if len(top_k_pred &amp; target_set) &gt; 0:\n                hits += 1\n            total += 1\n\n        return hits / total if total &gt; 0 else 0.0\n\n    @staticmethod\n    def coverage(predictions: List[List[int]], total_items: int) -&gt; float:\n        \"\"\"\u8ba1\u7b97\u7269\u54c1\u8986\u76d6\u5ea6\"\"\"\n        recommended_items = set()\n        for pred in predictions:\n            recommended_items.update(pred)\n\n        return len(recommended_items) / total_items\n\n    @staticmethod\n    def diversity(predictions: List[List[int]]) -&gt; float:\n        \"\"\"\u8ba1\u7b97\u63a8\u8350\u591a\u6837\u6027\uff08\u5e73\u5747 Jaccard \u8ddd\u79bb\uff09\"\"\"\n        if len(predictions) &lt; 2:\n            return 0.0\n\n        distances = []\n        for i in range(len(predictions)):\n            for j in range(i + 1, len(predictions)):\n                set_i = set(predictions[i])\n                set_j = set(predictions[j])\n\n                if len(set_i | set_j) &gt; 0:\n                    jaccard_sim = len(set_i &amp; set_j) / len(set_i | set_j)\n                    jaccard_dist = 1 - jaccard_sim\n                    distances.append(jaccard_dist)\n\n        return np.mean(distances) if distances else 0.0\n</code></pre>"},{"location":"zh/api/modules/#_4","title":"\u5de5\u5177\u6a21\u5757","text":""},{"location":"zh/api/modules/#attentionvisualization","title":"AttentionVisualization","text":"<p>\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u5de5\u5177\u3002</p> <pre><code>class AttentionVisualization:\n    \"\"\"\u6ce8\u610f\u529b\u6743\u91cd\u53ef\u89c6\u5316\"\"\"\n\n    @staticmethod\n    def plot_attention_heatmap(\n        attention_weights: torch.Tensor,\n        input_tokens: List[str],\n        output_tokens: List[str],\n        save_path: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"\n        \u7ed8\u5236\u6ce8\u610f\u529b\u70ed\u529b\u56fe\n\n        Args:\n            attention_weights: \u6ce8\u610f\u529b\u6743\u91cd (seq_len_out, seq_len_in)\n            input_tokens: \u8f93\u5165\u6807\u8bb0\u5217\u8868\n            output_tokens: \u8f93\u51fa\u6807\u8bb0\u5217\u8868\n            save_path: \u4fdd\u5b58\u8def\u5f84\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        plt.figure(figsize=(10, 8))\n\n        # \u521b\u5efa\u70ed\u529b\u56fe\n        sns.heatmap(\n            attention_weights.cpu().numpy(),\n            xticklabels=input_tokens,\n            yticklabels=output_tokens,\n            cmap='Blues',\n            annot=True,\n            fmt='.2f'\n        )\n\n        plt.title('Attention Weights')\n        plt.xlabel('Input Tokens')\n        plt.ylabel('Output Tokens')\n        plt.xticks(rotation=45)\n        plt.yticks(rotation=0)\n\n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n\n        plt.show()\n</code></pre>"},{"location":"zh/api/modules/#modelutils","title":"ModelUtils","text":"<p>\u6a21\u578b\u5de5\u5177\u51fd\u6570\u3002</p> <pre><code>class ModelUtils:\n    \"\"\"\u6a21\u578b\u5de5\u5177\u51fd\u6570\"\"\"\n\n    @staticmethod\n    def count_parameters(model: nn.Module) -&gt; int:\n        \"\"\"\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u6570\u91cf\"\"\"\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    @staticmethod\n    def get_model_size(model: nn.Module) -&gt; str:\n        \"\"\"\u83b7\u53d6\u6a21\u578b\u5927\u5c0f\uff08\u4ee5 MB \u4e3a\u5355\u4f4d\uff09\"\"\"\n        param_size = 0\n        buffer_size = 0\n\n        for param in model.parameters():\n            param_size += param.nelement() * param.element_size()\n\n        for buffer in model.buffers():\n            buffer_size += buffer.nelement() * buffer.element_size()\n\n        size_mb = (param_size + buffer_size) / 1024 / 1024\n        return f\"{size_mb:.2f} MB\"\n\n    @staticmethod\n    def freeze_layers(model: nn.Module, layer_names: List[str]) -&gt; None:\n        \"\"\"\u51bb\u7ed3\u6307\u5b9a\u5c42\u7684\u53c2\u6570\"\"\"\n        for name, param in model.named_parameters():\n            for layer_name in layer_names:\n                if layer_name in name:\n                    param.requires_grad = False\n                    break\n\n    @staticmethod\n    def unfreeze_layers(model: nn.Module, layer_names: List[str]) -&gt; None:\n        \"\"\"\u89e3\u51bb\u6307\u5b9a\u5c42\u7684\u53c2\u6570\"\"\"\n        for name, param in model.named_parameters():\n            for layer_name in layer_names:\n                if layer_name in name:\n                    param.requires_grad = True\n                    break\n\n    @staticmethod\n    def initialize_weights(model: nn.Module, init_type: str = 'xavier') -&gt; None:\n        \"\"\"\u521d\u59cb\u5316\u6a21\u578b\u6743\u91cd\"\"\"\n        for name, param in model.named_parameters():\n            if 'weight' in name:\n                if init_type == 'xavier':\n                    nn.init.xavier_uniform_(param)\n                elif init_type == 'kaiming':\n                    nn.init.kaiming_uniform_(param)\n                elif init_type == 'normal':\n                    nn.init.normal_(param, mean=0, std=0.02)\n            elif 'bias' in name:\n                nn.init.constant_(param, 0)\n</code></pre>"},{"location":"zh/api/modules/#_5","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/api/modules/#_6","title":"\u4f7f\u7528\u7f16\u7801\u5668","text":"<pre><code>from generative_recommenders.modules import TransformerEncoder\n\n# \u521b\u5efa\u7f16\u7801\u5668\nencoder = TransformerEncoder(\n    vocab_size=1000,\n    embedding_dim=512,\n    num_heads=8,\n    num_layers=6,\n    attn_dim=2048\n)\n\n# \u7f16\u7801\u5e8f\u5217\ninput_ids = torch.randint(0, 1000, (32, 50))  # (batch_size, seq_len)\nattention_mask = torch.ones_like(input_ids)\n\nencoded = encoder(input_ids, attention_mask)\nprint(f\"Encoded shape: {encoded.shape}\")  # (32, 50, 512)\n</code></pre>"},{"location":"zh/api/modules/#_7","title":"\u4f7f\u7528\u635f\u5931\u51fd\u6570","text":"<pre><code>from generative_recommenders.modules import VQVAELoss, SequenceLoss\n\n# VQVAE \u635f\u5931\nvqvae_loss = VQVAELoss(commitment_cost=0.25)\nlosses = vqvae_loss(x, x_recon, commitment_loss, embedding_loss)\n\n# \u5e8f\u5217\u635f\u5931\nseq_loss = SequenceLoss(vocab_size=1000, label_smoothing=0.1)\nlosses = seq_loss(logits, labels, attention_mask)\n</code></pre>"},{"location":"zh/api/modules/#_8","title":"\u8ba1\u7b97\u8bc4\u4f30\u6307\u6807","text":"<pre><code>from generative_recommenders.modules import RecommendationMetrics\n\n# \u793a\u4f8b\u6570\u636e\npredictions = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\ntargets = [[1, 3, 5], [7, 9]]\n\n# \u8ba1\u7b97\u6307\u6807\nrecall_5 = RecommendationMetrics.recall_at_k(predictions, targets, 5)\nndcg_5 = RecommendationMetrics.ndcg_at_k(predictions, targets, 5)\nhit_rate = RecommendationMetrics.hit_rate_at_k(predictions, targets, 5)\n\nprint(f\"Recall@5: {recall_5:.4f}\")\nprint(f\"NDCG@5: {ndcg_5:.4f}\")\nprint(f\"Hit Rate@5: {hit_rate:.4f}\")\n</code></pre>"},{"location":"zh/api/modules/#_9","title":"\u6a21\u578b\u5de5\u5177","text":"<pre><code>from generative_recommenders.modules import ModelUtils\n\n# \u6a21\u578b\u4fe1\u606f\nparam_count = ModelUtils.count_parameters(model)\nmodel_size = ModelUtils.get_model_size(model)\n\nprint(f\"Parameters: {param_count:,}\")\nprint(f\"Model size: {model_size}\")\n\n# \u51bb\u7ed3/\u89e3\u51bb\u5c42\nModelUtils.freeze_layers(model, ['embedding', 'pos_encoding'])\nModelUtils.unfreeze_layers(model, ['transformer'])\n\n# \u6743\u91cd\u521d\u59cb\u5316\nModelUtils.initialize_weights(model, init_type='xavier')\n</code></pre>"},{"location":"zh/api/processors/","title":"\u5904\u7406\u5668 API \u53c2\u8003","text":"<p>\u6587\u672c\u548c\u5e8f\u5217\u5904\u7406\u5de5\u5177\u7684\u8be6\u7ec6\u6587\u6863\u3002</p>"},{"location":"zh/api/processors/#_1","title":"\u6587\u672c\u5904\u7406\u5668","text":""},{"location":"zh/api/processors/#textprocessor","title":"TextProcessor","text":"<p>\u6587\u672c\u7f16\u7801\u548c\u5904\u7406\u7684\u6838\u5fc3\u7c7b\u3002</p> <pre><code>class TextProcessor:\n    def __init__(self, config: TextEncodingConfig):\n        self.config = config\n        self.model = None\n        self.device = config.device\n        self.cache_manager = CacheManager(config.cache_dir)\n</code></pre> <p>\u53c2\u6570: - <code>config</code>: \u6587\u672c\u7f16\u7801\u914d\u7f6e\u5bf9\u8c61</p> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/processors/#load_model","title":"load_model()","text":"<p>\u52a0\u8f7d\u6587\u672c\u7f16\u7801\u6a21\u578b\u3002</p> <pre><code>def load_model(self) -&gt; None:\n    \"\"\"\n    \u52a0\u8f7d Sentence Transformer \u6a21\u578b\n    \"\"\"\n    if self.model is None:\n        from sentence_transformers import SentenceTransformer\n        self.model = SentenceTransformer(self.config.encoder_model)\n        self.model.to(self.device)\n        print(f\"Loaded text encoder: {self.config.encoder_model}\")\n</code></pre>"},{"location":"zh/api/processors/#encode_textstexts-cache_key-force_reload","title":"encode_texts(texts, cache_key, force_reload)","text":"<p>\u7f16\u7801\u6587\u672c\u5217\u8868\u3002</p> <pre><code>def encode_texts(\n    self,\n    texts: List[str],\n    cache_key: Optional[str] = None,\n    force_reload: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    \u7f16\u7801\u6587\u672c\u5217\u8868\u4e3a\u5d4c\u5165\u5411\u91cf\n\n    Args:\n        texts: \u6587\u672c\u5217\u8868\n        cache_key: \u7f13\u5b58\u952e\uff0c\u5982\u679c\u63d0\u4f9b\u5c06\u5c1d\u8bd5\u4f7f\u7528\u7f13\u5b58\n        force_reload: \u662f\u5426\u5f3a\u5236\u91cd\u65b0\u8ba1\u7b97\n\n    Returns:\n        \u5d4c\u5165\u77e9\u9635 (num_texts, embedding_dim)\n    \"\"\"\n    # \u68c0\u67e5\u7f13\u5b58\n    if cache_key and not force_reload and self.cache_manager.exists(cache_key):\n        print(f\"Loading embeddings from cache: {cache_key}\")\n        return self.cache_manager.load(cache_key)\n\n    # \u52a0\u8f7d\u6a21\u578b\n    self.load_model()\n\n    # \u6279\u91cf\u7f16\u7801\n    print(f\"Encoding {len(texts)} texts with {self.config.encoder_model}\")\n    embeddings = []\n\n    for i in range(0, len(texts), self.config.batch_size):\n        batch_texts = texts[i:i + self.config.batch_size]\n        batch_embeddings = self.model.encode(\n            batch_texts,\n            convert_to_numpy=True,\n            normalize_embeddings=self.config.normalize_embeddings,\n            show_progress_bar=True\n        )\n        embeddings.append(batch_embeddings)\n\n    # \u5408\u5e76\u7ed3\u679c\n    embeddings = np.vstack(embeddings)\n\n    # \u4fdd\u5b58\u7f13\u5b58\n    if cache_key:\n        self.cache_manager.save(cache_key, embeddings)\n        print(f\"Saved embeddings to cache: {cache_key}\")\n\n    return embeddings\n</code></pre>"},{"location":"zh/api/processors/#encode_item_featuresitems_df-cache_key-force_reload","title":"encode_item_features(items_df, cache_key, force_reload)","text":"<p>\u7f16\u7801\u7269\u54c1\u7279\u5f81\u3002</p> <pre><code>def encode_item_features(\n    self,\n    items_df: pd.DataFrame,\n    cache_key: Optional[str] = None,\n    force_reload: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    \u7f16\u7801\u7269\u54c1\u7279\u5f81\u4e3a\u5d4c\u5165\u5411\u91cf\n\n    Args:\n        items_df: \u7269\u54c1\u6570\u636e\u6846\n        cache_key: \u7f13\u5b58\u952e\n        force_reload: \u662f\u5426\u5f3a\u5236\u91cd\u65b0\u8ba1\u7b97\n\n    Returns:\n        \u7269\u54c1\u5d4c\u5165\u77e9\u9635 (num_items, embedding_dim)\n    \"\"\"\n    # \u683c\u5f0f\u5316\u6587\u672c\n    texts = []\n    for _, row in items_df.iterrows():\n        text = self.config.format_text(row.to_dict())\n        texts.append(text)\n\n    return self.encode_texts(texts, cache_key, force_reload)\n</code></pre>"},{"location":"zh/api/processors/#encode_single_texttext","title":"encode_single_text(text)","text":"<p>\u7f16\u7801\u5355\u4e2a\u6587\u672c\u3002</p> <pre><code>def encode_single_text(self, text: str) -&gt; np.ndarray:\n    \"\"\"\n    \u7f16\u7801\u5355\u4e2a\u6587\u672c\n\n    Args:\n        text: \u8f93\u5165\u6587\u672c\n\n    Returns:\n        \u6587\u672c\u5d4c\u5165\u5411\u91cf (embedding_dim,)\n    \"\"\"\n    self.load_model()\n\n    embedding = self.model.encode(\n        [text],\n        convert_to_numpy=True,\n        normalize_embeddings=self.config.normalize_embeddings\n    )[0]\n\n    return embedding\n</code></pre>"},{"location":"zh/api/processors/#compute_similaritytext1-text2","title":"compute_similarity(text1, text2)","text":"<p>\u8ba1\u7b97\u6587\u672c\u76f8\u4f3c\u5ea6\u3002</p> <pre><code>def compute_similarity(self, text1: str, text2: str) -&gt; float:\n    \"\"\"\n    \u8ba1\u7b97\u4e24\u4e2a\u6587\u672c\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\n\n    Args:\n        text1: \u7b2c\u4e00\u4e2a\u6587\u672c\n        text2: \u7b2c\u4e8c\u4e2a\u6587\u672c\n\n    Returns:\n        \u4f59\u5f26\u76f8\u4f3c\u5ea6\u503c [-1, 1]\n    \"\"\"\n    embedding1 = self.encode_single_text(text1)\n    embedding2 = self.encode_single_text(text2)\n\n    return np.dot(embedding1, embedding2) / (\n        np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n    )\n</code></pre>"},{"location":"zh/api/processors/#find_similar_textsquery_text-candidate_texts-top_k","title":"find_similar_texts(query_text, candidate_texts, top_k)","text":"<p>\u67e5\u627e\u76f8\u4f3c\u6587\u672c\u3002</p> <pre><code>def find_similar_texts(\n    self,\n    query_text: str,\n    candidate_texts: List[str],\n    top_k: int = 5\n) -&gt; List[Tuple[int, str, float]]:\n    \"\"\"\n    \u67e5\u627e\u4e0e\u67e5\u8be2\u6587\u672c\u6700\u76f8\u4f3c\u7684\u5019\u9009\u6587\u672c\n\n    Args:\n        query_text: \u67e5\u8be2\u6587\u672c\n        candidate_texts: \u5019\u9009\u6587\u672c\u5217\u8868\n        top_k: \u8fd4\u56de\u524d k \u4e2a\u6700\u76f8\u4f3c\u7684\n\n    Returns:\n        (\u7d22\u5f15, \u6587\u672c, \u76f8\u4f3c\u5ea6) \u7684\u5217\u8868\uff0c\u6309\u76f8\u4f3c\u5ea6\u964d\u5e8f\u6392\u5217\n    \"\"\"\n    query_embedding = self.encode_single_text(query_text)\n    candidate_embeddings = self.encode_texts(candidate_texts)\n\n    # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\n    similarities = np.dot(candidate_embeddings, query_embedding)\n\n    # \u83b7\u53d6 top-k\n    top_indices = np.argsort(similarities)[-top_k:][::-1]\n\n    results = []\n    for idx in top_indices:\n        results.append((idx, candidate_texts[idx], similarities[idx]))\n\n    return results\n</code></pre>"},{"location":"zh/api/processors/#_2","title":"\u5e8f\u5217\u5904\u7406\u5668","text":""},{"location":"zh/api/processors/#sequenceprocessor","title":"SequenceProcessor","text":"<p>\u5e8f\u5217\u6570\u636e\u5904\u7406\u7684\u6838\u5fc3\u7c7b\u3002</p> <pre><code>class SequenceProcessor:\n    def __init__(self, config: SequenceConfig):\n        self.config = config\n\n    def build_user_sequences(\n        self, \n        interactions_df: pd.DataFrame\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        \u6784\u5efa\u7528\u6237\u4ea4\u4e92\u5e8f\u5217\n\n        Args:\n            interactions_df: \u4ea4\u4e92\u6570\u636e\u6846\uff0c\u5305\u542b user_id, item_id, timestamp\n\n        Returns:\n            \u7528\u6237\u5e8f\u5217\u5217\u8868\uff0c\u6bcf\u4e2a\u5e8f\u5217\u5305\u542b\u7528\u6237ID\u548c\u7269\u54c1\u5e8f\u5217\n        \"\"\"\n        sequences = []\n\n        # \u6309\u7528\u6237\u5206\u7ec4\u5e76\u6309\u65f6\u95f4\u6392\u5e8f\n        for user_id, group in interactions_df.groupby('user_id'):\n            user_interactions = group.sort_values('timestamp')\n            item_sequence = user_interactions['item_id'].tolist()\n\n            # \u8fc7\u6ee4\u8fc7\u77ed\u7684\u5e8f\u5217\n            if len(item_sequence) &gt;= self.config.min_seq_length:\n                sequences.append({\n                    'user_id': user_id,\n                    'item_sequence': item_sequence,\n                    'timestamps': user_interactions['timestamp'].tolist() if self.config.include_timestamps else None\n                })\n\n        return sequences\n</code></pre>"},{"location":"zh/api/processors/#create_training_samplessequences","title":"create_training_samples(sequences)","text":"<p>\u521b\u5efa\u8bad\u7ec3\u6837\u672c\u3002</p> <pre><code>def create_training_samples(\n    self,\n    sequences: List[Dict[str, Any]]\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    \u4ece\u7528\u6237\u5e8f\u5217\u521b\u5efa\u8bad\u7ec3\u6837\u672c\n\n    Args:\n        sequences: \u7528\u6237\u5e8f\u5217\u5217\u8868\n\n    Returns:\n        \u8bad\u7ec3\u6837\u672c\u5217\u8868\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u8f93\u5165\u5e8f\u5217\u548c\u76ee\u6807\u5e8f\u5217\n    \"\"\"\n    training_samples = []\n\n    for seq_data in sequences:\n        item_sequence = seq_data['item_sequence']\n\n        # \u521b\u5efa\u591a\u4e2a\u5b50\u5e8f\u5217\n        for i in range(0, len(item_sequence) - self.config.min_seq_length + 1, self.config.sequence_stride):\n            # \u786e\u5b9a\u5b50\u5e8f\u5217\u957f\u5ea6\n            end_idx = min(i + self.config.max_seq_length, len(item_sequence))\n\n            if end_idx - i &gt;= self.config.min_seq_length:\n                input_seq = item_sequence[i:end_idx-self.config.target_offset]\n                target_seq = item_sequence[i+self.config.target_offset:end_idx]\n\n                if len(input_seq) &gt; 0 and len(target_seq) &gt; 0:\n                    sample = {\n                        'user_id': seq_data['user_id'],\n                        'input_sequence': input_seq,\n                        'target_sequence': target_seq\n                    }\n\n                    # \u6dfb\u52a0\u65f6\u95f4\u6233\u4fe1\u606f\n                    if self.config.include_timestamps and seq_data['timestamps']:\n                        sample['input_timestamps'] = seq_data['timestamps'][i:end_idx-self.config.target_offset]\n                        sample['target_timestamps'] = seq_data['timestamps'][i+self.config.target_offset:end_idx]\n\n                    training_samples.append(sample)\n\n    return training_samples\n</code></pre>"},{"location":"zh/api/processors/#pad_and_truncate_sequencesequence","title":"pad_and_truncate_sequence(sequence)","text":"<p>\u586b\u5145\u548c\u622a\u65ad\u5e8f\u5217\u3002</p> <pre><code>def pad_and_truncate_sequence(self, sequence: List[int]) -&gt; List[int]:\n    \"\"\"\n    \u586b\u5145\u548c\u622a\u65ad\u5e8f\u5217\u5230\u6307\u5b9a\u957f\u5ea6\n\n    Args:\n        sequence: \u8f93\u5165\u5e8f\u5217\n\n    Returns:\n        \u5904\u7406\u540e\u7684\u5e8f\u5217\n    \"\"\"\n    # \u622a\u65ad\n    if len(sequence) &gt; self.config.max_seq_length:\n        sequence = self.config.truncate_sequence(sequence)\n\n    # \u586b\u5145\n    if len(sequence) &lt; self.config.max_seq_length:\n        sequence = self.config.pad_sequence(sequence)\n\n    return sequence\n</code></pre>"},{"location":"zh/api/processors/#create_attention_masksequence","title":"create_attention_mask(sequence)","text":"<p>\u521b\u5efa\u6ce8\u610f\u529b\u63a9\u7801\u3002</p> <pre><code>def create_attention_mask(self, sequence: List[int]) -&gt; List[int]:\n    \"\"\"\n    \u4e3a\u5e8f\u5217\u521b\u5efa\u6ce8\u610f\u529b\u63a9\u7801\n\n    Args:\n        sequence: \u8f93\u5165\u5e8f\u5217\n\n    Returns:\n        \u6ce8\u610f\u529b\u63a9\u7801\uff0c1 \u8868\u793a\u6709\u6548\u4f4d\u7f6e\uff0c0 \u8868\u793a\u586b\u5145\u4f4d\u7f6e\n    \"\"\"\n    mask = []\n    for token in sequence:\n        if token == self.config.padding_token:\n            mask.append(0)\n        else:\n            mask.append(1)\n\n    return mask\n</code></pre>"},{"location":"zh/api/processors/#encode_time_featurestimestamps","title":"encode_time_features(timestamps)","text":"<p>\u7f16\u7801\u65f6\u95f4\u7279\u5f81\u3002</p> <pre><code>def encode_time_features(self, timestamps: List[float]) -&gt; np.ndarray:\n    \"\"\"\n    \u5c06\u65f6\u95f4\u6233\u7f16\u7801\u4e3a\u7279\u5f81\u5411\u91cf\n\n    Args:\n        timestamps: \u65f6\u95f4\u6233\u5217\u8868\n\n    Returns:\n        \u65f6\u95f4\u7279\u5f81\u77e9\u9635 (seq_len, time_encoding_dim)\n    \"\"\"\n    if not timestamps:\n        return np.zeros((0, self.config.time_encoding_dim))\n\n    # \u6807\u51c6\u5316\u65f6\u95f4\u6233\n    timestamps = np.array(timestamps)\n    min_time, max_time = timestamps.min(), timestamps.max()\n\n    if max_time &gt; min_time:\n        normalized_times = (timestamps - min_time) / (max_time - min_time)\n    else:\n        normalized_times = np.zeros_like(timestamps)\n\n    # \u521b\u5efa\u6b63\u5f26\u548c\u4f59\u5f26\u7f16\u7801\n    time_features = []\n    for i in range(self.config.time_encoding_dim // 2):\n        freq = 1.0 / (10000 ** (2 * i / self.config.time_encoding_dim))\n        sin_features = np.sin(normalized_times * freq)\n        cos_features = np.cos(normalized_times * freq)\n        time_features.extend([sin_features, cos_features])\n\n    # \u8f6c\u7f6e\u5e76\u622a\u65ad\u5230\u6307\u5b9a\u7ef4\u5ea6\n    time_features = np.array(time_features[:self.config.time_encoding_dim]).T\n\n    return time_features\n</code></pre>"},{"location":"zh/api/processors/#_3","title":"\u6570\u636e\u589e\u5f3a\u5904\u7406\u5668","text":""},{"location":"zh/api/processors/#dataaugmentor","title":"DataAugmentor","text":"<p>\u6570\u636e\u589e\u5f3a\u5904\u7406\u5668\u3002</p> <pre><code>class DataAugmentor:\n    def __init__(self, augmentation_config: Dict[str, Any]):\n        self.config = augmentation_config\n\n    def augment_sequence(self, sequence: List[int]) -&gt; List[int]:\n        \"\"\"\n        \u5bf9\u5e8f\u5217\u8fdb\u884c\u6570\u636e\u589e\u5f3a\n\n        Args:\n            sequence: \u539f\u59cb\u5e8f\u5217\n\n        Returns:\n            \u589e\u5f3a\u540e\u7684\u5e8f\u5217\n        \"\"\"\n        augmented = sequence.copy()\n\n        # \u968f\u673a\u5220\u9664\n        if self.config.get('random_drop', False):\n            drop_prob = self.config.get('drop_prob', 0.1)\n            augmented = [item for item in augmented if random.random() &gt; drop_prob]\n\n        # \u968f\u673a\u6253\u4e71\n        if self.config.get('random_shuffle', False):\n            shuffle_prob = self.config.get('shuffle_prob', 0.1)\n            if random.random() &lt; shuffle_prob:\n                # \u53ea\u6253\u4e71\u90e8\u5206\u5b50\u5e8f\u5217\n                start = random.randint(0, max(0, len(augmented) - 3))\n                end = min(start + random.randint(2, 4), len(augmented))\n                subseq = augmented[start:end]\n                random.shuffle(subseq)\n                augmented[start:end] = subseq\n\n        # \u968f\u673a\u66ff\u6362\n        if self.config.get('random_replace', False):\n            replace_prob = self.config.get('replace_prob', 0.05)\n            vocab_size = self.config.get('vocab_size', 1000)\n\n            for i in range(len(augmented)):\n                if random.random() &lt; replace_prob:\n                    augmented[i] = random.randint(1, vocab_size)\n\n        return augmented\n</code></pre>"},{"location":"zh/api/processors/#_4","title":"\u9884\u5904\u7406\u7ba1\u9053","text":""},{"location":"zh/api/processors/#preprocessingpipeline","title":"PreprocessingPipeline","text":"<p>\u6570\u636e\u9884\u5904\u7406\u7ba1\u9053\u3002</p> <pre><code>class PreprocessingPipeline:\n    def __init__(\n        self,\n        text_processor: TextProcessor,\n        sequence_processor: SequenceProcessor,\n        augmentor: Optional[DataAugmentor] = None\n    ):\n        self.text_processor = text_processor\n        self.sequence_processor = sequence_processor\n        self.augmentor = augmentor\n\n    def process_items(\n        self,\n        items_df: pd.DataFrame,\n        cache_key: str = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        \u5904\u7406\u7269\u54c1\u6570\u636e\n\n        Args:\n            items_df: \u7269\u54c1\u6570\u636e\u6846\n            cache_key: \u7f13\u5b58\u952e\n\n        Returns:\n            \u5904\u7406\u540e\u7684\u7269\u54c1\u6570\u636e\u6846\uff0c\u5305\u542b\u7279\u5f81\u5411\u91cf\n        \"\"\"\n        print(\"Processing item features...\")\n\n        # \u7f16\u7801\u6587\u672c\u7279\u5f81\n        embeddings = self.text_processor.encode_item_features(\n            items_df, cache_key=cache_key\n        )\n\n        # \u6dfb\u52a0\u7279\u5f81\u5230\u6570\u636e\u6846\n        processed_df = items_df.copy()\n        processed_df['features'] = embeddings.tolist()\n\n        return processed_df\n\n    def process_interactions(\n        self,\n        interactions_df: pd.DataFrame,\n        items_df: pd.DataFrame\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        \u5904\u7406\u4ea4\u4e92\u6570\u636e\u751f\u6210\u5e8f\u5217\n\n        Args:\n            interactions_df: \u4ea4\u4e92\u6570\u636e\u6846\n            items_df: \u7269\u54c1\u6570\u636e\u6846\n\n        Returns:\n            \u5904\u7406\u540e\u7684\u5e8f\u5217\u6570\u636e\n        \"\"\"\n        print(\"Building user sequences...\")\n\n        # \u6784\u5efa\u7528\u6237\u5e8f\u5217\n        sequences = self.sequence_processor.build_user_sequences(interactions_df)\n\n        # \u521b\u5efa\u8bad\u7ec3\u6837\u672c\n        training_samples = self.sequence_processor.create_training_samples(sequences)\n\n        # \u6570\u636e\u589e\u5f3a\n        if self.augmentor:\n            augmented_samples = []\n            for sample in training_samples:\n                # \u539f\u59cb\u6837\u672c\n                augmented_samples.append(sample)\n\n                # \u589e\u5f3a\u6837\u672c\n                aug_input = self.augmentor.augment_sequence(sample['input_sequence'])\n                aug_target = self.augmentor.augment_sequence(sample['target_sequence'])\n\n                augmented_sample = sample.copy()\n                augmented_sample['input_sequence'] = aug_input\n                augmented_sample['target_sequence'] = aug_target\n                augmented_samples.append(augmented_sample)\n\n            training_samples = augmented_samples\n\n        return training_samples\n</code></pre>"},{"location":"zh/api/processors/#_5","title":"\u5de5\u5177\u51fd\u6570","text":""},{"location":"zh/api/processors/#compute_sequence_statisticssequences","title":"compute_sequence_statistics(sequences)","text":"<p>\u8ba1\u7b97\u5e8f\u5217\u7edf\u8ba1\u4fe1\u606f\u3002</p> <pre><code>def compute_sequence_statistics(sequences: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"\n    \u8ba1\u7b97\u5e8f\u5217\u6570\u636e\u7684\u7edf\u8ba1\u4fe1\u606f\n\n    Args:\n        sequences: \u5e8f\u5217\u5217\u8868\n\n    Returns:\n        \u7edf\u8ba1\u4fe1\u606f\u5b57\u5178\n    \"\"\"\n    if not sequences:\n        return {}\n\n    lengths = [len(seq['item_sequence']) for seq in sequences]\n    unique_users = len(set(seq['user_id'] for seq in sequences))\n\n    # \u8ba1\u7b97\u7269\u54c1\u9891\u7387\n    item_counts = {}\n    for seq in sequences:\n        for item_id in seq['item_sequence']:\n            item_counts[item_id] = item_counts.get(item_id, 0) + 1\n\n    stats = {\n        'num_sequences': len(sequences),\n        'num_unique_users': unique_users,\n        'num_unique_items': len(item_counts),\n        'avg_sequence_length': np.mean(lengths),\n        'min_sequence_length': np.min(lengths),\n        'max_sequence_length': np.max(lengths),\n        'median_sequence_length': np.median(lengths),\n        'total_interactions': sum(lengths),\n        'most_popular_items': sorted(item_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n    }\n\n    return stats\n</code></pre>"},{"location":"zh/api/processors/#visualize_embeddingsembeddings-labels-method","title":"visualize_embeddings(embeddings, labels, method)","text":"<p>\u53ef\u89c6\u5316\u5d4c\u5165\u5411\u91cf\u3002</p> <pre><code>def visualize_embeddings(\n    embeddings: np.ndarray,\n    labels: List[str] = None,\n    method: str = 'tsne',\n    save_path: str = None\n) -&gt; None:\n    \"\"\"\n    \u53ef\u89c6\u5316\u9ad8\u7ef4\u5d4c\u5165\u5411\u91cf\n\n    Args:\n        embeddings: \u5d4c\u5165\u77e9\u9635 (n_samples, embedding_dim)\n        labels: \u6837\u672c\u6807\u7b7e\n        method: \u964d\u7ef4\u65b9\u6cd5 ('tsne', 'pca', 'umap')\n        save_path: \u4fdd\u5b58\u8def\u5f84\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    # \u964d\u7ef4\n    if method == 'tsne':\n        from sklearn.manifold import TSNE\n        reducer = TSNE(n_components=2, random_state=42)\n    elif method == 'pca':\n        from sklearn.decomposition import PCA\n        reducer = PCA(n_components=2)\n    elif method == 'umap':\n        import umap\n        reducer = umap.UMAP(n_components=2, random_state=42)\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    reduced_embeddings = reducer.fit_transform(embeddings)\n\n    # \u7ed8\u56fe\n    plt.figure(figsize=(10, 8))\n    if labels:\n        unique_labels = list(set(labels))\n        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n\n        for i, label in enumerate(unique_labels):\n            mask = np.array(labels) == label\n            plt.scatter(\n                reduced_embeddings[mask, 0],\n                reduced_embeddings[mask, 1],\n                c=[colors[i]],\n                label=label,\n                alpha=0.7\n            )\n        plt.legend()\n    else:\n        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)\n\n    plt.title(f'Embedding Visualization ({method.upper()})')\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n\n    plt.show()\n</code></pre>"},{"location":"zh/api/processors/#_6","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/api/processors/#_7","title":"\u6587\u672c\u5904\u7406","text":"<pre><code>from generative_recommenders.data.processors import TextProcessor\nfrom generative_recommenders.data.configs import TextEncodingConfig\n\n# \u521b\u5efa\u914d\u7f6e\nconfig = TextEncodingConfig(\n    encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    template=\"Title: {title}; Category: {category}\",\n    batch_size=32\n)\n\n# \u521b\u5efa\u5904\u7406\u5668\nprocessor = TextProcessor(config)\n\n# \u7f16\u7801\u6587\u672c\ntexts = [\"Apple iPhone 13\", \"Samsung Galaxy S21\", \"Sony WH-1000XM4\"]\nembeddings = processor.encode_texts(texts, cache_key=\"sample_texts\")\n\nprint(f\"Embeddings shape: {embeddings.shape}\")\n</code></pre>"},{"location":"zh/api/processors/#_8","title":"\u5e8f\u5217\u5904\u7406","text":"<pre><code>from generative_recommenders.data.processors import SequenceProcessor\nfrom generative_recommenders.data.configs import SequenceConfig\n\n# \u521b\u5efa\u914d\u7f6e\nconfig = SequenceConfig(\n    max_seq_length=50,\n    min_seq_length=3,\n    target_offset=1\n)\n\n# \u521b\u5efa\u5904\u7406\u5668\nprocessor = SequenceProcessor(config)\n\n# \u5904\u7406\u4ea4\u4e92\u6570\u636e\nsequences = processor.build_user_sequences(interactions_df)\ntraining_samples = processor.create_training_samples(sequences)\n\nprint(f\"Generated {len(training_samples)} training samples\")\n</code></pre>"},{"location":"zh/api/processors/#_9","title":"\u5b8c\u6574\u9884\u5904\u7406\u7ba1\u9053","text":"<pre><code>from generative_recommenders.data.processors import PreprocessingPipeline\n\n# \u521b\u5efa\u7ba1\u9053\npipeline = PreprocessingPipeline(\n    text_processor=text_processor,\n    sequence_processor=sequence_processor\n)\n\n# \u5904\u7406\u6570\u636e\nprocessed_items = pipeline.process_items(items_df, cache_key=\"items_beauty\")\nprocessed_sequences = pipeline.process_interactions(interactions_df, processed_items)\n\n# \u67e5\u770b\u7edf\u8ba1\u4fe1\u606f\nstats = compute_sequence_statistics(processed_sequences)\nprint(f\"Dataset statistics: {stats}\")\n</code></pre>"},{"location":"zh/api/rqvae/","title":"RQVAE API \u53c2\u8003","text":"<p>\u6b8b\u5dee\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668 (RQVAE) \u7684\u8be6\u7ec6 API \u6587\u6863\u3002</p>"},{"location":"zh/api/rqvae/#_1","title":"\u6838\u5fc3\u7c7b","text":""},{"location":"zh/api/rqvae/#rqvae","title":"RqVae","text":"<p>\u4e3b\u8981\u7684 RQVAE \u6a21\u578b\u7c7b\u3002</p> <pre><code>class RqVae(LightningModule):\n    def __init__(\n        self,\n        input_dim: int = 768,\n        hidden_dim: int = 512,\n        latent_dim: int = 256,\n        num_embeddings: int = 1024,\n        commitment_cost: float = 0.25,\n        learning_rate: float = 1e-3\n    )\n</code></pre> <p>\u53c2\u6570: - <code>input_dim</code>: \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 - <code>hidden_dim</code>: \u9690\u85cf\u5c42\u7ef4\u5ea6 - <code>latent_dim</code>: \u6f5c\u5728\u7a7a\u95f4\u7ef4\u5ea6 - <code>num_embeddings</code>: \u5d4c\u5165\u5411\u91cf\u6570\u91cf - <code>commitment_cost</code>: \u627f\u8bfa\u635f\u5931\u6743\u91cd - <code>learning_rate</code>: \u5b66\u4e60\u7387</p> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/rqvae/#forwardfeatures","title":"forward(features)","text":"<p>\u524d\u5411\u4f20\u64ad\u8ba1\u7b97\u3002</p> <pre><code>def forward(self, features: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Args:\n        features: \u8f93\u5165\u7279\u5f81 (batch_size, input_dim)\n\n    Returns:\n        reconstructed: \u91cd\u6784\u7279\u5f81 (batch_size, input_dim)\n        commitment_loss: \u627f\u8bfa\u635f\u5931\n        embedding_loss: \u5d4c\u5165\u635f\u5931\n        semantic_ids: \u8bed\u4e49ID (batch_size,)\n    \"\"\"\n</code></pre>"},{"location":"zh/api/rqvae/#encodefeatures","title":"encode(features)","text":"<p>\u7f16\u7801\u7279\u5f81\u4e3a\u6f5c\u5728\u8868\u793a\u3002</p> <pre><code>def encode(self, features: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        features: \u8f93\u5165\u7279\u5f81 (batch_size, input_dim)\n\n    Returns:\n        encoded: \u7f16\u7801\u540e\u7684\u6f5c\u5728\u8868\u793a (batch_size, latent_dim)\n    \"\"\"\n</code></pre>"},{"location":"zh/api/rqvae/#generate_semantic_idsfeatures","title":"generate_semantic_ids(features)","text":"<p>\u751f\u6210\u8bed\u4e49ID\u3002</p> <pre><code>def generate_semantic_ids(self, features: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        features: \u8f93\u5165\u7279\u5f81 (batch_size, input_dim)\n\n    Returns:\n        semantic_ids: \u8bed\u4e49ID (batch_size,)\n    \"\"\"\n</code></pre>"},{"location":"zh/api/rqvae/#_2","title":"\u7ec4\u4ef6\u7c7b","text":""},{"location":"zh/api/rqvae/#vectorquantizer","title":"VectorQuantizer","text":"<p>\u5411\u91cf\u91cf\u5316\u5c42\u5b9e\u73b0\u3002</p> <pre><code>class VectorQuantizer(nn.Module):\n    def __init__(\n        self,\n        num_embeddings: int,\n        embedding_dim: int,\n        commitment_cost: float = 0.25\n    )\n</code></pre> <p>\u53c2\u6570: - <code>num_embeddings</code>: \u5d4c\u5165\u5411\u91cf\u6570\u91cf - <code>embedding_dim</code>: \u5d4c\u5165\u7ef4\u5ea6 - <code>commitment_cost</code>: \u627f\u8bfa\u635f\u5931\u6743\u91cd</p> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/rqvae/#forwardinputs","title":"forward(inputs)","text":"<p>\u91cf\u5316\u8f93\u5165\u5411\u91cf\u3002</p> <pre><code>def forward(self, inputs: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Args:\n        inputs: \u8f93\u5165\u5411\u91cf (batch_size, embedding_dim)\n\n    Returns:\n        quantized: \u91cf\u5316\u540e\u7684\u5411\u91cf\n        commitment_loss: \u627f\u8bfa\u635f\u5931\n        embedding_loss: \u5d4c\u5165\u635f\u5931\n        encoding_indices: \u7f16\u7801\u7d22\u5f15\n    \"\"\"\n</code></pre>"},{"location":"zh/api/rqvae/#encoder","title":"Encoder","text":"<p>\u7f16\u7801\u5668\u7f51\u7edc\u3002</p> <pre><code>class Encoder(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        latent_dim: int\n    )\n</code></pre>"},{"location":"zh/api/rqvae/#decoder","title":"Decoder","text":"<p>\u89e3\u7801\u5668\u7f51\u7edc\u3002</p> <pre><code>class Decoder(nn.Module):\n    def __init__(\n        self,\n        latent_dim: int,\n        hidden_dim: int,\n        output_dim: int\n    )\n</code></pre>"},{"location":"zh/api/rqvae/#_3","title":"\u8bad\u7ec3\u63a5\u53e3","text":""},{"location":"zh/api/rqvae/#_4","title":"\u8bad\u7ec3\u6b65\u9aa4","text":"<pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"\u8bad\u7ec3\u6b65\u9aa4\"\"\"\n    features = batch['features']\n\n    # \u524d\u5411\u4f20\u64ad\n    reconstructed, commitment_loss, embedding_loss, semantic_ids = self(features)\n\n    # \u8ba1\u7b97\u635f\u5931\n    recon_loss = F.mse_loss(reconstructed, features)\n    total_loss = recon_loss + commitment_loss + embedding_loss\n\n    # \u8bb0\u5f55\u6307\u6807\n    self.log('train_loss', total_loss)\n    self.log('train_recon_loss', recon_loss)\n    self.log('train_commitment_loss', commitment_loss)\n    self.log('train_embedding_loss', embedding_loss)\n\n    return total_loss\n</code></pre>"},{"location":"zh/api/rqvae/#_5","title":"\u9a8c\u8bc1\u6b65\u9aa4","text":"<pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"\u9a8c\u8bc1\u6b65\u9aa4\"\"\"\n    features = batch['features']\n\n    # \u524d\u5411\u4f20\u64ad\n    reconstructed, commitment_loss, embedding_loss, semantic_ids = self(features)\n\n    # \u8ba1\u7b97\u635f\u5931\n    recon_loss = F.mse_loss(reconstructed, features)\n    total_loss = recon_loss + commitment_loss + embedding_loss\n\n    # \u8bb0\u5f55\u6307\u6807\n    self.log('val_loss', total_loss)\n    self.log('val_recon_loss', recon_loss)\n\n    return total_loss\n</code></pre>"},{"location":"zh/api/rqvae/#_6","title":"\u914d\u7f6e\u63a5\u53e3","text":""},{"location":"zh/api/rqvae/#_7","title":"\u4f18\u5316\u5668\u914d\u7f6e","text":"<pre><code>def configure_optimizers(self):\n    \"\"\"\u914d\u7f6e\u4f18\u5316\u5668\"\"\"\n    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=5\n    )\n\n    return {\n        'optimizer': optimizer,\n        'lr_scheduler': {\n            'scheduler': scheduler,\n            'monitor': 'val_loss'\n        }\n    }\n</code></pre>"},{"location":"zh/api/rqvae/#_8","title":"\u5de5\u5177\u51fd\u6570","text":""},{"location":"zh/api/rqvae/#_9","title":"\u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d","text":"<pre><code># \u4fdd\u5b58\u6a21\u578b\nmodel.save_pretrained(\"path/to/model\")\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = RqVae.load_from_checkpoint(\"path/to/checkpoint.ckpt\")\n</code></pre>"},{"location":"zh/api/rqvae/#_10","title":"\u6279\u91cf\u63a8\u7406","text":"<pre><code>def batch_inference(model, dataloader, device='cuda'):\n    \"\"\"\u6279\u91cf\u63a8\u7406\u751f\u6210\u8bed\u4e49ID\"\"\"\n    model.eval()\n    model.to(device)\n\n    all_semantic_ids = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            features = batch['features'].to(device)\n            semantic_ids = model.generate_semantic_ids(features)\n            all_semantic_ids.append(semantic_ids.cpu())\n\n    return torch.cat(all_semantic_ids, dim=0)\n</code></pre>"},{"location":"zh/api/rqvae/#_11","title":"\u8bc4\u4f30\u63a5\u53e3","text":""},{"location":"zh/api/rqvae/#_12","title":"\u91cd\u6784\u8d28\u91cf\u8bc4\u4f30","text":"<pre><code>def evaluate_reconstruction(model, dataloader, device='cuda'):\n    \"\"\"\u8bc4\u4f30\u91cd\u6784\u8d28\u91cf\"\"\"\n    model.eval()\n    model.to(device)\n\n    total_mse = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            features = batch['features'].to(device)\n            reconstructed, _, _, _ = model(features)\n\n            mse = F.mse_loss(reconstructed, features, reduction='sum')\n            total_mse += mse.item()\n            total_samples += features.size(0)\n\n    avg_mse = total_mse / total_samples\n    return {'mse': avg_mse, 'rmse': avg_mse ** 0.5}\n</code></pre>"},{"location":"zh/api/rqvae/#_13","title":"\u91cf\u5316\u8d28\u91cf\u8bc4\u4f30","text":"<pre><code>def evaluate_quantization(model, dataloader, device='cuda'):\n    \"\"\"\u8bc4\u4f30\u91cf\u5316\u8d28\u91cf\"\"\"\n    model.eval()\n    model.to(device)\n\n    all_indices = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            features = batch['features'].to(device)\n            _, _, _, semantic_ids = model(features)\n            all_indices.append(semantic_ids.cpu())\n\n    all_indices = torch.cat(all_indices, dim=0)\n\n    # \u8ba1\u7b97\u4f7f\u7528\u7edf\u8ba1\n    unique_codes = len(torch.unique(all_indices))\n    total_codes = model.quantizer.num_embeddings\n    usage_rate = unique_codes / total_codes\n\n    # \u8ba1\u7b97\u56f0\u60d1\u5ea6\n    counts = torch.bincount(all_indices, minlength=total_codes).float()\n    probs = counts / counts.sum()\n    perplexity = torch.exp(-torch.sum(probs * torch.log(probs + 1e-10)))\n\n    return {\n        'usage_rate': usage_rate,\n        'unique_codes': unique_codes,\n        'perplexity': perplexity.item()\n    }\n</code></pre>"},{"location":"zh/api/rqvae/#_14","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/api/rqvae/#_15","title":"\u57fa\u672c\u8bad\u7ec3","text":"<pre><code>from generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\nimport pytorch_lightning as pl\n\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# \u521b\u5efa\u6a21\u578b\nmodel = RqVae(\n    input_dim=768,\n    hidden_dim=512,\n    latent_dim=256,\n    num_embeddings=1024,\n    learning_rate=1e-3\n)\n\n# \u8bad\u7ec3\u6a21\u578b\ntrainer = pl.Trainer(max_epochs=100, gpus=1)\ntrainer.fit(model, dataloader)\n</code></pre>"},{"location":"zh/api/rqvae/#id","title":"\u8bed\u4e49ID\u751f\u6210","text":"<pre><code># \u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\nmodel = RqVae.load_from_checkpoint(\"checkpoints/rqvae.ckpt\")\nmodel.eval()\n\n# \u751f\u6210\u8bed\u4e49ID\nwith torch.no_grad():\n    features = torch.randn(10, 768)  # \u793a\u4f8b\u7279\u5f81\n    semantic_ids = model.generate_semantic_ids(features)\n    print(f\"Semantic IDs: {semantic_ids}\")\n</code></pre>"},{"location":"zh/api/tiger/","title":"TIGER API \u53c2\u8003","text":"<p>\u57fa\u4e8e Transformer \u7684\u751f\u6210\u5f0f\u68c0\u7d22\u6a21\u578b (TIGER) \u7684\u8be6\u7ec6 API \u6587\u6863\u3002</p>"},{"location":"zh/api/tiger/#_1","title":"\u6838\u5fc3\u7c7b","text":""},{"location":"zh/api/tiger/#tiger","title":"Tiger","text":"<p>\u4e3b\u8981\u7684 TIGER \u6a21\u578b\u7c7b\u3002</p> <pre><code>class Tiger(LightningModule):\n    def __init__(\n        self,\n        vocab_size: int,\n        embedding_dim: int = 512,\n        num_heads: int = 8,\n        num_layers: int = 6,\n        attn_dim: int = 2048,\n        dropout: float = 0.1,\n        max_seq_length: int = 1024,\n        learning_rate: float = 1e-4\n    )\n</code></pre> <p>\u53c2\u6570: - <code>vocab_size</code>: \u8bcd\u6c47\u8868\u5927\u5c0f - <code>embedding_dim</code>: \u5d4c\u5165\u7ef4\u5ea6 - <code>num_heads</code>: \u6ce8\u610f\u529b\u5934\u6570 - <code>num_layers</code>: Transformer \u5c42\u6570 - <code>attn_dim</code>: \u6ce8\u610f\u529b\u7ef4\u5ea6 - <code>dropout</code>: Dropout \u6982\u7387 - <code>max_seq_length</code>: \u6700\u5927\u5e8f\u5217\u957f\u5ea6 - <code>learning_rate</code>: \u5b66\u4e60\u7387</p> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/tiger/#forwardinput_ids-attention_masknone","title":"forward(input_ids, attention_mask=None)","text":"<p>\u524d\u5411\u4f20\u64ad\u8ba1\u7b97\u3002</p> <pre><code>def forward(\n    self, \n    input_ids: torch.Tensor, \n    attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        input_ids: \u8f93\u5165\u5e8f\u5217 (batch_size, seq_len)\n        attention_mask: \u6ce8\u610f\u529b\u63a9\u7801 (batch_size, seq_len)\n\n    Returns:\n        logits: \u8f93\u51fa logits (batch_size, seq_len, vocab_size)\n    \"\"\"\n</code></pre>"},{"location":"zh/api/tiger/#generateinput_ids-max_length50-temperature10-top_knone-top_pnone","title":"generate(input_ids, max_length=50, temperature=1.0, top_k=None, top_p=None)","text":"<p>\u751f\u6210\u63a8\u8350\u5e8f\u5217\u3002</p> <pre><code>def generate(\n    self,\n    input_ids: torch.Tensor,\n    max_length: int = 50,\n    temperature: float = 1.0,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        input_ids: \u8f93\u5165\u5e8f\u5217\n        max_length: \u6700\u5927\u751f\u6210\u957f\u5ea6\n        temperature: \u6e29\u5ea6\u53c2\u6570\n        top_k: Top-k \u91c7\u6837\n        top_p: Top-p \u91c7\u6837\n\n    Returns:\n        generated: \u751f\u6210\u7684\u5e8f\u5217\n    \"\"\"\n</code></pre>"},{"location":"zh/api/tiger/#generate_with_trieinput_ids-trie-max_length50","title":"generate_with_trie(input_ids, trie, max_length=50)","text":"<p>\u4f7f\u7528 Trie \u7ea6\u675f\u751f\u6210\u3002</p> <pre><code>def generate_with_trie(\n    self,\n    input_ids: torch.Tensor,\n    trie: TrieNode,\n    max_length: int = 50\n) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        input_ids: \u8f93\u5165\u5e8f\u5217\n        trie: Trie \u7ea6\u675f\u7ed3\u6784\n        max_length: \u6700\u5927\u751f\u6210\u957f\u5ea6\n\n    Returns:\n        generated: \u7ea6\u675f\u751f\u6210\u7684\u5e8f\u5217\n    \"\"\"\n</code></pre>"},{"location":"zh/api/tiger/#_2","title":"\u7ec4\u4ef6\u7c7b","text":""},{"location":"zh/api/tiger/#transformerblock","title":"TransformerBlock","text":"<p>Transformer \u5757\u5b9e\u73b0\u3002</p> <pre><code>class TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_heads: int,\n        attn_dim: int,\n        dropout: float = 0.1\n    )\n</code></pre>"},{"location":"zh/api/tiger/#multiheadattention","title":"MultiHeadAttention","text":"<p>\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3002</p> <pre><code>class MultiHeadAttention(nn.Module):\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_heads: int,\n        dropout: float = 0.1\n    )\n</code></pre>"},{"location":"zh/api/tiger/#positionalencoding","title":"PositionalEncoding","text":"<p>\u4f4d\u7f6e\u7f16\u7801\u3002</p> <pre><code>class PositionalEncoding(nn.Module):\n    def __init__(\n        self,\n        embedding_dim: int,\n        max_seq_length: int = 5000\n    )\n</code></pre>"},{"location":"zh/api/tiger/#_3","title":"\u6570\u636e\u7ed3\u6784","text":""},{"location":"zh/api/tiger/#trienode","title":"TrieNode","text":"<p>Trie \u8282\u70b9\u7528\u4e8e\u7ea6\u675f\u751f\u6210\u3002</p> <pre><code>class TrieNode(defaultdict):\n    def __init__(self):\n        super().__init__(TrieNode)\n        self.is_end = False\n\n    def add_sequence(self, sequence: List[int]):\n        \"\"\"\u6dfb\u52a0\u5e8f\u5217\u5230 Trie\"\"\"\n        node = self\n        for token in sequence:\n            node = node[token]\n        node.is_end = True\n\n    def get_valid_tokens(self) -&gt; List[int]:\n        \"\"\"\u83b7\u53d6\u5f53\u524d\u8282\u70b9\u7684\u6709\u6548 token\"\"\"\n        return list(self.keys())\n</code></pre>"},{"location":"zh/api/tiger/#trie","title":"\u6784\u5efa Trie","text":"<pre><code>def build_trie(valid_sequences: List[List[int]]) -&gt; TrieNode:\n    \"\"\"\u6784\u5efa\u6709\u6548\u5e8f\u5217\u7684 Trie\"\"\"\n    root = TrieNode()\n    for sequence in valid_sequences:\n        root.add_sequence(sequence)\n    return root\n</code></pre>"},{"location":"zh/api/tiger/#_4","title":"\u8bad\u7ec3\u63a5\u53e3","text":""},{"location":"zh/api/tiger/#_5","title":"\u8bad\u7ec3\u6b65\u9aa4","text":"<pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"\u8bad\u7ec3\u6b65\u9aa4\"\"\"\n    input_ids = batch['input_ids']\n    labels = batch['labels']\n    attention_mask = batch.get('attention_mask', None)\n\n    # \u524d\u5411\u4f20\u64ad\n    logits = self(input_ids, attention_mask)\n\n    # \u8ba1\u7b97\u635f\u5931\n    shift_logits = logits[..., :-1, :].contiguous()\n    shift_labels = labels[..., 1:].contiguous()\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fn(\n        shift_logits.view(-1, shift_logits.size(-1)),\n        shift_labels.view(-1)\n    )\n\n    # \u8bb0\u5f55\u6307\u6807\n    self.log('train_loss', loss)\n\n    return loss\n</code></pre>"},{"location":"zh/api/tiger/#_6","title":"\u9a8c\u8bc1\u6b65\u9aa4","text":"<pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"\u9a8c\u8bc1\u6b65\u9aa4\"\"\"\n    input_ids = batch['input_ids']\n    labels = batch['labels']\n    attention_mask = batch.get('attention_mask', None)\n\n    # \u524d\u5411\u4f20\u64ad\n    logits = self(input_ids, attention_mask)\n\n    # \u8ba1\u7b97\u635f\u5931\n    shift_logits = logits[..., :-1, :].contiguous()\n    shift_labels = labels[..., 1:].contiguous()\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fn(\n        shift_logits.view(-1, shift_logits.size(-1)),\n        shift_labels.view(-1)\n    )\n\n    # \u8bb0\u5f55\u6307\u6807\n    self.log('val_loss', loss)\n\n    return loss\n</code></pre>"},{"location":"zh/api/tiger/#_7","title":"\u63a8\u7406\u63a5\u53e3","text":""},{"location":"zh/api/tiger/#_8","title":"\u6279\u91cf\u751f\u6210","text":"<pre><code>def batch_generate(\n    model: Tiger,\n    input_sequences: List[torch.Tensor],\n    max_length: int = 50,\n    device: str = 'cuda'\n) -&gt; List[torch.Tensor]:\n    \"\"\"\u6279\u91cf\u751f\u6210\u63a8\u8350\"\"\"\n    model.eval()\n    model.to(device)\n\n    results = []\n\n    with torch.no_grad():\n        for input_seq in input_sequences:\n            input_seq = input_seq.to(device)\n            generated = model.generate(input_seq, max_length=max_length)\n            results.append(generated.cpu())\n\n    return results\n</code></pre>"},{"location":"zh/api/tiger/#_9","title":"\u7ea6\u675f\u751f\u6210","text":"<pre><code>def constrained_generate(\n    model: Tiger,\n    input_ids: torch.Tensor,\n    valid_item_sequences: List[List[int]],\n    max_length: int = 50\n) -&gt; torch.Tensor:\n    \"\"\"\u7ea6\u675f\u751f\u6210\u63a8\u8350\"\"\"\n    # \u6784\u5efa Trie\n    trie = build_trie(valid_item_sequences)\n\n    # \u7ea6\u675f\u751f\u6210\n    return model.generate_with_trie(input_ids, trie, max_length)\n</code></pre>"},{"location":"zh/api/tiger/#_10","title":"\u8bc4\u4f30\u63a5\u53e3","text":""},{"location":"zh/api/tiger/#top-k","title":"Top-K \u63a8\u8350\u8bc4\u4f30","text":"<pre><code>def evaluate_recommendation(\n    model: Tiger,\n    test_dataloader: DataLoader,\n    k_values: List[int] = [5, 10, 20],\n    device: str = 'cuda'\n) -&gt; Dict[str, float]:\n    \"\"\"\u8bc4\u4f30\u63a8\u8350\u6027\u80fd\"\"\"\n    model.eval()\n    model.to(device)\n\n    all_predictions = []\n    all_targets = []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            targets = batch['targets']\n\n            # \u751f\u6210\u63a8\u8350\n            generated = model.generate(input_ids, max_length=50)\n\n            all_predictions.extend(generated.cpu().tolist())\n            all_targets.extend(targets.tolist())\n\n    # \u8ba1\u7b97\u6307\u6807\n    metrics = {}\n    for k in k_values:\n        recall_k = compute_recall_at_k(all_predictions, all_targets, k)\n        ndcg_k = compute_ndcg_at_k(all_predictions, all_targets, k)\n\n        metrics[f'recall@{k}'] = recall_k\n        metrics[f'ndcg@{k}'] = ndcg_k\n\n    return metrics\n</code></pre>"},{"location":"zh/api/tiger/#_11","title":"\u56f0\u60d1\u5ea6\u8bc4\u4f30","text":"<pre><code>def evaluate_perplexity(\n    model: Tiger,\n    test_dataloader: DataLoader,\n    device: str = 'cuda'\n) -&gt; float:\n    \"\"\"\u8bc4\u4f30\u56f0\u60d1\u5ea6\"\"\"\n    model.eval()\n    model.to(device)\n\n    total_loss = 0\n    total_tokens = 0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            attention_mask = batch.get('attention_mask', None)\n\n            logits = model(input_ids, attention_mask)\n\n            # \u8ba1\u7b97\u635f\u5931\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n\n            loss_fn = nn.CrossEntropyLoss(ignore_index=-100, reduction='sum')\n            loss = loss_fn(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1)\n            )\n\n            # \u7edf\u8ba1\u6709\u6548 token \u6570\u91cf\n            valid_tokens = (shift_labels != -100).sum()\n\n            total_loss += loss.item()\n            total_tokens += valid_tokens.item()\n\n    avg_loss = total_loss / total_tokens\n    perplexity = math.exp(avg_loss)\n\n    return perplexity\n</code></pre>"},{"location":"zh/api/tiger/#_12","title":"\u5de5\u5177\u51fd\u6570","text":""},{"location":"zh/api/tiger/#_13","title":"\u5e8f\u5217\u5904\u7406","text":"<pre><code>def pad_sequences(\n    sequences: List[torch.Tensor],\n    pad_token_id: int = 0,\n    max_length: Optional[int] = None\n) -&gt; torch.Tensor:\n    \"\"\"\u586b\u5145\u5e8f\u5217\u5230\u76f8\u540c\u957f\u5ea6\"\"\"\n    if max_length is None:\n        max_length = max(len(seq) for seq in sequences)\n\n    padded = []\n    for seq in sequences:\n        if len(seq) &lt; max_length:\n            pad_length = max_length - len(seq)\n            padded_seq = torch.cat([\n                seq, \n                torch.full((pad_length,), pad_token_id, dtype=seq.dtype)\n            ])\n        else:\n            padded_seq = seq[:max_length]\n        padded.append(padded_seq)\n\n    return torch.stack(padded)\n</code></pre>"},{"location":"zh/api/tiger/#_14","title":"\u91c7\u6837\u7b56\u7565","text":"<pre><code>def top_k_top_p_sampling(\n    logits: torch.Tensor,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    temperature: float = 1.0\n) -&gt; torch.Tensor:\n    \"\"\"Top-k \u548c Top-p \u91c7\u6837\"\"\"\n    logits = logits / temperature\n\n    # Top-k \u91c7\u6837\n    if top_k is not None:\n        top_k = min(top_k, logits.size(-1))\n        values, indices = torch.topk(logits, top_k)\n        logits[logits &lt; values[..., [-1]]] = float('-inf')\n\n    # Top-p \u91c7\u6837\n    if top_p is not None:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # \u627e\u5230\u7d2f\u79ef\u6982\u7387\u8d85\u8fc7 top_p \u7684\u4f4d\u7f6e\n        sorted_indices_to_remove = cumulative_probs &gt; top_p\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = float('-inf')\n\n    # \u91c7\u6837\n    probs = F.softmax(logits, dim=-1)\n    next_token = torch.multinomial(probs, 1)\n\n    return next_token\n</code></pre>"},{"location":"zh/api/tiger/#_15","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/api/tiger/#_16","title":"\u57fa\u672c\u8bad\u7ec3","text":"<pre><code>from generative_recommenders.models.tiger import Tiger\nfrom generative_recommenders.data.p5_amazon import P5AmazonSequenceDataset\nimport pytorch_lightning as pl\n\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset = P5AmazonSequenceDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\",\n    pretrained_rqvae_path=\"checkpoints/rqvae.ckpt\"\n)\n\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# \u521b\u5efa\u6a21\u578b\nmodel = Tiger(\n    vocab_size=1024,\n    embedding_dim=512,\n    num_heads=8,\n    num_layers=6,\n    learning_rate=1e-4\n)\n\n# \u8bad\u7ec3\u6a21\u578b\ntrainer = pl.Trainer(max_epochs=50, gpus=1)\ntrainer.fit(model, dataloader)\n</code></pre>"},{"location":"zh/api/tiger/#_17","title":"\u63a8\u8350\u751f\u6210","text":"<pre><code># \u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\nmodel = Tiger.load_from_checkpoint(\"checkpoints/tiger.ckpt\")\nmodel.eval()\n\n# \u7528\u6237\u5386\u53f2\u5e8f\u5217\nuser_sequence = torch.tensor([10, 25, 67, 89])  # \u8bed\u4e49ID\u5e8f\u5217\n\n# \u751f\u6210\u63a8\u8350\nwith torch.no_grad():\n    recommendations = model.generate(\n        user_sequence.unsqueeze(0),\n        max_length=20,\n        temperature=0.8,\n        top_k=50\n    )\n\nprint(f\"Recommendations: {recommendations.squeeze().tolist()}\")\n</code></pre>"},{"location":"zh/api/trainers/","title":"\u8bad\u7ec3\u5668 API \u53c2\u8003","text":"<p>\u8bad\u7ec3\u5de5\u5177\u548c\u811a\u672c\u7684\u8be6\u7ec6\u6587\u6863\u3002</p>"},{"location":"zh/api/trainers/#_1","title":"\u57fa\u7840\u8bad\u7ec3\u5668","text":""},{"location":"zh/api/trainers/#basetrainer","title":"BaseTrainer","text":"<p>\u6240\u6709\u8bad\u7ec3\u5668\u7684\u57fa\u7840\u7c7b\u3002</p> <pre><code>class BaseTrainer:\n    \"\"\"\u57fa\u7840\u8bad\u7ec3\u5668\u7c7b\"\"\"\n\n    def __init__(\n        self,\n        model: LightningModule,\n        config: TrainingConfig,\n        logger: Optional[Logger] = None\n    ):\n        self.model = model\n        self.config = config\n        self.logger = logger or self._create_default_logger()\n        self.trainer = None\n\n    def _create_default_logger(self) -&gt; Logger:\n        \"\"\"\u521b\u5efa\u9ed8\u8ba4\u65e5\u5fd7\u8bb0\u5f55\u5668\"\"\"\n        from pytorch_lightning.loggers import TensorBoardLogger\n\n        return TensorBoardLogger(\n            save_dir=self.config.log_dir,\n            name=self.config.experiment_name,\n            version=self.config.version\n        )\n</code></pre> <p>\u65b9\u6cd5:</p>"},{"location":"zh/api/trainers/#setup_trainer","title":"setup_trainer()","text":"<p>\u8bbe\u7f6e PyTorch Lightning \u8bad\u7ec3\u5668\u3002</p> <pre><code>def setup_trainer(self) -&gt; None:\n    \"\"\"\u8bbe\u7f6e\u8bad\u7ec3\u5668\"\"\"\n    from pytorch_lightning import Trainer\n    from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\n    # \u56de\u8c03\u51fd\u6570\n    callbacks = []\n\n    # \u68c0\u67e5\u70b9\u56de\u8c03\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=self.config.checkpoint_dir,\n        filename='{epoch}-{val_loss:.2f}',\n        monitor='val_loss',\n        mode='min',\n        save_top_k=self.config.save_top_k,\n        save_last=True\n    )\n    callbacks.append(checkpoint_callback)\n\n    # \u65e9\u505c\u56de\u8c03\n    if self.config.early_stopping_patience &gt; 0:\n        early_stop_callback = EarlyStopping(\n            monitor='val_loss',\n            patience=self.config.early_stopping_patience,\n            mode='min'\n        )\n        callbacks.append(early_stop_callback)\n\n    # \u521b\u5efa\u8bad\u7ec3\u5668\n    self.trainer = Trainer(\n        max_epochs=self.config.max_epochs,\n        gpus=self.config.gpus,\n        precision=self.config.precision,\n        gradient_clip_val=self.config.gradient_clip_val,\n        accumulate_grad_batches=self.config.accumulate_grad_batches,\n        val_check_interval=self.config.val_check_interval,\n        callbacks=callbacks,\n        logger=self.logger,\n        deterministic=self.config.deterministic,\n        enable_progress_bar=self.config.progress_bar\n    )\n</code></pre>"},{"location":"zh/api/trainers/#traintrain_dataloader-val_dataloader","title":"train(train_dataloader, val_dataloader)","text":"<p>\u6267\u884c\u8bad\u7ec3\u3002</p> <pre><code>def train(\n    self,\n    train_dataloader: DataLoader,\n    val_dataloader: Optional[DataLoader] = None\n) -&gt; None:\n    \"\"\"\n    \u6267\u884c\u6a21\u578b\u8bad\u7ec3\n\n    Args:\n        train_dataloader: \u8bad\u7ec3\u6570\u636e\u52a0\u8f7d\u5668\n        val_dataloader: \u9a8c\u8bc1\u6570\u636e\u52a0\u8f7d\u5668\n    \"\"\"\n    if self.trainer is None:\n        self.setup_trainer()\n\n    self.trainer.fit(self.model, train_dataloader, val_dataloader)\n</code></pre>"},{"location":"zh/api/trainers/#testtest_dataloader","title":"test(test_dataloader)","text":"<p>\u6267\u884c\u6d4b\u8bd5\u3002</p> <pre><code>def test(self, test_dataloader: DataLoader) -&gt; Dict[str, float]:\n    \"\"\"\n    \u6267\u884c\u6a21\u578b\u6d4b\u8bd5\n\n    Args:\n        test_dataloader: \u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668\n\n    Returns:\n        \u6d4b\u8bd5\u7ed3\u679c\u5b57\u5178\n    \"\"\"\n    if self.trainer is None:\n        self.setup_trainer()\n\n    results = self.trainer.test(self.model, test_dataloader)\n    return results[0] if results else {}\n</code></pre>"},{"location":"zh/api/trainers/#rqvae","title":"RQVAE \u8bad\u7ec3\u5668","text":""},{"location":"zh/api/trainers/#rqvaetrainer","title":"RQVAETrainer","text":"<p>\u4e13\u95e8\u7528\u4e8e\u8bad\u7ec3 RQVAE \u6a21\u578b\u7684\u8bad\u7ec3\u5668\u3002</p> <pre><code>class RQVAETrainer(BaseTrainer):\n    \"\"\"RQVAE \u8bad\u7ec3\u5668\"\"\"\n\n    def __init__(\n        self,\n        model: RqVae,\n        config: RQVAETrainingConfig,\n        dataset: ItemDataset,\n        logger: Optional[Logger] = None\n    ):\n        super().__init__(model, config, logger)\n        self.dataset = dataset\n\n    def create_dataloaders(self) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n        \"\"\"\n        \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\n\n        Returns:\n            (train_loader, val_loader, test_loader): \u6570\u636e\u52a0\u8f7d\u5668\u5143\u7ec4\n        \"\"\"\n        # \u5206\u5272\u6570\u636e\u96c6\n        train_size = int(0.8 * len(self.dataset))\n        val_size = int(0.1 * len(self.dataset))\n        test_size = len(self.dataset) - train_size - val_size\n\n        train_dataset, val_dataset, test_dataset = random_split(\n            self.dataset, \n            [train_size, val_size, test_size],\n            generator=torch.Generator().manual_seed(self.config.random_seed)\n        )\n\n        # \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            pin_memory=True\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=False,\n            num_workers=self.config.num_workers,\n            pin_memory=True\n        )\n\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=False,\n            num_workers=self.config.num_workers,\n            pin_memory=True\n        )\n\n        return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"zh/api/trainers/#train_model","title":"train_model()","text":"<p>\u8bad\u7ec3 RQVAE \u6a21\u578b\u3002</p> <pre><code>def train_model(self) -&gt; RqVae:\n    \"\"\"\n    \u8bad\u7ec3 RQVAE \u6a21\u578b\n\n    Returns:\n        \u8bad\u7ec3\u597d\u7684\u6a21\u578b\n    \"\"\"\n    # \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\n    train_loader, val_loader, test_loader = self.create_dataloaders()\n\n    # \u6267\u884c\u8bad\u7ec3\n    self.train(train_loader, val_loader)\n\n    # \u6d4b\u8bd5\u6a21\u578b\n    if self.config.run_test:\n        test_results = self.test(test_loader)\n        print(f\"Test results: {test_results}\")\n\n    return self.model\n</code></pre>"},{"location":"zh/api/trainers/#evaluate_reconstructiontest_dataloader","title":"evaluate_reconstruction(test_dataloader)","text":"<p>\u8bc4\u4f30\u91cd\u6784\u8d28\u91cf\u3002</p> <pre><code>def evaluate_reconstruction(self, test_dataloader: DataLoader) -&gt; Dict[str, float]:\n    \"\"\"\n    \u8bc4\u4f30\u91cd\u6784\u8d28\u91cf\n\n    Args:\n        test_dataloader: \u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668\n\n    Returns:\n        \u8bc4\u4f30\u6307\u6807\u5b57\u5178\n    \"\"\"\n    self.model.eval()\n    device = next(self.model.parameters()).device\n\n    total_mse = 0\n    total_cosine_sim = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            if isinstance(batch, dict):\n                features = batch['features'].to(device)\n            else:\n                features = batch.to(device)\n\n            # \u524d\u5411\u4f20\u64ad\n            reconstructed, _, _, _ = self.model(features)\n\n            # \u8ba1\u7b97\u6307\u6807\n            mse = F.mse_loss(reconstructed, features, reduction='sum')\n            cosine_sim = F.cosine_similarity(reconstructed, features, dim=1).sum()\n\n            total_mse += mse.item()\n            total_cosine_sim += cosine_sim.item()\n            total_samples += features.size(0)\n\n    return {\n        'mse': total_mse / total_samples,\n        'rmse': (total_mse / total_samples) ** 0.5,\n        'cosine_similarity': total_cosine_sim / total_samples\n    }\n</code></pre>"},{"location":"zh/api/trainers/#generate_semantic_idsdataloader","title":"generate_semantic_ids(dataloader)","text":"<p>\u4e3a\u6570\u636e\u96c6\u751f\u6210\u8bed\u4e49 ID\u3002</p> <pre><code>def generate_semantic_ids(self, dataloader: DataLoader) -&gt; torch.Tensor:\n    \"\"\"\n    \u4e3a\u6570\u636e\u96c6\u751f\u6210\u8bed\u4e49 ID\n\n    Args:\n        dataloader: \u6570\u636e\u52a0\u8f7d\u5668\n\n    Returns:\n        \u8bed\u4e49 ID \u5f20\u91cf (num_samples,)\n    \"\"\"\n    self.model.eval()\n    device = next(self.model.parameters()).device\n\n    all_semantic_ids = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            if isinstance(batch, dict):\n                features = batch['features'].to(device)\n            else:\n                features = batch.to(device)\n\n            semantic_ids = self.model.generate_semantic_ids(features)\n            all_semantic_ids.append(semantic_ids.cpu())\n\n    return torch.cat(all_semantic_ids, dim=0)\n</code></pre>"},{"location":"zh/api/trainers/#tiger","title":"TIGER \u8bad\u7ec3\u5668","text":""},{"location":"zh/api/trainers/#tigertrainer","title":"TIGERTrainer","text":"<p>\u4e13\u95e8\u7528\u4e8e\u8bad\u7ec3 TIGER \u6a21\u578b\u7684\u8bad\u7ec3\u5668\u3002</p> <pre><code>class TIGERTrainer(BaseTrainer):\n    \"\"\"TIGER \u8bad\u7ec3\u5668\"\"\"\n\n    def __init__(\n        self,\n        model: Tiger,\n        config: TIGERTrainingConfig,\n        dataset: SequenceDataset,\n        logger: Optional[Logger] = None\n    ):\n        super().__init__(model, config, logger)\n        self.dataset = dataset\n        self.collate_fn = self._create_collate_fn()\n\n    def _create_collate_fn(self) -&gt; Callable:\n        \"\"\"\u521b\u5efa\u6570\u636e\u6574\u7406\u51fd\u6570\"\"\"\n        def collate_fn(batch):\n            # \u63d0\u53d6\u5e8f\u5217\u6570\u636e\n            input_ids = [item['input_ids'] for item in batch]\n            labels = [item['labels'] for item in batch]\n\n            # \u586b\u5145\u5e8f\u5217\n            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n            labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n\n            # \u521b\u5efa\u6ce8\u610f\u529b\u63a9\u7801\n            attention_mask = (input_ids != 0).float()\n\n            return {\n                'input_ids': input_ids,\n                'labels': labels,\n                'attention_mask': attention_mask\n            }\n\n        return collate_fn\n</code></pre>"},{"location":"zh/api/trainers/#create_dataloaders","title":"create_dataloaders()","text":"<p>\u521b\u5efa TIGER \u6570\u636e\u52a0\u8f7d\u5668\u3002</p> <pre><code>def create_dataloaders(self) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"\n    \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\n\n    Returns:\n        (train_loader, val_loader, test_loader): \u6570\u636e\u52a0\u8f7d\u5668\u5143\u7ec4\n    \"\"\"\n    # \u5206\u5272\u6570\u636e\u96c6\n    train_size = int(0.8 * len(self.dataset))\n    val_size = int(0.1 * len(self.dataset))\n    test_size = len(self.dataset) - train_size - val_size\n\n    train_dataset, val_dataset, test_dataset = random_split(\n        self.dataset,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(self.config.random_seed)\n    )\n\n    # \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=self.config.batch_size,\n        shuffle=True,\n        num_workers=self.config.num_workers,\n        collate_fn=self.collate_fn,\n        pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=self.config.batch_size,\n        shuffle=False,\n        num_workers=self.config.num_workers,\n        collate_fn=self.collate_fn,\n        pin_memory=True\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=self.config.batch_size,\n        shuffle=False,\n        num_workers=self.config.num_workers,\n        collate_fn=self.collate_fn,\n        pin_memory=True\n    )\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"zh/api/trainers/#evaluate_generationtest_dataloader-k_values","title":"evaluate_generation(test_dataloader, k_values)","text":"<p>\u8bc4\u4f30\u751f\u6210\u8d28\u91cf\u3002</p> <pre><code>def evaluate_generation(\n    self,\n    test_dataloader: DataLoader,\n    k_values: List[int] = [5, 10, 20]\n) -&gt; Dict[str, float]:\n    \"\"\"\n    \u8bc4\u4f30\u751f\u6210\u8d28\u91cf\n\n    Args:\n        test_dataloader: \u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668\n        k_values: Top-K \u503c\u5217\u8868\n\n    Returns:\n        \u8bc4\u4f30\u6307\u6807\u5b57\u5178\n    \"\"\"\n    self.model.eval()\n    device = next(self.model.parameters()).device\n\n    all_predictions = []\n    all_targets = []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n\n            # \u751f\u6210\u63a8\u8350\n            generated = self.model.generate(\n                input_ids,\n                max_length=self.config.max_generation_length,\n                temperature=self.config.generation_temperature,\n                top_k=self.config.generation_top_k\n            )\n\n            # \u63d0\u53d6\u76ee\u6807\u5e8f\u5217\n            targets = []\n            for label_seq in labels:\n                target = label_seq[label_seq != -100].cpu().tolist()\n                targets.append(target)\n\n            all_predictions.extend(generated.cpu().tolist())\n            all_targets.extend(targets)\n\n    # \u8ba1\u7b97\u6307\u6807\n    metrics = {}\n    for k in k_values:\n        recall_k = self._compute_recall_at_k(all_predictions, all_targets, k)\n        ndcg_k = self._compute_ndcg_at_k(all_predictions, all_targets, k)\n\n        metrics[f'recall@{k}'] = recall_k\n        metrics[f'ndcg@{k}'] = ndcg_k\n\n    return metrics\n</code></pre>"},{"location":"zh/api/trainers/#_compute_recall_at_kpredictions-targets-k","title":"_compute_recall_at_k(predictions, targets, k)","text":"<p>\u8ba1\u7b97 Recall@K\u3002</p> <pre><code>def _compute_recall_at_k(\n    self,\n    predictions: List[List[int]],\n    targets: List[List[int]],\n    k: int\n) -&gt; float:\n    \"\"\"\u8ba1\u7b97 Recall@K\"\"\"\n    recall_scores = []\n\n    for pred, target in zip(predictions, targets):\n        if len(target) == 0:\n            continue\n\n        top_k_pred = set(pred[:k])\n        target_set = set(target)\n\n        recall = len(top_k_pred &amp; target_set) / len(target_set)\n        recall_scores.append(recall)\n\n    return np.mean(recall_scores) if recall_scores else 0.0\n</code></pre>"},{"location":"zh/api/trainers/#_compute_ndcg_at_kpredictions-targets-k","title":"_compute_ndcg_at_k(predictions, targets, k)","text":"<p>\u8ba1\u7b97 NDCG@K\u3002</p> <pre><code>def _compute_ndcg_at_k(\n    self,\n    predictions: List[List[int]],\n    targets: List[List[int]],\n    k: int\n) -&gt; float:\n    \"\"\"\u8ba1\u7b97 NDCG@K\"\"\"\n    ndcg_scores = []\n\n    for pred, target in zip(predictions, targets):\n        if len(target) == 0:\n            continue\n\n        # \u8ba1\u7b97 DCG\n        dcg = 0\n        for i, item in enumerate(pred[:k]):\n            if item in target:\n                dcg += 1 / np.log2(i + 2)\n\n        # \u8ba1\u7b97 IDCG\n        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(target), k)))\n\n        # \u8ba1\u7b97 NDCG\n        ndcg = dcg / idcg if idcg &gt; 0 else 0\n        ndcg_scores.append(ndcg)\n\n    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n</code></pre>"},{"location":"zh/api/trainers/#_2","title":"\u8bad\u7ec3\u914d\u7f6e","text":""},{"location":"zh/api/trainers/#trainingconfig","title":"TrainingConfig","text":"<p>\u57fa\u7840\u8bad\u7ec3\u914d\u7f6e\u3002</p> <pre><code>@dataclass\nclass TrainingConfig:\n    # \u57fa\u7840\u8bbe\u7f6e\n    max_epochs: int = 100\n    batch_size: int = 32\n    learning_rate: float = 1e-3\n\n    # \u786c\u4ef6\u8bbe\u7f6e\n    gpus: int = 1 if torch.cuda.is_available() else 0\n    precision: int = 32\n    num_workers: int = 4\n\n    # \u8bad\u7ec3\u7b56\u7565\n    gradient_clip_val: float = 1.0\n    accumulate_grad_batches: int = 1\n    val_check_interval: float = 1.0\n\n    # \u68c0\u67e5\u70b9\u548c\u65e5\u5fd7\n    checkpoint_dir: str = \"checkpoints\"\n    log_dir: str = \"logs\"\n    experiment_name: str = \"experiment\"\n    version: Optional[str] = None\n    save_top_k: int = 3\n\n    # \u65e9\u505c\n    early_stopping_patience: int = 10\n\n    # \u5176\u4ed6\n    deterministic: bool = True\n    random_seed: int = 42\n    progress_bar: bool = True\n    run_test: bool = True\n</code></pre>"},{"location":"zh/api/trainers/#rqvaetrainingconfig","title":"RQVAETrainingConfig","text":"<p>RQVAE \u8bad\u7ec3\u914d\u7f6e\u3002</p> <pre><code>@dataclass\nclass RQVAETrainingConfig(TrainingConfig):\n    # \u6a21\u578b\u53c2\u6570\n    input_dim: int = 768\n    hidden_dim: int = 512\n    latent_dim: int = 256\n    num_embeddings: int = 1024\n    commitment_cost: float = 0.25\n\n    # \u8bad\u7ec3\u53c2\u6570\n    learning_rate: float = 1e-3\n    batch_size: int = 64\n    max_epochs: int = 100\n\n    # \u6570\u636e\u96c6\u53c2\u6570\n    dataset_name: str = \"p5_amazon\"\n    dataset_split: str = \"beauty\"\n\n    # \u8bc4\u4f30\u53c2\u6570\n    eval_reconstruction: bool = True\n    eval_quantization: bool = True\n</code></pre>"},{"location":"zh/api/trainers/#tigertrainingconfig","title":"TIGERTrainingConfig","text":"<p>TIGER \u8bad\u7ec3\u914d\u7f6e\u3002</p> <pre><code>@dataclass\nclass TIGERTrainingConfig(TrainingConfig):\n    # \u6a21\u578b\u53c2\u6570\n    vocab_size: int = 1024\n    embedding_dim: int = 512\n    num_heads: int = 8\n    num_layers: int = 6\n    attn_dim: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 100\n\n    # \u8bad\u7ec3\u53c2\u6570\n    learning_rate: float = 1e-4\n    batch_size: int = 16\n    max_epochs: int = 50\n\n    # \u751f\u6210\u53c2\u6570\n    max_generation_length: int = 50\n    generation_temperature: float = 1.0\n    generation_top_k: int = 50\n    generation_top_p: float = 0.9\n\n    # \u6570\u636e\u96c6\u53c2\u6570\n    dataset_name: str = \"p5_amazon\"\n    dataset_split: str = \"beauty\"\n    pretrained_rqvae_path: str = \"checkpoints/rqvae.ckpt\"\n\n    # \u8bc4\u4f30\u53c2\u6570\n    eval_generation: bool = True\n    eval_k_values: List[int] = field(default_factory=lambda: [5, 10, 20])\n</code></pre>"},{"location":"zh/api/trainers/#_3","title":"\u8bad\u7ec3\u811a\u672c","text":""},{"location":"zh/api/trainers/#rqvae_1","title":"\u8bad\u7ec3 RQVAE","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\u8bad\u7ec3 RQVAE \u6a21\u578b\u7684\u811a\u672c\"\"\"\n\nimport argparse\nfrom pathlib import Path\n\nfrom generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.dataset_factory import DatasetFactory\nfrom generative_recommenders.trainers import RQVAETrainer, RQVAETrainingConfig\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train RQVAE model\")\n    parser.add_argument(\"--dataset\", default=\"p5_amazon\", help=\"Dataset name\")\n    parser.add_argument(\"--split\", default=\"beauty\", help=\"Dataset split\")\n    parser.add_argument(\"--root\", default=\"dataset\", help=\"Dataset root directory\")\n    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n    parser.add_argument(\"--max_epochs\", type=int, default=100, help=\"Max epochs\")\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate\")\n    parser.add_argument(\"--checkpoint_dir\", default=\"checkpoints\", help=\"Checkpoint directory\")\n\n    args = parser.parse_args()\n\n    # \u521b\u5efa\u914d\u7f6e\n    config = RQVAETrainingConfig(\n        batch_size=args.batch_size,\n        max_epochs=args.max_epochs,\n        learning_rate=args.learning_rate,\n        checkpoint_dir=args.checkpoint_dir,\n        dataset_name=args.dataset,\n        dataset_split=args.split\n    )\n\n    # \u521b\u5efa\u6570\u636e\u96c6\n    dataset = DatasetFactory.create_item_dataset(\n        args.dataset,\n        root=args.root,\n        split=args.split,\n        train_test_split=\"all\"\n    )\n\n    # \u521b\u5efa\u6a21\u578b\n    model = RqVae(\n        input_dim=config.input_dim,\n        hidden_dim=config.hidden_dim,\n        latent_dim=config.latent_dim,\n        num_embeddings=config.num_embeddings,\n        commitment_cost=config.commitment_cost,\n        learning_rate=config.learning_rate\n    )\n\n    # \u521b\u5efa\u8bad\u7ec3\u5668\n    trainer = RQVAETrainer(model, config, dataset)\n\n    # \u8bad\u7ec3\u6a21\u578b\n    trained_model = trainer.train_model()\n\n    print(f\"Training completed. Model saved to {config.checkpoint_dir}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/api/trainers/#tiger_1","title":"\u8bad\u7ec3 TIGER","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\u8bad\u7ec3 TIGER \u6a21\u578b\u7684\u811a\u672c\"\"\"\n\nimport argparse\nfrom pathlib import Path\n\nfrom generative_recommenders.models.tiger import Tiger\nfrom generative_recommenders.data.dataset_factory import DatasetFactory\nfrom generative_recommenders.trainers import TIGERTrainer, TIGERTrainingConfig\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train TIGER model\")\n    parser.add_argument(\"--dataset\", default=\"p5_amazon\", help=\"Dataset name\")\n    parser.add_argument(\"--split\", default=\"beauty\", help=\"Dataset split\")\n    parser.add_argument(\"--root\", default=\"dataset\", help=\"Dataset root directory\")\n    parser.add_argument(\"--rqvae_path\", required=True, help=\"Pretrained RQVAE checkpoint path\")\n    parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size\")\n    parser.add_argument(\"--max_epochs\", type=int, default=50, help=\"Max epochs\")\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help=\"Learning rate\")\n    parser.add_argument(\"--checkpoint_dir\", default=\"checkpoints\", help=\"Checkpoint directory\")\n\n    args = parser.parse_args()\n\n    # \u521b\u5efa\u914d\u7f6e\n    config = TIGERTrainingConfig(\n        batch_size=args.batch_size,\n        max_epochs=args.max_epochs,\n        learning_rate=args.learning_rate,\n        checkpoint_dir=args.checkpoint_dir,\n        dataset_name=args.dataset,\n        dataset_split=args.split,\n        pretrained_rqvae_path=args.rqvae_path\n    )\n\n    # \u521b\u5efa\u6570\u636e\u96c6\n    dataset = DatasetFactory.create_sequence_dataset(\n        args.dataset,\n        root=args.root,\n        split=args.split,\n        train_test_split=\"train\",\n        pretrained_rqvae_path=args.rqvae_path\n    )\n\n    # \u521b\u5efa\u6a21\u578b\n    model = Tiger(\n        vocab_size=config.vocab_size,\n        embedding_dim=config.embedding_dim,\n        num_heads=config.num_heads,\n        num_layers=config.num_layers,\n        learning_rate=config.learning_rate\n    )\n\n    # \u521b\u5efa\u8bad\u7ec3\u5668\n    trainer = TIGERTrainer(model, config, dataset)\n\n    # \u8bad\u7ec3\u6a21\u578b\n    trained_model = trainer.train_model()\n\n    print(f\"Training completed. Model saved to {config.checkpoint_dir}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/api/trainers/#_4","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/api/trainers/#_5","title":"\u57fa\u672c\u8bad\u7ec3","text":"<pre><code>from generative_recommenders.trainers import RQVAETrainer, RQVAETrainingConfig\nfrom generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.dataset_factory import DatasetFactory\n\n# \u521b\u5efa\u914d\u7f6e\nconfig = RQVAETrainingConfig(\n    batch_size=64,\n    max_epochs=100,\n    learning_rate=1e-3\n)\n\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset = DatasetFactory.create_item_dataset(\n    \"p5_amazon\",\n    root=\"dataset/amazon\",\n    split=\"beauty\"\n)\n\n# \u521b\u5efa\u6a21\u578b\nmodel = RqVae(\n    input_dim=768,\n    hidden_dim=512,\n    num_embeddings=1024\n)\n\n# \u521b\u5efa\u8bad\u7ec3\u5668\u5e76\u8bad\u7ec3\ntrainer = RQVAETrainer(model, config, dataset)\ntrained_model = trainer.train_model()\n</code></pre>"},{"location":"zh/dataset/custom/","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6","text":"<p>\u672c\u6307\u5357\u4ecb\u7ecd\u5982\u4f55\u4e3a GenerativeRecommenders \u6846\u67b6\u6dfb\u52a0\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002</p>"},{"location":"zh/dataset/custom/#_2","title":"\u57fa\u672c\u6982\u5ff5","text":""},{"location":"zh/dataset/custom/#_3","title":"\u6570\u636e\u96c6\u7c7b\u578b","text":"<p>\u6846\u67b6\u652f\u6301\u4e24\u79cd\u4e3b\u8981\u7684\u6570\u636e\u96c6\u7c7b\u578b\uff1a</p> <ol> <li>ItemDataset: \u7269\u54c1\u7ea7\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3 RQVAE</li> <li>SequenceDataset: \u5e8f\u5217\u7ea7\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3 TIGER</li> </ol>"},{"location":"zh/dataset/custom/#_4","title":"\u57fa\u7840\u67b6\u6784","text":"<p>\u6240\u6709\u6570\u636e\u96c6\u90fd\u7ee7\u627f\u81ea <code>BaseRecommenderDataset</code>\uff1a</p> <pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\n\nclass MyCustomDataset(BaseRecommenderDataset):\n    def __init__(self, config):\n        super().__init__(config)\n        # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u53c2\u6570\n\n    def download(self):\n        # \u5b9e\u73b0\u6570\u636e\u4e0b\u8f7d\u903b\u8f91\n        pass\n\n    def load_raw_data(self):\n        # \u52a0\u8f7d\u539f\u59cb\u6570\u636e\u6587\u4ef6\n        pass\n\n    def preprocess_data(self, raw_data):\n        # \u9884\u5904\u7406\u6570\u636e\n        pass\n\n    def extract_items(self, processed_data):\n        # \u63d0\u53d6\u7269\u54c1\u4fe1\u606f\n        pass\n\n    def extract_interactions(self, processed_data):\n        # \u63d0\u53d6\u7528\u6237\u4ea4\u4e92\u4fe1\u606f\n        pass\n</code></pre>"},{"location":"zh/dataset/custom/#_5","title":"\u5b9e\u73b0\u6b65\u9aa4","text":""},{"location":"zh/dataset/custom/#1","title":"1. \u521b\u5efa\u914d\u7f6e\u7c7b","text":"<p>\u9996\u5148\u5b9a\u4e49\u6570\u636e\u96c6\u7279\u5b9a\u7684\u914d\u7f6e\uff1a</p> <pre><code>from dataclasses import dataclass\nfrom generative_recommenders.data.configs import DatasetConfig\n\n@dataclass\nclass MyDatasetConfig(DatasetConfig):\n    # \u6570\u636e\u96c6\u7279\u5b9a\u53c2\u6570\n    api_key: str = \"\"\n    data_format: str = \"json\"\n    categories: List[str] = None\n</code></pre>"},{"location":"zh/dataset/custom/#2","title":"2. \u5b9e\u73b0\u6570\u636e\u4e0b\u8f7d","text":"<pre><code>def download(self):\n    \"\"\"\u4e0b\u8f7d\u6570\u636e\u96c6\u5230\u672c\u5730\"\"\"\n    if self._data_exists():\n        return\n\n    print(\"Downloading custom dataset...\")\n\n    # \u793a\u4f8b\uff1a\u4ece API \u4e0b\u8f7d\u6570\u636e\n    import requests\n    response = requests.get(f\"https://api.example.com/data?key={self.config.api_key}\")\n\n    # \u4fdd\u5b58\u5230\u672c\u5730\n    data_path = self.root_path / \"raw\" / \"data.json\"\n    data_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(data_path, 'w') as f:\n        json.dump(response.json(), f)\n\n    print(\"Download completed.\")\n\ndef _data_exists(self):\n    \"\"\"\u68c0\u67e5\u6570\u636e\u662f\u5426\u5df2\u5b58\u5728\"\"\"\n    data_path = self.root_path / \"raw\" / \"data.json\"\n    return data_path.exists()\n</code></pre>"},{"location":"zh/dataset/custom/#3","title":"3. \u5b9e\u73b0\u6570\u636e\u52a0\u8f7d","text":"<pre><code>def load_raw_data(self):\n    \"\"\"\u52a0\u8f7d\u539f\u59cb\u6570\u636e\"\"\"\n    data_path = self.root_path / \"raw\" / \"data.json\"\n\n    with open(data_path, 'r') as f:\n        raw_data = json.load(f)\n\n    # \u89e3\u6790\u6570\u636e\u7ed3\u6784\n    users = raw_data.get('users', [])\n    items = raw_data.get('items', [])\n    interactions = raw_data.get('interactions', [])\n\n    return {\n        \"users\": pd.DataFrame(users),\n        \"items\": pd.DataFrame(items), \n        \"interactions\": pd.DataFrame(interactions)\n    }\n</code></pre>"},{"location":"zh/dataset/custom/#4","title":"4. \u5b9e\u73b0\u6570\u636e\u9884\u5904\u7406","text":"<pre><code>def preprocess_data(self, raw_data):\n    \"\"\"\u9884\u5904\u7406\u6570\u636e\"\"\"\n    # \u6e05\u6d17\u7269\u54c1\u6570\u636e\n    items_df = self._clean_items(raw_data[\"items\"])\n\n    # \u6e05\u6d17\u4ea4\u4e92\u6570\u636e\n    interactions_df = self._clean_interactions(raw_data[\"interactions\"])\n\n    # \u8fc7\u6ee4\u4f4e\u9891\u7528\u6237\u548c\u7269\u54c1\n    interactions_df = self.filter_low_interactions(\n        interactions_df,\n        min_user_interactions=self.config.processing_config.min_user_interactions,\n        min_item_interactions=self.config.processing_config.min_item_interactions\n    )\n\n    # \u5904\u7406\u7269\u54c1\u7279\u5f81\n    items_df = self._process_item_features(items_df)\n\n    return {\n        \"items\": items_df,\n        \"interactions\": interactions_df\n    }\n\ndef _clean_items(self, items_df):\n    \"\"\"\u6e05\u6d17\u7269\u54c1\u6570\u636e\"\"\"\n    # \u586b\u5145\u7f3a\u5931\u503c\n    items_df[\"title\"] = items_df[\"title\"].fillna(\"Unknown\")\n    items_df[\"category\"] = items_df[\"category\"].fillna(\"Unknown\")\n\n    # \u6807\u51c6\u5316\u6587\u672c\n    items_df[\"title\"] = items_df[\"title\"].str.strip()\n\n    return items_df\n\ndef _clean_interactions(self, interactions_df):\n    \"\"\"\u6e05\u6d17\u4ea4\u4e92\u6570\u636e\"\"\"\n    # \u79fb\u9664\u91cd\u590d\u4ea4\u4e92\n    interactions_df = interactions_df.drop_duplicates([\"user_id\", \"item_id\"])\n\n    # \u6dfb\u52a0\u65f6\u95f4\u6233\uff08\u5982\u679c\u6ca1\u6709\uff09\n    if \"timestamp\" not in interactions_df.columns:\n        interactions_df[\"timestamp\"] = range(len(interactions_df))\n\n    return interactions_df\n</code></pre>"},{"location":"zh/dataset/custom/#5","title":"5. \u5b9e\u73b0\u7279\u5f81\u63d0\u53d6","text":"<pre><code>def _process_item_features(self, items_df):\n    \"\"\"\u5904\u7406\u7269\u54c1\u7279\u5f81\"\"\"\n    # \u4f7f\u7528\u6587\u672c\u5904\u7406\u5668\u751f\u6210\u5d4c\u5165\n    cache_key = f\"custom_dataset_{self.config.split}\"\n    embeddings = self.text_processor.encode_item_features(\n        items_df,\n        cache_key=cache_key,\n        force_reload=self.config.force_reload\n    )\n\n    # \u6dfb\u52a0\u5d4c\u5165\u7279\u5f81\n    items_df = items_df.copy()\n    items_df[\"features\"] = embeddings.tolist()\n\n    # \u521b\u5efa\u6587\u672c\u5b57\u6bb5\u7528\u4e8e\u53c2\u8003\n    texts = []\n    for _, row in items_df.iterrows():\n        text = f\"Title: {row['title']}; Category: {row['category']}\"\n        texts.append(text)\n\n    items_df[\"text\"] = texts\n\n    return items_df\n\ndef extract_items(self, processed_data):\n    \"\"\"\u63d0\u53d6\u7269\u54c1\u4fe1\u606f\"\"\"\n    return processed_data[\"items\"]\n\ndef extract_interactions(self, processed_data):\n    \"\"\"\u63d0\u53d6\u4ea4\u4e92\u4fe1\u606f\"\"\"\n    return processed_data[\"interactions\"]\n</code></pre>"},{"location":"zh/dataset/custom/#_6","title":"\u521b\u5efa\u6570\u636e\u96c6\u5305\u88c5\u5668","text":""},{"location":"zh/dataset/custom/#_7","title":"\u7269\u54c1\u6570\u636e\u96c6","text":"<pre><code>from generative_recommenders.data.base_dataset import ItemDataset\nimport gin\n\n@gin.configurable\nclass MyItemDataset(ItemDataset):\n    \"\"\"\u81ea\u5b9a\u4e49\u7269\u54c1\u6570\u636e\u96c6\"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        split: str = \"default\",\n        train_test_split: str = \"all\",\n        api_key: str = \"\",\n        **kwargs\n    ):\n        # \u521b\u5efa\u914d\u7f6e\n        config = MyDatasetConfig(\n            root_dir=root,\n            split=split,\n            api_key=api_key,\n            **kwargs\n        )\n\n        # \u521b\u5efa\u57fa\u7840\u6570\u636e\u96c6\n        base_dataset = MyCustomDataset(config)\n\n        # \u521d\u59cb\u5316\u7269\u54c1\u6570\u636e\u96c6\n        super().__init__(\n            base_dataset=base_dataset,\n            split=train_test_split,\n            return_text=False\n        )\n</code></pre>"},{"location":"zh/dataset/custom/#_8","title":"\u5e8f\u5217\u6570\u636e\u96c6","text":"<pre><code>from generative_recommenders.data.base_dataset import SequenceDataset\n\n@gin.configurable\nclass MySequenceDataset(SequenceDataset):\n    \"\"\"\u81ea\u5b9a\u4e49\u5e8f\u5217\u6570\u636e\u96c6\"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        split: str = \"default\",\n        train_test_split: str = \"train\",\n        api_key: str = \"\",\n        pretrained_rqvae_path: Optional[str] = None,\n        **kwargs\n    ):\n        # \u521b\u5efa\u914d\u7f6e\n        config = MyDatasetConfig(\n            root_dir=root,\n            split=split,\n            api_key=api_key,\n            **kwargs\n        )\n\n        # \u52a0\u8f7d\u8bed\u4e49\u7f16\u7801\u5668\n        semantic_encoder = None\n        if pretrained_rqvae_path:\n            from generative_recommenders.models.rqvae import RqVae\n            semantic_encoder = RqVae.load_from_checkpoint(pretrained_rqvae_path)\n            semantic_encoder.eval()\n\n        # \u521b\u5efa\u57fa\u7840\u6570\u636e\u96c6\n        base_dataset = MyCustomDataset(config)\n\n        # \u521d\u59cb\u5316\u5e8f\u5217\u6570\u636e\u96c6\n        super().__init__(\n            base_dataset=base_dataset,\n            split=train_test_split,\n            semantic_encoder=semantic_encoder\n        )\n</code></pre>"},{"location":"zh/dataset/custom/#_9","title":"\u6ce8\u518c\u6570\u636e\u96c6","text":""},{"location":"zh/dataset/custom/#_10","title":"\u4f7f\u7528\u5de5\u5382\u6a21\u5f0f","text":"<pre><code>from generative_recommenders.data.dataset_factory import DatasetFactory\n\n# \u6ce8\u518c\u6570\u636e\u96c6\nDatasetFactory.register_dataset(\n    \"my_dataset\",\n    base_class=MyCustomDataset,\n    item_class=MyItemDataset,\n    sequence_class=MySequenceDataset\n)\n\n# \u4f7f\u7528\u5de5\u5382\u521b\u5efa\u6570\u636e\u96c6\nitem_dataset = DatasetFactory.create_item_dataset(\n    \"my_dataset\",\n    root_dir=\"path/to/data\",\n    api_key=\"your_api_key\"\n)\n</code></pre>"},{"location":"zh/dataset/custom/#_11","title":"\u914d\u7f6e\u6587\u4ef6\u96c6\u6210","text":""},{"location":"zh/dataset/custom/#gin","title":"Gin \u914d\u7f6e\u6587\u4ef6","text":"<p>\u521b\u5efa\u914d\u7f6e\u6587\u4ef6 <code>config/my_dataset.gin</code>\uff1a</p> <pre><code>import my_module.my_dataset\n\n# \u6570\u636e\u96c6\u53c2\u6570\ntrain.dataset_folder=\"dataset/my_dataset\"\ntrain.dataset=@MyItemDataset\n\n# \u81ea\u5b9a\u4e49\u53c2\u6570\nMyItemDataset.api_key=\"your_api_key\"\nMyItemDataset.split=\"category_a\"\n\n# \u6587\u672c\u7f16\u7801\u53c2\u6570\nMyItemDataset.encoder_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n</code></pre>"},{"location":"zh/dataset/custom/#_12","title":"\u6d4b\u8bd5\u548c\u9a8c\u8bc1","text":""},{"location":"zh/dataset/custom/#_13","title":"\u5355\u5143\u6d4b\u8bd5","text":"<pre><code>import unittest\nfrom my_dataset import MyCustomDataset, MyDatasetConfig\n\nclass TestMyDataset(unittest.TestCase):\n    def setUp(self):\n        self.config = MyDatasetConfig(\n            root_dir=\"test_data\",\n            api_key=\"test_key\"\n        )\n        self.dataset = MyCustomDataset(self.config)\n\n    def test_data_loading(self):\n        \"\"\"\u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\"\"\"\n        # \u6a21\u62df\u6570\u636e\n        raw_data = self.dataset.load_raw_data()\n        self.assertIn(\"items\", raw_data)\n        self.assertIn(\"interactions\", raw_data)\n\n    def test_preprocessing(self):\n        \"\"\"\u6d4b\u8bd5\u9884\u5904\u7406\"\"\"\n        raw_data = {\"items\": pd.DataFrame(), \"interactions\": pd.DataFrame()}\n        processed = self.dataset.preprocess_data(raw_data)\n        self.assertIn(\"items\", processed)\n        self.assertIn(\"interactions\", processed)\n</code></pre>"},{"location":"zh/dataset/custom/#_14","title":"\u6570\u636e\u8d28\u91cf\u9a8c\u8bc1","text":"<pre><code>def validate_dataset(dataset):\n    \"\"\"\u9a8c\u8bc1\u6570\u636e\u96c6\u8d28\u91cf\"\"\"\n    # \u68c0\u67e5\u6570\u636e\u5b8c\u6574\u6027\n    assert len(dataset) &gt; 0, \"\u6570\u636e\u96c6\u4e3a\u7a7a\"\n\n    # \u68c0\u67e5\u7279\u5f81\u7ef4\u5ea6\n    sample = dataset[0]\n    assert len(sample) == 768, f\"\u7279\u5f81\u7ef4\u5ea6\u9519\u8bef: {len(sample)}\"\n\n    # \u68c0\u67e5\u6570\u636e\u7c7b\u578b\n    assert isinstance(sample, list), \"\u6570\u636e\u7c7b\u578b\u9519\u8bef\"\n\n    print(\"\u6570\u636e\u96c6\u9a8c\u8bc1\u901a\u8fc7\")\n</code></pre>"},{"location":"zh/dataset/custom/#_15","title":"\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"zh/dataset/custom/#1_1","title":"1. \u9519\u8bef\u5904\u7406","text":"<pre><code>def load_raw_data(self):\n    try:\n        # \u6570\u636e\u52a0\u8f7d\u903b\u8f91\n        return data\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"\u6570\u636e\u6587\u4ef6\u4e0d\u5b58\u5728: {self.data_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"\u6570\u636e\u52a0\u8f7d\u5931\u8d25: {str(e)}\")\n</code></pre>"},{"location":"zh/dataset/custom/#2_1","title":"2. \u65e5\u5fd7\u8bb0\u5f55","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef preprocess_data(self, raw_data):\n    logger.info(\"\u5f00\u59cb\u9884\u5904\u7406\u6570\u636e\")\n\n    # \u9884\u5904\u7406\u903b\u8f91\n\n    logger.info(f\"\u9884\u5904\u7406\u5b8c\u6210\uff0c\u7269\u54c1\u6570\u91cf: {len(items_df)}, \u4ea4\u4e92\u6570\u91cf: {len(interactions_df)}\")\n</code></pre>"},{"location":"zh/dataset/custom/#3_1","title":"3. \u914d\u7f6e\u9a8c\u8bc1","text":"<pre><code>def __post_init__(self):\n    super().__post_init__()\n\n    if not self.api_key:\n        raise ValueError(\"API key \u4e0d\u80fd\u4e3a\u7a7a\")\n\n    if self.data_format not in [\"json\", \"csv\"]:\n        raise ValueError(f\"\u4e0d\u652f\u6301\u7684\u6570\u636e\u683c\u5f0f: {self.data_format}\")\n</code></pre>"},{"location":"zh/dataset/custom/#movielens","title":"\u793a\u4f8b\uff1aMovieLens \u6570\u636e\u96c6","text":"<p>\u5b8c\u6574\u7684 MovieLens \u6570\u636e\u96c6\u5b9e\u73b0\u793a\u4f8b\uff1a</p> <pre><code>@dataclass\nclass MovieLensConfig(DatasetConfig):\n    \"\"\"MovieLens \u6570\u636e\u96c6\u914d\u7f6e\"\"\"\n    version: str = \"1m\"  # 1m, 10m, 20m\n\nclass MovieLensDataset(BaseRecommenderDataset):\n    \"\"\"MovieLens \u6570\u636e\u96c6\u5b9e\u73b0\"\"\"\n\n    URLS = {\n        \"1m\": \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\",\n        \"10m\": \"http://files.grouplens.org/datasets/movielens/ml-10m.zip\",\n    }\n\n    def download(self):\n        if self._data_exists():\n            return\n\n        url = self.URLS[self.config.version]\n        # \u4e0b\u8f7d\u548c\u89e3\u538b\u903b\u8f91\n\n    def load_raw_data(self):\n        # \u52a0\u8f7d ratings.dat, movies.dat, users.dat\n        pass\n\n    def preprocess_data(self, raw_data):\n        # MovieLens \u7279\u5b9a\u7684\u9884\u5904\u7406\n        pass\n</code></pre> <p>\u901a\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u60a8\u53ef\u4ee5\u6210\u529f\u4e3a GenerativeRecommenders \u6846\u67b6\u6dfb\u52a0\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u652f\u6301\u3002</p>"},{"location":"zh/dataset/overview/","title":"\u6570\u636e\u96c6\u6982\u8ff0","text":"<p>GenerativeRecommenders \u6846\u67b6\u652f\u6301\u591a\u79cd\u63a8\u8350\u7cfb\u7edf\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\u3002</p>"},{"location":"zh/dataset/overview/#_2","title":"\u652f\u6301\u7684\u6570\u636e\u96c6","text":""},{"location":"zh/dataset/overview/#p5-amazon","title":"P5 Amazon \u6570\u636e\u96c6","text":"<p>P5 Amazon \u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u4ea7\u54c1\u63a8\u8350\u6570\u636e\u96c6\uff0c\u5305\u542b\u7528\u6237\u8bc4\u8bba\u548c\u4ea7\u54c1\u5143\u6570\u636e\u3002</p> <p>\u7279\u70b9: - \u591a\u4e2a\u4ea7\u54c1\u7c7b\u522b\uff08\u7f8e\u5bb9\u3001\u7535\u5b50\u4ea7\u54c1\u7b49\uff09 - \u4e30\u5bcc\u7684\u6587\u672c\u4fe1\u606f\uff08\u6807\u9898\u3001\u54c1\u724c\u3001\u7c7b\u522b\u3001\u4ef7\u683c\uff09 - \u7528\u6237\u4ea4\u4e92\u5e8f\u5217\u6570\u636e - \u81ea\u52a8\u4e0b\u8f7d\u548c\u9884\u5904\u7406</p> <p>\u4f7f\u7528\u65b9\u6cd5: <pre><code>from generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n</code></pre></p>"},{"location":"zh/dataset/overview/#_3","title":"\u6570\u636e\u5904\u7406\u67b6\u6784","text":""},{"location":"zh/dataset/overview/#_4","title":"\u6a21\u5757\u5316\u8bbe\u8ba1","text":"<pre><code>graph TD\n    A[\u539f\u59cb\u6570\u636e] --&gt; B[\u6570\u636e\u4e0b\u8f7d\u5668]\n    B --&gt; C[\u9884\u5904\u7406\u5668]\n    C --&gt; D[\u6587\u672c\u5904\u7406\u5668]\n    C --&gt; E[\u5e8f\u5217\u5904\u7406\u5668]\n    D --&gt; F[\u7279\u5f81\u5d4c\u5165]\n    E --&gt; G[\u5e8f\u5217\u6570\u636e]\n    F --&gt; H[\u7269\u54c1\u6570\u636e\u96c6]\n    G --&gt; I[\u5e8f\u5217\u6570\u636e\u96c6]</code></pre>"},{"location":"zh/dataset/overview/#_5","title":"\u914d\u7f6e\u7cfb\u7edf","text":"<p>\u4f7f\u7528\u914d\u7f6e\u7c7b\u7ba1\u7406\u6570\u636e\u5904\u7406\u53c2\u6570\uff1a</p> <pre><code>from generative_recommenders.data.configs import P5AmazonConfig, TextEncodingConfig\n\nconfig = P5AmazonConfig(\n    root_dir=\"dataset/amazon\",\n    split=\"beauty\",\n    text_config=TextEncodingConfig(\n        encoder_model=\"sentence-transformers/sentence-t5-xl\",\n        template=\"Title: {title}; Brand: {brand}\"\n    )\n)\n</code></pre>"},{"location":"zh/dataset/overview/#_6","title":"\u6570\u636e\u5904\u7406\u6d41\u6c34\u7ebf","text":""},{"location":"zh/dataset/overview/#1","title":"1. \u6570\u636e\u4e0b\u8f7d","text":"<ul> <li>\u81ea\u52a8\u4ece\u4e91\u7aef\u4e0b\u8f7d\u6570\u636e\u96c6</li> <li>\u89e3\u538b\u548c\u6587\u4ef6\u7ec4\u7ec7</li> <li>\u9a8c\u8bc1\u6570\u636e\u5b8c\u6574\u6027</li> </ul>"},{"location":"zh/dataset/overview/#2","title":"2. \u9884\u5904\u7406","text":"<ul> <li>\u6570\u636e\u6e05\u6d17\u548c\u6807\u51c6\u5316</li> <li>\u7f3a\u5931\u503c\u5904\u7406</li> <li>ID \u6620\u5c04\u548c\u91cd\u7f16\u7801</li> </ul>"},{"location":"zh/dataset/overview/#3","title":"3. \u7279\u5f81\u63d0\u53d6","text":"<ul> <li>\u6587\u672c\u7279\u5f81\u7f16\u7801</li> <li>\u7c7b\u522b\u7279\u5f81\u5904\u7406</li> <li>\u6570\u503c\u7279\u5f81\u6807\u51c6\u5316</li> </ul>"},{"location":"zh/dataset/overview/#4","title":"4. \u5e8f\u5217\u751f\u6210","text":"<ul> <li>\u7528\u6237\u4ea4\u4e92\u5e8f\u5217\u6784\u5efa</li> <li>\u65f6\u5e8f\u5206\u5272\uff08\u8bad\u7ec3/\u9a8c\u8bc1/\u6d4b\u8bd5\uff09</li> <li>\u5e8f\u5217\u586b\u5145\u548c\u622a\u65ad</li> </ul>"},{"location":"zh/dataset/overview/#_7","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6","text":"<p>\u8981\u6dfb\u52a0\u65b0\u7684\u6570\u636e\u96c6\uff0c\u9700\u8981\u7ee7\u627f\u57fa\u7c7b\u5e76\u5b9e\u73b0\u5fc5\u8981\u65b9\u6cd5\uff1a</p> <pre><code>from generative_recommenders.data.base_dataset import BaseRecommenderDataset\n\nclass MyDataset(BaseRecommenderDataset):\n    def download(self):\n        # \u5b9e\u73b0\u6570\u636e\u4e0b\u8f7d\u903b\u8f91\n        pass\n\n    def load_raw_data(self):\n        # \u52a0\u8f7d\u539f\u59cb\u6570\u636e\n        return {\"items\": items_df, \"interactions\": interactions_df}\n\n    def preprocess_data(self, raw_data):\n        # \u6570\u636e\u9884\u5904\u7406\n        return processed_data\n\n    def extract_items(self, processed_data):\n        return processed_data[\"items\"]\n\n    def extract_interactions(self, processed_data):\n        return processed_data[\"interactions\"]\n</code></pre>"},{"location":"zh/dataset/overview/#_8","title":"\u6027\u80fd\u4f18\u5316","text":""},{"location":"zh/dataset/overview/#_9","title":"\u7f13\u5b58\u673a\u5236","text":"<ul> <li>\u6587\u672c\u7f16\u7801\u7ed3\u679c\u7f13\u5b58</li> <li>\u9884\u5904\u7406\u6570\u636e\u7f13\u5b58  </li> <li>\u667a\u80fd\u7f13\u5b58\u5931\u6548</li> </ul>"},{"location":"zh/dataset/overview/#_10","title":"\u5185\u5b58\u7ba1\u7406","text":"<ul> <li>\u61d2\u52a0\u8f7d\u5927\u578b\u6570\u636e\u96c6</li> <li>\u6279\u91cf\u6570\u636e\u5904\u7406</li> <li>\u5185\u5b58\u4f7f\u7528\u76d1\u63a7</li> </ul>"},{"location":"zh/dataset/overview/#_11","title":"\u5e76\u884c\u5904\u7406","text":"<ul> <li>\u591a\u8fdb\u7a0b\u6570\u636e\u52a0\u8f7d</li> <li>\u5e76\u884c\u6587\u672c\u7f16\u7801</li> <li>\u5206\u5e03\u5f0f\u9884\u5904\u7406</li> </ul>"},{"location":"zh/dataset/overview/#_12","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u6570\u636e\u9a8c\u8bc1: \u59cb\u7ec8\u9a8c\u8bc1\u6570\u636e\u8d28\u91cf\u548c\u5b8c\u6574\u6027</li> <li>\u7248\u672c\u63a7\u5236: \u8ddf\u8e2a\u6570\u636e\u96c6\u7248\u672c\u548c\u53d8\u66f4</li> <li>\u6587\u6863\u8bb0\u5f55: \u8be6\u7ec6\u8bb0\u5f55\u6570\u636e\u5904\u7406\u6b65\u9aa4</li> <li>\u6027\u80fd\u76d1\u63a7: \u76d1\u63a7\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6027\u80fd</li> </ol>"},{"location":"zh/dataset/overview/#_13","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>Q: \u5982\u4f55\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1f A: \u4f7f\u7528\u5206\u5757\u52a0\u8f7d\u3001\u7f13\u5b58\u548c\u5e76\u884c\u5904\u7406</p> <p>Q: \u5982\u4f55\u6dfb\u52a0\u65b0\u7684\u6587\u672c\u7f16\u7801\u5668\uff1f A: \u7ee7\u627f TextProcessor \u7c7b\u5e76\u5b9e\u73b0\u81ea\u5b9a\u4e49\u7f16\u7801\u903b\u8f91</p> <p>Q: \u5982\u4f55\u5904\u7406\u4e0d\u540c\u683c\u5f0f\u7684\u6570\u636e\uff1f A: \u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u52a0\u8f7d\u5668\u548c\u9884\u5904\u7406\u7ba1\u9053</p>"},{"location":"zh/dataset/p5-amazon/","title":"P5 Amazon \u6570\u636e\u96c6","text":"<p>P5 Amazon \u6570\u636e\u96c6\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u4ea7\u54c1\u63a8\u8350\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea Amazon \u7684\u7528\u6237\u8bc4\u8bba\u548c\u4ea7\u54c1\u5143\u6570\u636e\u3002</p>"},{"location":"zh/dataset/p5-amazon/#_1","title":"\u6982\u8ff0","text":"<p>P5 Amazon \u6765\u6e90\u4e8e Amazon \u4ea7\u54c1\u6570\u636e\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u4ea7\u54c1\u6587\u672c\u4fe1\u606f\u4ee5\u53ca\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u3002\u5b83\u4e13\u95e8\u4e3a\u8bad\u7ec3\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u800c\u8bbe\u8ba1\u3002</p> <p>\u4e3b\u8981\u7279\u70b9\uff1a - \u591a\u4e2a\u4ea7\u54c1\u7c7b\u522b\uff08\u7f8e\u5bb9\u3001\u7535\u5b50\u4ea7\u54c1\u3001\u8fd0\u52a8\u7b49\uff09 - \u4e30\u5bcc\u7684\u4ea7\u54c1\u5143\u6570\u636e\uff08\u6807\u9898\u3001\u54c1\u724c\u3001\u7c7b\u522b\u3001\u4ef7\u683c\u3001\u63cf\u8ff0\uff09 - \u5e26\u65f6\u95f4\u6233\u7684\u7528\u6237\u4ea4\u4e92\u5e8f\u5217 - \u9002\u5408\u795e\u7ecf\u6a21\u578b\u7684\u9884\u5904\u7406\u6587\u672c\u7279\u5f81</p>"},{"location":"zh/dataset/p5-amazon/#_2","title":"\u6570\u636e\u96c6\u7ed3\u6784","text":""},{"location":"zh/dataset/p5-amazon/#_3","title":"\u6570\u636e\u6587\u4ef6","text":"<p>\u4e0b\u8f7d\u540e\uff0c\u6570\u636e\u96c6\u5305\u542b\uff1a <pre><code>dataset/amazon/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 beauty_5.json.gz          # \u539f\u59cb\u4ea4\u4e92\u6570\u636e\n\u2502   \u251c\u2500\u2500 meta_beauty.json.gz       # \u4ea7\u54c1\u5143\u6570\u636e\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 processed/\n\u2502   \u251c\u2500\u2500 items.parquet             # \u5904\u7406\u540e\u7684\u7269\u54c1\u7279\u5f81\n\u2502   \u251c\u2500\u2500 interactions.parquet      # \u5904\u7406\u540e\u7684\u4ea4\u4e92\u6570\u636e\n\u2502   \u2514\u2500\u2500 mappings.pkl              # ID \u6620\u5c04\n\u2514\u2500\u2500 cache/\n    \u2514\u2500\u2500 text_embeddings/          # \u7f13\u5b58\u7684\u6587\u672c\u5d4c\u5165\n</code></pre></p>"},{"location":"zh/dataset/p5-amazon/#_4","title":"\u6570\u636e\u683c\u5f0f","text":"<p>\u7269\u54c1\u6570\u636e\u6846\uff1a | \u5217\u540d | \u7c7b\u578b | \u63cf\u8ff0 | |------|------|------| | item_id | int | \u552f\u4e00\u7269\u54c1\u6807\u8bc6\u7b26 | | title | str | \u4ea7\u54c1\u6807\u9898 | | brand | str | \u4ea7\u54c1\u54c1\u724c | | category | str | \u4ea7\u54c1\u7c7b\u522b | | price | float | \u4ea7\u54c1\u4ef7\u683c | | features | List[float] | \u6587\u672c\u5d4c\u5165\u7279\u5f81\uff08768\u7ef4\uff09 | | text | str | \u683c\u5f0f\u5316\u6587\u672c\uff08\u7528\u4e8e\u53c2\u8003\uff09 |</p> <p>\u4ea4\u4e92\u6570\u636e\u6846\uff1a | \u5217\u540d | \u7c7b\u578b | \u63cf\u8ff0 | |------|------|------| | user_id | int | \u552f\u4e00\u7528\u6237\u6807\u8bc6\u7b26 | | item_id | int | \u7269\u54c1\u6807\u8bc6\u7b26 | | rating | float | \u7528\u6237\u8bc4\u5206\uff081-5\uff09 | | timestamp | int | \u4ea4\u4e92\u65f6\u95f4\u6233 |</p>"},{"location":"zh/dataset/p5-amazon/#_5","title":"\u53ef\u7528\u7c7b\u522b","text":""},{"location":"zh/dataset/p5-amazon/#beauty","title":"\u7f8e\u5bb9 (Beauty)","text":"<ul> <li>\u5927\u5c0f: ~52K \u4ea7\u54c1\uff0c~1.2M \u4ea4\u4e92</li> <li>\u63cf\u8ff0: \u5316\u5986\u54c1\u3001\u62a4\u80a4\u54c1\u3001\u62a4\u53d1\u4ea7\u54c1</li> <li>\u7279\u70b9: \u4e30\u5bcc\u7684\u54c1\u724c\u548c\u7c7b\u522b\u4fe1\u606f</li> </ul>"},{"location":"zh/dataset/p5-amazon/#electronics","title":"\u7535\u5b50\u4ea7\u54c1 (Electronics)","text":"<ul> <li>\u5927\u5c0f: ~63K \u4ea7\u54c1\uff0c~1.7M \u4ea4\u4e92</li> <li>\u63cf\u8ff0: \u7535\u5b50\u8bbe\u5907\u3001\u914d\u4ef6\u3001\u5c0f\u5de5\u5177</li> <li>\u7279\u70b9: \u63cf\u8ff0\u4e2d\u5305\u542b\u6280\u672f\u89c4\u683c</li> </ul>"},{"location":"zh/dataset/p5-amazon/#sports","title":"\u8fd0\u52a8 (Sports)","text":"<ul> <li>\u5927\u5c0f: ~35K \u4ea7\u54c1\uff0c~296K \u4ea4\u4e92</li> <li>\u63cf\u8ff0: \u8fd0\u52a8\u8bbe\u5907\u3001\u6237\u5916\u88c5\u5907\u3001\u5065\u8eab\u4ea7\u54c1</li> <li>\u7279\u70b9: \u6d3b\u52a8\u7279\u5b9a\u7684\u5206\u7c7b</li> </ul>"},{"location":"zh/dataset/p5-amazon/#_6","title":"\u6240\u6709\u7c7b\u522b","text":"<ul> <li>\u603b\u8ba1: 29 \u4e2a\u53ef\u7528\u7c7b\u522b</li> <li>\u5408\u5e76\u5927\u5c0f: \u591a GB \u6570\u636e\u96c6</li> <li>\u7528\u9014: \u5927\u89c4\u6a21\u5b9e\u9a8c</li> </ul>"},{"location":"zh/dataset/p5-amazon/#_7","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"zh/dataset/p5-amazon/#_8","title":"\u57fa\u672c\u52a0\u8f7d","text":"<pre><code>from generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\n# \u52a0\u8f7d\u7f8e\u5bb9\u7c7b\u522b\u7528\u4e8e\u7269\u54c1\u7ea7\u8bad\u7ec3\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n\nprint(f\"\u6570\u636e\u96c6\u5927\u5c0f: {len(dataset)}\")\nprint(f\"\u7279\u5f81\u7ef4\u5ea6: {dataset[0].shape}\")\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_9","title":"\u5e8f\u5217\u6570\u636e","text":"<pre><code>from generative_recommenders.data.p5_amazon import P5AmazonSequenceDataset\n\n# \u52a0\u8f7d\u7528\u4e8e\u5e8f\u5217\u5efa\u6a21\uff08\u9700\u8981\u9884\u8bad\u7ec3\u7684 RQVAE\uff09\nseq_dataset = P5AmazonSequenceDataset(\n    root=\"dataset/amazon\", \n    split=\"beauty\",\n    train_test_split=\"train\",\n    pretrained_rqvae_path=\"checkpoints/rqvae_beauty.ckpt\"\n)\n\n# \u83b7\u53d6\u6837\u672c\u5e8f\u5217\nsample = seq_dataset[0]\nprint(f\"\u8f93\u5165\u5e8f\u5217: {sample['input_ids']}\")\nprint(f\"\u76ee\u6807\u5e8f\u5217: {sample['labels']}\")\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_10","title":"\u914d\u7f6e\u9009\u9879","text":"<pre><code>from generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\",\n\n    # \u6587\u672c\u7f16\u7801\u9009\u9879\n    encoder_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n    force_reload=False,  # \u5982\u679c\u53ef\u7528\uff0c\u4f7f\u7528\u7f13\u5b58\u7684\u5d4c\u5165\n\n    # \u6570\u636e\u8fc7\u6ee4\n    min_user_interactions=5,\n    min_item_interactions=5,\n\n    # \u6587\u672c\u6a21\u677f\n    text_template=\"\u6807\u9898: {title}; \u54c1\u724c: {brand}; \u7c7b\u522b: {category}\"\n)\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_11","title":"\u6570\u636e\u5904\u7406\u6d41\u6c34\u7ebf","text":""},{"location":"zh/dataset/p5-amazon/#1","title":"1. \u4e0b\u8f7d\u548c\u89e3\u538b","text":"<pre><code># \u4ece UCSD \u4ed3\u5e93\u81ea\u52a8\u4e0b\u8f7d\ndataset = P5AmazonItemDataset(root=\"dataset/amazon\", split=\"beauty\")\n# \u7f8e\u5bb9\u7c7b\u522b\u4e0b\u8f7d\u7ea6 500MB\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#2","title":"2. \u6587\u672c\u5904\u7406","text":"<p>\u6846\u67b6\u4f7f\u7528\u53e5\u5b50\u53d8\u6362\u5668\u81ea\u52a8\u5904\u7406\u4ea7\u54c1\u6587\u672c\uff1a</p> <pre><code># \u9ed8\u8ba4\u6587\u672c\u6a21\u677f\ntemplate = \"Title: {title}; Brand: {brand}; Category: {category}; Price: {price}\"\n\n# \u5904\u7406\u540e\u7684\u6587\u672c\u793a\u4f8b\n\"Title: Maybelline Mascara; Brand: Maybelline; Category: Beauty; Price: $8.99\"\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#3","title":"3. \u7279\u5f81\u63d0\u53d6","text":"<ul> <li>\u6587\u672c\u5d4c\u5165: \u6765\u81ea\u53e5\u5b50\u53d8\u6362\u5668\u7684 768 \u7ef4\u5411\u91cf</li> <li>\u7f13\u5b58: \u5d4c\u5165\u88ab\u7f13\u5b58\u4ee5\u52a0\u5feb\u540e\u7eed\u52a0\u8f7d</li> <li>\u6807\u51c6\u5316: \u9ed8\u8ba4\u5e94\u7528 L2 \u6807\u51c6\u5316</li> </ul>"},{"location":"zh/dataset/p5-amazon/#4","title":"4. \u5e8f\u5217\u6784\u5efa","text":"<p>\u5bf9\u4e8e TIGER \u8bad\u7ec3\uff0c\u4ea4\u4e92\u88ab\u8f6c\u6362\u4e3a\u5e8f\u5217\uff1a</p> <pre><code># \u7528\u6237\u4ea4\u4e92\u5386\u53f2\nuser_history = [item1, item2, item3, item4]\n\n# \u4f7f\u7528 RQVAE \u8f6c\u6362\u4e3a\u8bed\u4e49 ID \u5e8f\u5217\nsemantic_sequence = [1, 45, 123, 67, 234, 189, 45, 123, 567, 234, 88, 192]\n#                   |--item1--| |--item2--| |--item3--| |--item4--|\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_12","title":"\u7edf\u8ba1\u4fe1\u606f","text":""},{"location":"zh/dataset/p5-amazon/#_13","title":"\u7f8e\u5bb9\u7c7b\u522b","text":"<pre><code>\u7269\u54c1\u6570: 52,024\n\u7528\u6237\u6570: 40,226  \n\u4ea4\u4e92\u6570: 1,235,316\n\u5bc6\u5ea6: 0.059%\n\u6bcf\u7528\u6237\u5e73\u5747\u7269\u54c1\u6570: 30.7\n\u6bcf\u7269\u54c1\u5e73\u5747\u7528\u6237\u6570: 23.7\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_14","title":"\u7535\u5b50\u4ea7\u54c1\u7c7b\u522b","text":"<pre><code>\u7269\u54c1\u6570: 63,001\n\u7528\u6237\u6570: 192,403\n\u4ea4\u4e92\u6570: 1,689,188\n\u5bc6\u5ea6: 0.014%\n\u6bcf\u7528\u6237\u5e73\u5747\u7269\u54c1\u6570: 8.8\n\u6bcf\u7269\u54c1\u5e73\u5747\u7528\u6237\u6570: 26.8\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_15","title":"\u6570\u636e\u8d28\u91cf","text":""},{"location":"zh/dataset/p5-amazon/#_16","title":"\u9884\u5904\u7406\u6b65\u9aa4","text":"<ol> <li>\u53bb\u91cd: \u79fb\u9664\u91cd\u590d\u7684\u7528\u6237-\u7269\u54c1\u4ea4\u4e92</li> <li>\u4f4e\u6d3b\u8dc3\u5ea6\u8fc7\u6ee4: \u8fc7\u6ee4\u4ea4\u4e92\u5c11\u4e8e 5 \u6b21\u7684\u7528\u6237/\u7269\u54c1</li> <li>\u6587\u672c\u6e05\u6d17: \u6807\u51c6\u5316\u6807\u9898\uff0c\u5904\u7406\u7f3a\u5931\u7684\u54c1\u724c/\u7c7b\u522b</li> <li>\u4ef7\u683c\u5904\u7406: \u6e05\u6d17\u548c\u6807\u51c6\u5316\u4ef7\u683c\u683c\u5f0f</li> <li>ID \u91cd\u6620\u5c04: \u521b\u5efa\u8fde\u7eed\u7684 ID \u6620\u5c04</li> </ol>"},{"location":"zh/dataset/p5-amazon/#_17","title":"\u8d28\u91cf\u68c0\u67e5","text":"<pre><code>from generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\ndataset = P5AmazonItemDataset(root=\"dataset/amazon\", split=\"beauty\")\n\n# \u68c0\u67e5\u6570\u636e\u8d28\u91cf\nitems_df, interactions_df = dataset.base_dataset.get_dataset()\n\nprint(\"\u6570\u636e\u8d28\u91cf\u62a5\u544a:\")\nprint(f\"\u7f3a\u5c11\u6807\u9898\u7684\u7269\u54c1: {items_df['title'].isna().sum()}\")\nprint(f\"\u7f3a\u5c11\u54c1\u724c\u7684\u7269\u54c1: {items_df['brand'].isna().sum()}\")\nprint(f\"\u6709\u6548\u8bc4\u5206\u7684\u4ea4\u4e92: {(interactions_df['rating'] &gt; 0).sum()}\")\nprint(f\"\u7279\u5f81\u5411\u91cf\u7ef4\u5ea6: {len(items_df.iloc[0]['features'])}\")\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_18","title":"\u9ad8\u7ea7\u7528\u6cd5","text":""},{"location":"zh/dataset/p5-amazon/#_19","title":"\u81ea\u5b9a\u4e49\u6587\u672c\u6a21\u677f","text":"<pre><code># \u4ee5\u4ea7\u54c1\u4e3a\u4e2d\u5fc3\u7684\u6a21\u677f\ntemplate = \"\u4ea7\u54c1: {brand} \u7684 {title}\uff0c\u5c5e\u4e8e {category} \u7c7b\u522b\"\n\n# \u4ef7\u683c\u611f\u77e5\u6a21\u677f  \ntemplate = \"\u8d2d\u4e70 {brand} \u7684 {title}\uff0c\u4ef7\u683c ${price}\uff0c\u5c5e\u4e8e {category}\"\n\n# \u7b80\u5316\u6a21\u677f\ntemplate = \"{title} - {brand}\"\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_20","title":"\u6279\u5904\u7406","text":"<pre><code>from torch.utils.data import DataLoader\n\ndataset = P5AmazonItemDataset(root=\"dataset/amazon\", split=\"beauty\")\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n\nfor batch in dataloader:\n    # batch \u5f62\u72b6: (128, 768)\n    features = batch\n    # \u5904\u7406\u6279\u6b21...\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_21","title":"\u591a\u7c7b\u522b\u52a0\u8f7d","text":"<pre><code># \u52a0\u8f7d\u591a\u4e2a\u7c7b\u522b\ncategories = [\"beauty\", \"electronics\", \"sports\"]\ndatasets = []\n\nfor category in categories:\n    dataset = P5AmazonItemDataset(\n        root=\"dataset/amazon\",\n        split=category,\n        train_test_split=\"train\"\n    )\n    datasets.append(dataset)\n\n# \u5408\u5e76\u6570\u636e\u96c6\nfrom torch.utils.data import ConcatDataset\ncombined_dataset = ConcatDataset(datasets)\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_22","title":"\u6545\u969c\u6392\u9664","text":""},{"location":"zh/dataset/p5-amazon/#_23","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>Q: \u4e0b\u8f7d\u5931\u8d25\uff0c\u51fa\u73b0\u7f51\u7edc\u9519\u8bef A: \u68c0\u67e5\u7f51\u7edc\u8fde\u63a5\u5e76\u91cd\u8bd5\u3002\u6587\u4ef6\u5f88\u5927\uff08100MB-2GB\uff09\u3002</p> <p>Q: \u6587\u672c\u7f16\u7801\u8017\u65f6\u5f88\u957f A: \u8bbe\u7f6e <code>force_reload=False</code> \u4f7f\u7528\u7f13\u5b58\u7684\u5d4c\u5165\uff0c\u5e76\u786e\u4fdd\u7f13\u5b58\u76ee\u5f55\u53ef\u5199\u3002</p> <p>Q: \u52a0\u8f7d\u65f6\u5185\u5b58\u4e0d\u8db3 A: \u51cf\u5c11\u6279\u6b21\u5927\u5c0f\u6216\u4f7f\u7528\u8f83\u5c0f\u7684\u7c7b\u522b\u5982 \"beauty\" \u800c\u4e0d\u662f \"all\"\u3002</p> <p>Q: \u7f3a\u5c11\u54c1\u724c/\u7c7b\u522b\u4fe1\u606f A: \u8fd9\u662f\u6b63\u5e38\u7684 - \u6570\u636e\u96c6\u7528 \"Unknown\" \u586b\u5145\u7f3a\u5931\u503c\u3002</p>"},{"location":"zh/dataset/p5-amazon/#_24","title":"\u6027\u80fd\u63d0\u793a","text":"<pre><code># \u4f7f\u7528\u7f13\u5b58\u52a0\u5feb\u540e\u7eed\u52a0\u8f7d\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\", \n    force_reload=False  # \u4f7f\u7528\u7f13\u5b58\n)\n\n# \u4f7f\u7528\u66f4\u8f7b\u7684\u6587\u672c\u7f16\u7801\u5668\u52a0\u5feb\u5904\u7406\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    encoder_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # \u66f4\u5c0f\u7684\u6a21\u578b\n)\n\n# \u4f7f\u7528\u66f4\u5c0f\u7684\u6279\u6b21\u5904\u7406\nfrom generative_recommenders.data.configs import TextEncodingConfig\ntext_config = TextEncodingConfig(batch_size=16)  # \u4ece\u9ed8\u8ba4\u7684 32 \u51cf\u5c11\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_25","title":"\u5f15\u7528","text":"<p>\u5982\u679c\u60a8\u4f7f\u7528 P5 Amazon \u6570\u636e\u96c6\uff0c\u8bf7\u5f15\u7528\uff1a</p> <pre><code>@article{geng2022recommendation,\n  title={Recommendation as language processing (rlp): A unified pretrain, personalized prompt \\&amp; predict paradigm (p5)},\n  author={Geng, Shijie and Liu, Shuchang and Fu, Zuohui and Ge, Yingqiang and Zhang, Yongfeng},\n  journal={arXiv preprint arXiv:2203.13366},\n  year={2022}\n}\n</code></pre>"},{"location":"zh/dataset/p5-amazon/#_26","title":"\u76f8\u5173\u6587\u6863","text":"<ul> <li>\u6570\u636e\u96c6\u6982\u8ff0 - \u901a\u7528\u6570\u636e\u96c6\u6982\u5ff5</li> <li>\u81ea\u5b9a\u4e49\u6570\u636e\u96c6 - \u521b\u5efa\u60a8\u81ea\u5df1\u7684\u6570\u636e\u96c6  </li> <li>RQVAE \u8bad\u7ec3 - \u8bad\u7ec3\u7269\u54c1\u7f16\u7801\u5668</li> <li>TIGER \u8bad\u7ec3 - \u8bad\u7ec3\u5e8f\u5217\u6a21\u578b</li> </ul>"},{"location":"zh/models/rqvae/","title":"RQVAE \u6a21\u578b","text":"<p>RQVAE (Residual Quantized Variational Autoencoder) \u662f\u4e00\u79cd\u57fa\u4e8e\u5411\u91cf\u91cf\u5316\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u4e13\u95e8\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u7269\u54c1\u8868\u793a\u5b66\u4e60\u3002</p>"},{"location":"zh/models/rqvae/#_1","title":"\u6a21\u578b\u67b6\u6784","text":""},{"location":"zh/models/rqvae/#_2","title":"\u6838\u5fc3\u7ec4\u4ef6","text":"<pre><code>graph TD\n    A[\u8f93\u5165\u7279\u5f81] --&gt; B[\u7f16\u7801\u5668]\n    B --&gt; C[\u91cf\u5316\u5c42]\n    C --&gt; D[\u89e3\u7801\u5668]\n    D --&gt; E[\u91cd\u6784\u8f93\u51fa]\n    C --&gt; F[\u8bed\u4e49 ID]</code></pre>"},{"location":"zh/models/rqvae/#encoder","title":"\u7f16\u7801\u5668 (Encoder)","text":"<p>\u7f16\u7801\u5668\u5c06\u539f\u59cb\u7269\u54c1\u7279\u5f81\uff08\u5982\u6587\u672c\u5d4c\u5165\uff09\u6620\u5c04\u5230\u6f5c\u5728\u7a7a\u95f4\uff1a</p> <pre><code>class Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dims, embed_dim):\n        self.layers = nn.ModuleList([\n            nn.Linear(input_dim, hidden_dims[0]),\n            *[nn.Linear(hidden_dims[i], hidden_dims[i+1]) \n              for i in range(len(hidden_dims)-1)],\n            nn.Linear(hidden_dims[-1], embed_dim)\n        ])\n\n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = F.relu(layer(x))\n        return self.layers[-1](x)\n</code></pre>"},{"location":"zh/models/rqvae/#quantization","title":"\u91cf\u5316\u5c42 (Quantization)","text":"<p>\u91cf\u5316\u5c42\u662f RQVAE \u7684\u6838\u5fc3\uff0c\u5c06\u8fde\u7eed\u7684\u5d4c\u5165\u5411\u91cf\u6620\u5c04\u5230\u79bb\u6563\u7684\u7801\u672c\uff1a</p>"},{"location":"zh/models/rqvae/#_3","title":"\u652f\u6301\u7684\u91cf\u5316\u7b56\u7565","text":"<ol> <li>Gumbel-Softmax: \u53ef\u5fae\u5206\u7684\u79bb\u6563\u5316</li> <li>STE (Straight-Through Estimator): \u76f4\u901a\u4f30\u8ba1\u5668</li> <li>Rotation Trick: \u65cb\u8f6c\u6280\u5de7</li> <li>Sinkhorn: Sinkhorn \u8fed\u4ee3\u7b97\u6cd5</li> </ol> <pre><code>@gin.constants_from_enum\nclass QuantizeForwardMode(Enum):\n    GUMBEL_SOFTMAX = 1\n    STE = 2\n    ROTATION_TRICK = 3\n    SINKHORN = 4\n</code></pre>"},{"location":"zh/models/rqvae/#decoder","title":"\u89e3\u7801\u5668 (Decoder)","text":"<p>\u89e3\u7801\u5668\u5c06\u91cf\u5316\u540e\u7684\u8868\u793a\u91cd\u6784\u56de\u539f\u59cb\u7279\u5f81\u7a7a\u95f4\uff1a</p> <pre><code>class Decoder(nn.Module):\n    def __init__(self, embed_dim, hidden_dims, output_dim):\n        reversed_dims = list(reversed(hidden_dims))\n        self.layers = nn.ModuleList([\n            nn.Linear(embed_dim, reversed_dims[0]),\n            *[nn.Linear(reversed_dims[i], reversed_dims[i+1]) \n              for i in range(len(reversed_dims)-1)],\n            nn.Linear(reversed_dims[-1], output_dim)\n        ])\n\n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = F.relu(layer(x))\n        return self.layers[-1](x)\n</code></pre>"},{"location":"zh/models/rqvae/#_4","title":"\u8bad\u7ec3\u8fc7\u7a0b","text":""},{"location":"zh/models/rqvae/#_5","title":"\u635f\u5931\u51fd\u6570","text":"<p>RQVAE \u7684\u8bad\u7ec3\u76ee\u6807\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1a</p> <ol> <li>\u91cd\u6784\u635f\u5931 (Reconstruction Loss)</li> <li>\u91cf\u5316\u635f\u5931 (Quantization Loss)  </li> <li>\u627f\u8bfa\u635f\u5931 (Commitment Loss)</li> </ol> <pre><code>total_loss = reconstruction_loss + \u03b2 * quantization_loss + \u03b3 * commitment_loss\n</code></pre>"},{"location":"zh/models/rqvae/#_6","title":"\u914d\u7f6e\u53c2\u6570","text":"<pre><code># \u6a21\u578b\u67b6\u6784\ntrain.vae_input_dim=768        # \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6\ntrain.vae_embed_dim=32         # \u5d4c\u5165\u7ef4\u5ea6\ntrain.vae_hidden_dims=[512, 256, 128]  # \u9690\u85cf\u5c42\u7ef4\u5ea6\ntrain.vae_codebook_size=256    # \u7801\u672c\u5927\u5c0f\ntrain.vae_n_layers=3           # \u91cf\u5316\u5c42\u6570\n\n# \u91cf\u5316\u8bbe\u7f6e\ntrain.vae_codebook_mode=%generative_recommenders.models.rqvae.QuantizeForwardMode.ROTATION_TRICK\ntrain.vae_codebook_normalize=False\ntrain.vae_sim_vq=False\n\n# \u635f\u5931\u6743\u91cd\ntrain.commitment_weight=0.25   # \u627f\u8bfa\u635f\u5931\u6743\u91cd\n</code></pre>"},{"location":"zh/models/rqvae/#_7","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"zh/models/rqvae/#_8","title":"\u521b\u5efa\u6a21\u578b","text":"<pre><code>from generative_recommenders.models.rqvae import RqVae, QuantizeForwardMode\n\nmodel = RqVae(\n    input_dim=768,\n    embed_dim=32,\n    hidden_dims=[512, 256, 128],\n    codebook_size=256,\n    codebook_normalize=False,\n    codebook_sim_vq=False,\n    n_layers=3,\n    n_cat_features=0,\n    commitment_weight=0.25,\n    codebook_mode=QuantizeForwardMode.ROTATION_TRICK\n)\n</code></pre>"},{"location":"zh/models/rqvae/#_9","title":"\u8bad\u7ec3\u6a21\u578b","text":"<pre><code>from generative_recommenders.data.p5_amazon import P5AmazonItemDataset\nfrom torch.utils.data import DataLoader\n\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    split=\"beauty\",\n    train_test_split=\"train\"\n)\n\n# \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n\n# \u8bad\u7ec3\u5faa\u73af\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        optimizer.zero_grad()\n\n        # \u524d\u5411\u4f20\u64ad\n        outputs = model(batch)\n        loss = outputs.loss\n\n        # \u53cd\u5411\u4f20\u64ad\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"zh/models/rqvae/#id","title":"\u751f\u6210\u8bed\u4e49 ID","text":"<pre><code># \u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\nmodel = RqVae.load_from_checkpoint(\"checkpoint.pt\")\nmodel.eval()\n\n# \u751f\u6210\u8bed\u4e49 ID\nwith torch.no_grad():\n    semantic_ids = model.get_semantic_ids(item_features)\n    print(f\"Semantic IDs shape: {semantic_ids.shape}\")\n</code></pre>"},{"location":"zh/models/rqvae/#\u8bc4\u4f30\u6307\u6807","title":"\u8bc4\u4f30\u6307\u6807","text":""},{"location":"zh/models/rqvae/#_10","title":"\u91cd\u6784\u8d28\u91cf","text":"<pre><code>def evaluate_reconstruction(model, test_dataloader):\n    model.eval()\n    total_loss = 0\n    num_samples = 0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            outputs = model(batch)\n            total_loss += outputs.reconstruction_loss.item() * len(batch)\n            num_samples += len(batch)\n\n    return total_loss / num_samples\n</code></pre>"},{"location":"zh/models/rqvae/#_11","title":"\u7801\u672c\u5229\u7528\u7387","text":"<pre><code>def compute_codebook_usage(model, dataloader):\n    model.eval()\n    used_codes = set()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            semantic_ids = model.get_semantic_ids(batch)\n            used_codes.update(semantic_ids.flatten().tolist())\n\n    usage_rate = len(used_codes) / model.codebook_size\n    return usage_rate\n</code></pre>"},{"location":"zh/models/rqvae/#_12","title":"\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"zh/models/rqvae/#_13","title":"\u8d85\u53c2\u6570\u8c03\u4f18","text":"<ol> <li>\u5b66\u4e60\u7387: \u4ece 0.0005 \u5f00\u59cb\uff0c\u6839\u636e\u8bad\u7ec3\u7a33\u5b9a\u6027\u8c03\u6574</li> <li>\u6279\u91cf\u5927\u5c0f: 64-128 \u901a\u5e38\u6548\u679c\u8f83\u597d</li> <li>\u7801\u672c\u5927\u5c0f: 256-512\uff0c\u53d6\u51b3\u4e8e\u6570\u636e\u590d\u6742\u5ea6</li> <li>\u627f\u8bfa\u6743\u91cd: 0.25 \u662f\u4e00\u4e2a\u597d\u7684\u8d77\u59cb\u70b9</li> </ol>"},{"location":"zh/models/rqvae/#_14","title":"\u8bad\u7ec3\u6280\u5de7","text":"<ol> <li>\u9884\u8bad\u7ec3: \u53ef\u4ee5\u5148\u7528\u8f83\u5927\u5b66\u4e60\u7387\u8fdb\u884c\u9884\u8bad\u7ec3</li> <li>\u5b66\u4e60\u7387\u8c03\u5ea6: \u4f7f\u7528\u4f59\u5f26\u9000\u706b\u6216\u9636\u68af\u5f0f\u8870\u51cf</li> <li>\u65e9\u505c: \u76d1\u63a7\u9a8c\u8bc1\u96c6\u91cd\u6784\u635f\u5931\u8fdb\u884c\u65e9\u505c</li> <li>\u6b63\u5219\u5316: \u9002\u5f53\u7684\u6743\u91cd\u8870\u51cf\u9632\u6b62\u8fc7\u62df\u5408</li> </ol>"},{"location":"zh/models/rqvae/#_15","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>Q: \u7801\u672c\u5d29\u584c\u600e\u4e48\u529e\uff1f A:  - \u589e\u52a0 commitment_weight - \u4f7f\u7528 ROTATION_TRICK \u6a21\u5f0f - \u8c03\u6574\u5b66\u4e60\u7387</p> <p>Q: \u91cd\u6784\u8d28\u91cf\u5dee\uff1f A:  - \u589e\u52a0\u6a21\u578b\u5bb9\u91cf (hidden_dims) - \u8c03\u6574\u91cf\u5316\u7b56\u7565 - \u68c0\u67e5\u6570\u636e\u9884\u5904\u7406</p>"},{"location":"zh/models/rqvae/#_16","title":"\u6269\u5c55\u529f\u80fd","text":""},{"location":"zh/models/rqvae/#_17","title":"\u591a\u5c42\u91cf\u5316","text":"<pre><code># \u652f\u6301\u591a\u5c42\u91cf\u5316\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u8868\u793a\u80fd\u529b\ntrain.vae_n_layers=3  # \u4f7f\u7528 3 \u5c42\u91cf\u5316\n</code></pre>"},{"location":"zh/models/rqvae/#_18","title":"\u81ea\u5b9a\u4e49\u635f\u5931","text":"<pre><code>class CustomRqVae(RqVae):\n    def compute_loss(self, x, outputs):\n        # \u57fa\u7840\u635f\u5931\n        base_loss = super().compute_loss(x, outputs)\n\n        # \u6dfb\u52a0\u81ea\u5b9a\u4e49\u6b63\u5219\u5316\n        regularization = self.custom_regularization(outputs)\n\n        return base_loss + regularization\n</code></pre>"},{"location":"zh/models/rqvae/#_19","title":"\u76f8\u5173\u8bba\u6587","text":"<ul> <li>RQ-VAE Recommender</li> <li>Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation</li> </ul>"},{"location":"zh/models/tiger/","title":"TIGER \u6a21\u578b","text":"<p>TIGER (Recommender Systems with Generative Retrieval) \u662f\u4e00\u79cd\u57fa\u4e8e Transformer \u7684\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\uff0c\u901a\u8fc7\u5e8f\u5217\u5efa\u6a21\u7684\u65b9\u5f0f\u8fdb\u884c\u7269\u54c1\u63a8\u8350\u3002</p>"},{"location":"zh/models/tiger/#_1","title":"\u6a21\u578b\u67b6\u6784","text":""},{"location":"zh/models/tiger/#_2","title":"\u6838\u5fc3\u601d\u60f3","text":"<p>TIGER \u5c06\u63a8\u8350\u95ee\u9898\u8f6c\u5316\u4e3a\u5e8f\u5217\u751f\u6210\u95ee\u9898\uff1a - \u8f93\u5165\uff1a\u7528\u6237\u5386\u53f2\u4ea4\u4e92\u5e8f\u5217\uff08\u8bed\u4e49ID\u8868\u793a\uff09 - \u8f93\u51fa\uff1a\u63a8\u8350\u7269\u54c1\u7684\u8bed\u4e49ID\u5e8f\u5217</p> <pre><code>graph TD\n    A[\u7528\u6237\u5386\u53f2\u5e8f\u5217] --&gt; B[\u8bed\u4e49ID\u7f16\u7801]\n    B --&gt; C[Transformer\u7f16\u7801\u5668]\n    C --&gt; D[Transformer\u89e3\u7801\u5668]\n    D --&gt; E[\u751f\u6210\u63a8\u8350\u5e8f\u5217]\n    E --&gt; F[\u8bed\u4e49ID\u89e3\u7801]\n    F --&gt; G[\u63a8\u8350\u7269\u54c1]</code></pre>"},{"location":"zh/models/tiger/#_3","title":"\u6a21\u578b\u7ec4\u4ef6","text":""},{"location":"zh/models/tiger/#1","title":"1. \u5d4c\u5165\u5c42","text":"<pre><code>class SemIdEmbedding(nn.Module):\n    \"\"\"\u8bed\u4e49ID\u5d4c\u5165\u5c42\"\"\"\n    def __init__(self, num_embeddings, embedding_dim, sem_id_dim=3):\n        super().__init__()\n        self.sem_id_dim = sem_id_dim\n        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n\n    def forward(self, sem_ids):\n        # sem_ids: (batch_size, seq_len, sem_id_dim)\n        embedded = self.embedding(sem_ids)\n        return embedded.mean(dim=-2)  # \u805a\u5408\u8bed\u4e49ID\u7ef4\u5ea6\n</code></pre>"},{"location":"zh/models/tiger/#2-transformer","title":"2. Transformer \u67b6\u6784","text":"<pre><code>class TransformerEncoderDecoder(nn.Module):\n    \"\"\"Transformer \u7f16\u7801\u5668-\u89e3\u7801\u5668\"\"\"\n    def __init__(self, config):\n        super().__init__()\n\n        # \u7f16\u7801\u5668\n        encoder_layer = TransformerEncoderLayer(\n            d_model=config.embedding_dim,\n            nhead=config.num_heads,\n            dim_feedforward=config.attn_dim,\n            dropout=config.dropout\n        )\n        self.encoder = TransformerEncoder(encoder_layer, config.n_layers)\n\n        # \u89e3\u7801\u5668  \n        decoder_layer = TransformerDecoderLayer(\n            d_model=config.embedding_dim,\n            nhead=config.num_heads,\n            dim_feedforward=config.attn_dim,\n            dropout=config.dropout\n        )\n        self.decoder = TransformerDecoder(decoder_layer, config.n_layers)\n</code></pre>"},{"location":"zh/models/tiger/#3","title":"3. \u7ea6\u675f\u751f\u6210","text":"<p>TIGER \u4f7f\u7528 Trie \u6570\u636e\u7ed3\u6784\u7ea6\u675f\u751f\u6210\u8fc7\u7a0b\uff1a</p> <pre><code>class TrieNode(defaultdict):\n    \"\"\"Trie \u8282\u70b9\"\"\"\n    def __init__(self):\n        super().__init__(TrieNode)\n        self.is_end = False\n\ndef build_trie(valid_item_ids):\n    \"\"\"\u6784\u5efa\u6709\u6548\u7269\u54c1ID\u7684Trie\"\"\"\n    root = TrieNode()\n    for seq in valid_item_ids.tolist():\n        node = root\n        for token in seq:\n            node = node[token]\n        node.is_end = True\n    return root\n</code></pre>"},{"location":"zh/models/tiger/#id","title":"\u8bed\u4e49ID\u6620\u5c04","text":""},{"location":"zh/models/tiger/#id_1","title":"\u4ece\u7269\u54c1\u5230\u8bed\u4e49ID","text":"<p>TIGER \u4f7f\u7528\u9884\u8bad\u7ec3\u7684 RQVAE \u5c06\u7269\u54c1\u7279\u5f81\u6620\u5c04\u4e3a\u8bed\u4e49ID\uff1a</p> <pre><code># \u7269\u54c1\u7279\u5f81 -&gt; \u8bed\u4e49ID\nitem_features = torch.tensor([...])  # (768,)\nrqvae_output = rqvae(item_features)\nsemantic_ids = rqvae_output.sem_ids  # (3,) \u4e09\u4e2a\u8bed\u4e49ID\n</code></pre>"},{"location":"zh/models/tiger/#id_2","title":"\u8bed\u4e49ID\u5e8f\u5217","text":"<p>\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u8f6c\u6362\u4e3a\u8bed\u4e49ID\u5e8f\u5217\uff1a</p> <pre><code>user_history = [item1, item2, item3, ...]\nsemantic_sequence = []\n\nfor item_id in user_history:\n    item_sem_ids = rqvae.get_semantic_ids(item_features[item_id])\n    semantic_sequence.extend(item_sem_ids.tolist())\n\n# \u7ed3\u679c\uff1a[id1, id2, id3, id4, id5, id6, ...]\n</code></pre>"},{"location":"zh/models/tiger/#_4","title":"\u8bad\u7ec3\u8fc7\u7a0b","text":""},{"location":"zh/models/tiger/#_5","title":"\u6570\u636e\u51c6\u5907","text":"<pre><code>class SeqData(NamedTuple):\n    \"\"\"\u5e8f\u5217\u6570\u636e\u683c\u5f0f\"\"\"\n    user_id: int\n    item_ids: List[int]      # \u8f93\u5165\u5e8f\u5217\uff08\u8bed\u4e49ID\uff09\n    target_ids: List[int]    # \u76ee\u6807\u5e8f\u5217\uff08\u8bed\u4e49ID\uff09\n</code></pre>"},{"location":"zh/models/tiger/#_6","title":"\u635f\u5931\u51fd\u6570","text":"<p>\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u8fdb\u884c\u5e8f\u5217\u5efa\u6a21\uff1a</p> <pre><code>def compute_loss(logits, target_ids, mask):\n    \"\"\"\u8ba1\u7b97\u5e8f\u5217\u5efa\u6a21\u635f\u5931\"\"\"\n    # logits: (batch_size, seq_len, vocab_size)\n    # target_ids: (batch_size, seq_len)\n    # mask: (batch_size, seq_len)\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n\n    # \u91cd\u5851\u4e3a 2D\n    logits_flat = logits.view(-1, logits.size(-1))\n    target_flat = target_ids.view(-1)\n\n    # \u8ba1\u7b97\u635f\u5931\n    loss = loss_fn(logits_flat, target_flat)\n\n    return loss\n</code></pre>"},{"location":"zh/models/tiger/#_7","title":"\u8bad\u7ec3\u5faa\u73af","text":"<pre><code>def train_step(model, batch, optimizer):\n    \"\"\"\u5355\u6b65\u8bad\u7ec3\"\"\"\n    optimizer.zero_grad()\n\n    # \u524d\u5411\u4f20\u64ad\n    user_ids = batch[\"user_input_ids\"]\n    item_ids = batch[\"item_input_ids\"] \n    target_ids = batch[\"target_input_ids\"]\n\n    logits = model(user_ids, item_ids)\n\n    # \u8ba1\u7b97\u635f\u5931\n    loss = compute_loss(logits, target_ids, batch[\"seq_mask\"])\n\n    # \u53cd\u5411\u4f20\u64ad\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n</code></pre>"},{"location":"zh/models/tiger/#_8","title":"\u63a8\u7406\u8fc7\u7a0b","text":""},{"location":"zh/models/tiger/#_9","title":"\u751f\u6210\u5f0f\u63a8\u8350","text":"<pre><code>def generate_recommendations(model, user_sequence, max_length=10):\n    \"\"\"\u751f\u6210\u63a8\u8350\u5e8f\u5217\"\"\"\n    model.eval()\n\n    with torch.no_grad():\n        # \u7f16\u7801\u7528\u6237\u5e8f\u5217\n        encoded = model.encode_sequence(user_sequence)\n\n        # \u81ea\u56de\u5f52\u751f\u6210\n        generated = []\n        current_input = encoded\n\n        for _ in range(max_length):\n            # \u9884\u6d4b\u4e0b\u4e00\u4e2atoken\n            logits = model.decode_step(current_input)\n            next_token = torch.argmax(logits, dim=-1)\n\n            generated.append(next_token.item())\n\n            # \u66f4\u65b0\u8f93\u5165\n            current_input = torch.cat([current_input, next_token.unsqueeze(0)])\n\n    return generated\n</code></pre>"},{"location":"zh/models/tiger/#trie","title":"Trie\u7ea6\u675f\u751f\u6210","text":"<pre><code>def generate_with_trie_constraint(model, user_sequence, trie, max_length=10):\n    \"\"\"\u4f7f\u7528Trie\u7ea6\u675f\u7684\u751f\u6210\"\"\"\n    model.eval()\n\n    generated = []\n    current_node = trie\n\n    with torch.no_grad():\n        for step in range(max_length):\n            # \u83b7\u53d6\u6709\u6548\u7684\u4e0b\u4e00\u4e2atokens\n            valid_tokens = list(current_node.keys())\n            if not valid_tokens:\n                break\n\n            # \u9884\u6d4b\u5e76\u7ea6\u675f\n            logits = model.decode_step(user_sequence + generated)\n            masked_logits = mask_invalid_tokens(logits, valid_tokens)\n\n            next_token = torch.argmax(masked_logits, dim=-1).item()\n            generated.append(next_token)\n\n            # \u66f4\u65b0Trie\u4f4d\u7f6e\n            current_node = current_node[next_token]\n\n            # \u68c0\u67e5\u662f\u5426\u662f\u5b8c\u6574\u7684\u7269\u54c1ID\n            if current_node.is_end:\n                break\n\n    return generated\n</code></pre>"},{"location":"zh/models/tiger/#\u8bc4\u4f30\u6307\u6807","title":"\u8bc4\u4f30\u6307\u6807","text":""},{"location":"zh/models/tiger/#top-k","title":"Top-K \u63a8\u8350\u6307\u6807","text":"<pre><code>def compute_recall_at_k(predictions, targets, k=10):\n    \"\"\"\u8ba1\u7b97 Recall@K\"\"\"\n    recall_scores = []\n\n    for pred, target in zip(predictions, targets):\n        # \u83b7\u53d6 top-k \u9884\u6d4b\n        top_k_pred = set(pred[:k])\n        target_set = set(target)\n\n        # \u8ba1\u7b97\u53ec\u56de\u7387\n        if len(target_set) &gt; 0:\n            recall = len(top_k_pred &amp; target_set) / len(target_set)\n            recall_scores.append(recall)\n\n    return np.mean(recall_scores)\n\ndef compute_ndcg_at_k(predictions, targets, k=10):\n    \"\"\"\u8ba1\u7b97 NDCG@K\"\"\"\n    ndcg_scores = []\n\n    for pred, target in zip(predictions, targets):\n        # \u8ba1\u7b97DCG\n        dcg = 0\n        for i, item in enumerate(pred[:k]):\n            if item in target:\n                dcg += 1 / np.log2(i + 2)  # +2 \u56e0\u4e3alog2(1)=0\n\n        # \u8ba1\u7b97IDCG\n        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(target), k)))\n\n        # \u8ba1\u7b97NDCG\n        ndcg = dcg / idcg if idcg &gt; 0 else 0\n        ndcg_scores.append(ndcg)\n\n    return np.mean(ndcg_scores)\n</code></pre>"},{"location":"zh/models/tiger/#_10","title":"\u591a\u6837\u6027\u6307\u6807","text":"<pre><code>def compute_diversity(recommendations):\n    \"\"\"\u8ba1\u7b97\u63a8\u8350\u591a\u6837\u6027\"\"\"\n    all_items = set()\n    for rec_list in recommendations:\n        all_items.update(rec_list)\n\n    # \u7269\u54c1\u8986\u76d6\u5ea6\n    coverage = len(all_items) / total_items\n\n    # \u57fa\u5c3c\u7cfb\u6570\n    item_counts = defaultdict(int)\n    for rec_list in recommendations:\n        for item in rec_list:\n            item_counts[item] += 1\n\n    counts = list(item_counts.values())\n    gini = compute_gini_coefficient(counts)\n\n    return {\"coverage\": coverage, \"gini\": gini}\n</code></pre>"},{"location":"zh/models/tiger/#_11","title":"\u6a21\u578b\u4f18\u5316","text":""},{"location":"zh/models/tiger/#_12","title":"\u4f4d\u7f6e\u7f16\u7801","text":"<pre><code>class PositionalEncoding(nn.Module):\n    \"\"\"\u4f4d\u7f6e\u7f16\u7801\"\"\"\n    def __init__(self, d_model, max_seq_length=5000):\n        super().__init__()\n\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n</code></pre>"},{"location":"zh/models/tiger/#_13","title":"\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316","text":"<pre><code>class MultiHeadAttention(nn.Module):\n    \"\"\"\u4f18\u5316\u7684\u591a\u5934\u6ce8\u610f\u529b\"\"\"\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0\n\n        self.d_k = d_model // num_heads\n        self.num_heads = num_heads\n\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # Linear transformations\n        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n        # Scaled dot-product attention\n        attention, weights = scaled_dot_product_attention(Q, K, V, mask, self.dropout)\n\n        # Concatenate heads\n        attention = attention.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.num_heads * self.d_k\n        )\n\n        output = self.w_o(attention)\n        return output, weights\n</code></pre>"},{"location":"zh/models/tiger/#_14","title":"\u9ad8\u7ea7\u529f\u80fd","text":""},{"location":"zh/models/tiger/#_15","title":"\u7528\u6237\u884c\u4e3a\u5efa\u6a21","text":"<pre><code>class UserBehaviorEncoder(nn.Module):\n    \"\"\"\u7528\u6237\u884c\u4e3a\u7f16\u7801\u5668\"\"\"\n    def __init__(self, config):\n        super().__init__()\n\n        # \u65f6\u95f4\u7f16\u7801\n        self.time_encoder = nn.Linear(1, config.embedding_dim)\n\n        # \u884c\u4e3a\u7c7b\u578b\u7f16\u7801\n        self.behavior_embedding = nn.Embedding(\n            config.num_behavior_types, \n            config.embedding_dim\n        )\n\n    def forward(self, sequences, timestamps, behavior_types):\n        # \u5e8f\u5217\u7f16\u7801\n        seq_encoded = self.item_encoder(sequences)\n\n        # \u65f6\u95f4\u7f16\u7801\n        time_encoded = self.time_encoder(timestamps.unsqueeze(-1))\n\n        # \u884c\u4e3a\u7f16\u7801\n        behavior_encoded = self.behavior_embedding(behavior_types)\n\n        # \u878d\u5408\u7279\u5f81\n        combined = seq_encoded + time_encoded + behavior_encoded\n\n        return combined\n</code></pre>"},{"location":"zh/models/tiger/#_16","title":"\u51b7\u542f\u52a8\u5904\u7406","text":"<pre><code>class ColdStartHandler:\n    \"\"\"\u51b7\u542f\u52a8\u5904\u7406\u5668\"\"\"\n\n    def __init__(self, model, item_features):\n        self.model = model\n        self.item_features = item_features\n\n    def recommend_for_new_user(self, user_profile, k=10):\n        \"\"\"\u4e3a\u65b0\u7528\u6237\u63a8\u8350\"\"\"\n        # \u57fa\u4e8e\u7528\u6237\u753b\u50cf\u627e\u76f8\u4f3c\u7269\u54c1\n        similar_items = self.find_similar_items(user_profile)\n\n        # \u4f7f\u7528\u7269\u54c1\u7279\u5f81\u8fdb\u884c\u63a8\u8350\n        recommendations = self.model.recommend_by_content(similar_items, k)\n\n        return recommendations\n\n    def recommend_new_item(self, item_features, k=10):\n        \"\"\"\u63a8\u8350\u65b0\u7269\u54c1\"\"\"\n        # \u627e\u5230\u7279\u5f81\u76f8\u4f3c\u7684\u73b0\u6709\u7269\u54c1\n        similar_existing = self.find_similar_existing_items(item_features)\n\n        # \u57fa\u4e8e\u76f8\u4f3c\u7269\u54c1\u7684\u7528\u6237\u8fdb\u884c\u63a8\u8350\n        target_users = self.get_users_who_liked(similar_existing)\n\n        return target_users[:k]\n</code></pre>"},{"location":"zh/models/tiger/#_17","title":"\u5b9e\u9645\u5e94\u7528","text":""},{"location":"zh/models/tiger/#_18","title":"\u5728\u7ebf\u670d\u52a1","text":"<pre><code>class TIGERRecommendationService:\n    \"\"\"TIGER \u63a8\u8350\u670d\u52a1\"\"\"\n\n    def __init__(self, model_path, device='cuda'):\n        self.model = Tiger.load_from_checkpoint(model_path)\n        self.model.to(device)\n        self.model.eval()\n        self.device = device\n\n    def get_recommendations(self, user_id, user_history, k=10):\n        \"\"\"\u83b7\u53d6\u63a8\u8350\u7ed3\u679c\"\"\"\n        # \u9884\u5904\u7406\u7528\u6237\u5386\u53f2\n        semantic_sequence = self.preprocess_user_history(user_history)\n\n        # \u751f\u6210\u63a8\u8350\n        with torch.no_grad():\n            recommendations = self.model.generate_recommendations(\n                semantic_sequence, max_length=k\n            )\n\n        # \u540e\u5904\u7406\uff1a\u8bed\u4e49ID -&gt; \u7269\u54c1ID\n        item_recommendations = self.semantic_to_items(recommendations)\n\n        return item_recommendations\n\n    def batch_recommend(self, user_requests):\n        \"\"\"\u6279\u91cf\u63a8\u8350\"\"\"\n        batch_results = []\n\n        for user_id, user_history in user_requests:\n            recommendations = self.get_recommendations(user_id, user_history)\n            batch_results.append((user_id, recommendations))\n\n        return batch_results\n</code></pre>"},{"location":"zh/models/tiger/#ab","title":"A/B\u6d4b\u8bd5\u652f\u6301","text":"<pre><code>class ABTestingFramework:\n    \"\"\"A/B\u6d4b\u8bd5\u6846\u67b6\"\"\"\n\n    def __init__(self, model_a, model_b):\n        self.model_a = model_a  # \u63a7\u5236\u7ec4\u6a21\u578b\n        self.model_b = model_b  # \u5b9e\u9a8c\u7ec4\u6a21\u578b\n\n    def recommend_with_ab_test(self, user_id, user_history, test_group=None):\n        \"\"\"A/B\u6d4b\u8bd5\u63a8\u8350\"\"\"\n        if test_group is None:\n            # \u968f\u673a\u5206\u7ec4\n            test_group = 'A' if hash(user_id) % 2 == 0 else 'B'\n\n        if test_group == 'A':\n            return self.model_a.recommend(user_history), 'A'\n        else:\n            return self.model_b.recommend(user_history), 'B'\n\n    def collect_metrics(self, user_id, recommendations, group, feedback):\n        \"\"\"\u6536\u96c6A/B\u6d4b\u8bd5\u6307\u6807\"\"\"\n        # \u8bb0\u5f55\u7528\u6237\u53cd\u9988\u548c\u63a8\u8350\u7ed3\u679c\n        metrics_data = {\n            'user_id': user_id,\n            'group': group,\n            'recommendations': recommendations,\n            'feedback': feedback,\n            'timestamp': time.time()\n        }\n\n        # \u5b58\u50a8\u5230\u6570\u636e\u5e93\u6216\u65e5\u5fd7\u7cfb\u7edf\n        self.save_metrics(metrics_data)\n</code></pre> <p>TIGER \u6a21\u578b\u901a\u8fc7\u751f\u6210\u5f0f\u7684\u65b9\u6cd5\u89e3\u51b3\u63a8\u8350\u95ee\u9898\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548c\u7075\u6d3b\u7684\u751f\u6210\u673a\u5236\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u590d\u6742\u7684\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u548c\u591a\u6837\u5316\u7684\u63a8\u8350\u573a\u666f\u3002</p>"},{"location":"zh/training/rqvae/","title":"RQVAE \u8bad\u7ec3","text":"<p>\u672c\u6307\u5357\u8be6\u7ec6\u4ecb\u7ecd\u5982\u4f55\u8bad\u7ec3 RQVAE \u6a21\u578b\u3002</p>"},{"location":"zh/training/rqvae/#_1","title":"\u8bad\u7ec3\u51c6\u5907","text":""},{"location":"zh/training/rqvae/#1","title":"1. \u6570\u636e\u51c6\u5907","text":"<p>\u786e\u4fdd\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d\u5e76\u653e\u7f6e\u5728\u6b63\u786e\u4f4d\u7f6e\uff1a</p> <pre><code># \u6570\u636e\u5c06\u81ea\u52a8\u4e0b\u8f7d\u5230\u6307\u5b9a\u76ee\u5f55\nmkdir -p dataset/amazon\n</code></pre>"},{"location":"zh/training/rqvae/#2","title":"2. \u68c0\u67e5\u914d\u7f6e\u6587\u4ef6","text":"<p>\u67e5\u770b\u9ed8\u8ba4\u914d\u7f6e\uff1a</p> <pre><code>cat config/rqvae/p5_amazon.gin\n</code></pre> <p>\u4e3b\u8981\u914d\u7f6e\u53c2\u6570\uff1a</p> <pre><code># \u8bad\u7ec3\u53c2\u6570\ntrain.iterations=400000          # \u8bad\u7ec3\u8fed\u4ee3\u6570\ntrain.learning_rate=0.0005      # \u5b66\u4e60\u7387\ntrain.batch_size=64             # \u6279\u91cf\u5927\u5c0f\ntrain.weight_decay=0.01         # \u6743\u91cd\u8870\u51cf\n\n# \u6a21\u578b\u53c2\u6570\ntrain.vae_input_dim=768         # \u8f93\u5165\u7ef4\u5ea6\ntrain.vae_embed_dim=32          # \u5d4c\u5165\u7ef4\u5ea6\ntrain.vae_hidden_dims=[512, 256, 128]  # \u9690\u85cf\u5c42\u7ef4\u5ea6\ntrain.vae_codebook_size=256     # \u7801\u672c\u5927\u5c0f\ntrain.vae_n_layers=3            # \u91cf\u5316\u5c42\u6570\n\n# \u91cf\u5316\u8bbe\u7f6e\ntrain.vae_codebook_mode=%generative_recommenders.models.rqvae.QuantizeForwardMode.ROTATION_TRICK\ntrain.commitment_weight=0.25    # \u627f\u8bfa\u635f\u5931\u6743\u91cd\n</code></pre>"},{"location":"zh/training/rqvae/#_2","title":"\u5f00\u59cb\u8bad\u7ec3","text":""},{"location":"zh/training/rqvae/#_3","title":"\u57fa\u672c\u8bad\u7ec3\u547d\u4ee4","text":"<pre><code>python generative_recommenders/trainers/rqvae_trainer.py config/rqvae/p5_amazon.gin\n</code></pre>"},{"location":"zh/training/rqvae/#_4","title":"\u8bad\u7ec3\u8fc7\u7a0b\u76d1\u63a7","text":"<p>\u5982\u679c\u542f\u7528\u4e86 Weights &amp; Biases\uff1a</p> <pre><code>train.wandb_logging=True\ntrain.wandb_project=\"my_rqvae_project\"\n</code></pre>"},{"location":"zh/training/rqvae/#gpu","title":"GPU \u8bad\u7ec3","text":"<p>\u4f7f\u7528\u591aGPU\u8bad\u7ec3\uff1a</p> <pre><code>accelerate config  # \u9996\u6b21\u8fd0\u884c\u65f6\u914d\u7f6e\naccelerate launch generative_recommenders/trainers/rqvae_trainer.py config/rqvae/p5_amazon.gin\n</code></pre>"},{"location":"zh/training/rqvae/#_5","title":"\u81ea\u5b9a\u4e49\u914d\u7f6e","text":""},{"location":"zh/training/rqvae/#_6","title":"\u521b\u5efa\u81ea\u5b9a\u4e49\u914d\u7f6e\u6587\u4ef6","text":"<pre><code># my_rqvae_config.gin\nimport generative_recommenders.data.p5_amazon\nimport generative_recommenders.models.rqvae\n\n# \u81ea\u5b9a\u4e49\u8bad\u7ec3\u53c2\u6570\ntrain.iterations=200000\ntrain.batch_size=32\ntrain.learning_rate=0.001\n\n# \u81ea\u5b9a\u4e49\u6a21\u578b\u67b6\u6784\ntrain.vae_embed_dim=64\ntrain.vae_hidden_dims=[512, 256, 128, 64]\ntrain.vae_codebook_size=512\n\n# \u6570\u636e\u8def\u5f84\ntrain.dataset_folder=\"path/to/my/dataset\"\ntrain.save_dir_root=\"path/to/my/output\"\n\n# \u5b9e\u9a8c\u8ddf\u8e2a\ntrain.wandb_logging=True\ntrain.wandb_project=\"custom_rqvae_experiment\"\n</code></pre> <p>\u4f7f\u7528\u81ea\u5b9a\u4e49\u914d\u7f6e\uff1a</p> <pre><code>python generative_recommenders/trainers/rqvae_trainer.py my_rqvae_config.gin\n</code></pre>"},{"location":"zh/training/rqvae/#_7","title":"\u8bad\u7ec3\u76d1\u63a7","text":""},{"location":"zh/training/rqvae/#_8","title":"\u5173\u952e\u6307\u6807","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5173\u6ce8\u4ee5\u4e0b\u6307\u6807\uff1a</p> <ul> <li>\u603b\u635f\u5931 (Total Loss): \u6574\u4f53\u8bad\u7ec3\u635f\u5931</li> <li>\u91cd\u6784\u635f\u5931 (Reconstruction Loss): \u91cd\u6784\u8d28\u91cf</li> <li>\u91cf\u5316\u635f\u5931 (Quantization Loss): \u91cf\u5316\u6548\u679c</li> <li>\u627f\u8bfa\u635f\u5931 (Commitment Loss): \u7f16\u7801\u5668\u627f\u8bfa\u5ea6</li> </ul>"},{"location":"zh/training/rqvae/#_9","title":"\u65e5\u5fd7\u8f93\u51fa\u793a\u4f8b","text":"<pre><code>Epoch 1000: Loss=2.3456, Recon=2.1234, Quant=0.1234, Commit=0.0988\nEpoch 2000: Loss=1.9876, Recon=1.8234, Quant=0.0987, Commit=0.0655\n...\n</code></pre>"},{"location":"zh/training/rqvae/#_10","title":"\u6a21\u578b\u8bc4\u4f30","text":""},{"location":"zh/training/rqvae/#_11","title":"\u91cd\u6784\u8d28\u91cf\u8bc4\u4f30","text":"<pre><code>from generative_recommenders.models.rqvae import RqVae\nfrom generative_recommenders.data.p5_amazon import P5AmazonItemDataset\n\n# \u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\nmodel = RqVae.load_from_checkpoint(\"out/rqvae/checkpoint_299999.pt\")\n\n# \u8bc4\u4f30\u6570\u636e\u96c6\neval_dataset = P5AmazonItemDataset(\n    root=\"dataset/amazon\",\n    train_test_split=\"eval\"\n)\n\n# \u8ba1\u7b97\u91cd\u6784\u635f\u5931\nmodel.eval()\nwith torch.no_grad():\n    eval_loss = model.evaluate(eval_dataset)\n    print(f\"Evaluation loss: {eval_loss:.4f}\")\n</code></pre>"},{"location":"zh/training/rqvae/#_12","title":"\u7801\u672c\u5229\u7528\u7387\u5206\u6790","text":"<pre><code>def analyze_codebook_usage(model, dataloader):\n    used_codes = set()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            outputs = model(batch)\n            semantic_ids = outputs.sem_ids\n            used_codes.update(semantic_ids.flatten().tolist())\n\n    usage_rate = len(used_codes) / model.codebook_size\n    print(f\"Codebook usage: {usage_rate:.2%}\")\n    print(f\"Used codes: {len(used_codes)}/{model.codebook_size}\")\n\n    return used_codes\n</code></pre>"},{"location":"zh/training/rqvae/#_13","title":"\u6545\u969c\u6392\u9664","text":""},{"location":"zh/training/rqvae/#_14","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>Q: \u8bad\u7ec3\u635f\u5931\u4e0d\u6536\u655b\uff1f</p> <p>A: \u5c1d\u8bd5\u4ee5\u4e0b\u89e3\u51b3\u65b9\u6848\uff1a - \u964d\u4f4e\u5b66\u4e60\u7387\uff1a<code>train.learning_rate=0.0001</code> - \u8c03\u6574\u627f\u8bfa\u6743\u91cd\uff1a<code>train.commitment_weight=0.1</code> - \u68c0\u67e5\u6570\u636e\u9884\u5904\u7406\u662f\u5426\u6b63\u786e</p> <p>Q: \u7801\u672c\u5d29\u584c\uff08\u6240\u6709\u6837\u672c\u4f7f\u7528\u540c\u4e00\u4e2a\u7801\uff09\uff1f</p> <p>A:  - \u4f7f\u7528 ROTATION_TRICK \u6a21\u5f0f - \u589e\u52a0\u627f\u8bfa\u6743\u91cd - \u51cf\u5c0f\u5b66\u4e60\u7387</p> <p>Q: GPU \u5185\u5b58\u4e0d\u8db3\uff1f</p> <p>A: - \u51cf\u5c0f\u6279\u91cf\u5927\u5c0f\uff1a<code>train.batch_size=32</code> - \u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\uff1a<code>train.vae_hidden_dims=[256, 128]</code> - \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3</p>"},{"location":"zh/training/rqvae/#_15","title":"\u8c03\u8bd5\u6280\u5de7","text":"<ol> <li> <p>\u68af\u5ea6\u68c0\u67e5\uff1a <pre><code>for name, param in model.named_parameters():\n    if param.grad is not None:\n        grad_norm = param.grad.norm().item()\n        print(f\"{name}: {grad_norm:.6f}\")\n</code></pre></p> </li> <li> <p>\u635f\u5931\u5206\u6790\uff1a <pre><code># \u5206\u522b\u6253\u5370\u5404\u4e2a\u635f\u5931\u7ec4\u4ef6\nprint(f\"Reconstruction: {outputs.reconstruction_loss:.4f}\")\nprint(f\"Quantization: {outputs.quantization_loss:.4f}\")\nprint(f\"Commitment: {outputs.commitment_loss:.4f}\")\n</code></pre></p> </li> </ol>"},{"location":"zh/training/rqvae/#_16","title":"\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"zh/training/rqvae/#_17","title":"\u8d85\u53c2\u6570\u8c03\u4f18\u5efa\u8bae","text":"<ol> <li> <p>\u5b66\u4e60\u7387\u8c03\u5ea6\uff1a <pre><code># \u4f7f\u7528\u4f59\u5f26\u9000\u706b\ntrain.scheduler=\"cosine\"\ntrain.min_lr=1e-6\n</code></pre></p> </li> <li> <p>\u65e9\u505c\u7b56\u7565\uff1a <pre><code>train.early_stopping=True\ntrain.patience=10000\n</code></pre></p> </li> <li> <p>\u6a21\u578b\u4fdd\u5b58\u9891\u7387\uff1a <pre><code>train.save_model_every=50000  # \u6bcf5\u4e07\u6b21\u8fed\u4ee3\u4fdd\u5b58\u4e00\u6b21\ntrain.eval_every=10000        # \u6bcf1\u4e07\u6b21\u8fed\u4ee3\u8bc4\u4f30\u4e00\u6b21\n</code></pre></p> </li> </ol>"},{"location":"zh/training/rqvae/#_18","title":"\u5b9e\u9a8c\u7ba1\u7406","text":"<p>\u5efa\u8bae\u4f7f\u7528\u7248\u672c\u63a7\u5236\u548c\u5b9e\u9a8c\u8ddf\u8e2a\uff1a</p> <pre><code># \u521b\u5efa\u5b9e\u9a8c\u5206\u652f\ngit checkout -b experiment/rqvae-large-codebook\n\n# \u4fee\u6539\u914d\u7f6e\nvim config/rqvae/large_codebook.gin\n\n# \u8fd0\u884c\u5b9e\u9a8c\npython generative_recommenders/trainers/rqvae_trainer.py config/rqvae/large_codebook.gin\n\n# \u8bb0\u5f55\u7ed3\u679c\ngit add .\ngit commit -m \"Experiment: large codebook (size=1024)\"\n</code></pre>"},{"location":"zh/training/rqvae/#_19","title":"\u4e0b\u4e00\u6b65","text":"<p>\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u4f60\u53ef\u4ee5\uff1a</p> <ol> <li>\u4f7f\u7528\u8bad\u7ec3\u597d\u7684 RQVAE \u8fdb\u884c TIGER \u8bad\u7ec3</li> <li>\u5206\u6790\u6a21\u578b\u6027\u80fd</li> <li>\u5c1d\u8bd5\u4e0d\u540c\u7684\u6570\u636e\u96c6</li> </ol>"},{"location":"zh/training/tiger/","title":"TIGER \u8bad\u7ec3","text":"<p>\u672c\u6307\u5357\u8be6\u7ec6\u4ecb\u7ecd\u5982\u4f55\u8bad\u7ec3 TIGER \u6a21\u578b\u3002</p>"},{"location":"zh/training/tiger/#_1","title":"\u524d\u7f6e\u6761\u4ef6","text":""},{"location":"zh/training/tiger/#1-rqvae","title":"1. \u9884\u8bad\u7ec3\u7684 RQVAE \u6a21\u578b","text":"<p>TIGER \u9700\u8981\u9884\u8bad\u7ec3\u7684 RQVAE \u6a21\u578b\u6765\u751f\u6210\u8bed\u4e49 ID\uff1a</p> <pre><code># \u786e\u4fdd RQVAE \u6a21\u578b\u5df2\u8bad\u7ec3\u5b8c\u6210\nls out/rqvae/p5_amazon/beauty/checkpoint_*.pt\n</code></pre> <p>\u5982\u679c\u6ca1\u6709\uff0c\u8bf7\u5148\u5b8c\u6210 RQVAE \u8bad\u7ec3\u3002</p>"},{"location":"zh/training/tiger/#2","title":"2. \u6570\u636e\u51c6\u5907","text":"<p>\u786e\u4fdd\u4f7f\u7528\u4e0e RQVAE \u76f8\u540c\u7684\u6570\u636e\u96c6\uff1a</p> <pre><code># \u6570\u636e\u5e94\u8be5\u5df2\u7ecf\u5b58\u5728\nls dataset/amazon/\n</code></pre>"},{"location":"zh/training/tiger/#_2","title":"\u8bad\u7ec3\u914d\u7f6e","text":""},{"location":"zh/training/tiger/#_3","title":"\u9ed8\u8ba4\u914d\u7f6e","text":"<p>\u67e5\u770b TIGER \u914d\u7f6e\u6587\u4ef6\uff1a</p> <pre><code>cat config/tiger/p5_amazon.gin\n</code></pre> <p>\u5173\u952e\u53c2\u6570\uff1a</p> <pre><code># \u8bad\u7ec3\u53c2\u6570\ntrain.epochs=5000               # \u8bad\u7ec3\u8f6e\u6570\ntrain.learning_rate=3e-4        # \u5b66\u4e60\u7387\ntrain.batch_size=256            # \u6279\u91cf\u5927\u5c0f\ntrain.weight_decay=0.035        # \u6743\u91cd\u8870\u51cf\n\n# \u6a21\u578b\u53c2\u6570\ntrain.embedding_dim=128         # \u5d4c\u5165\u7ef4\u5ea6\ntrain.attn_dim=512             # \u6ce8\u610f\u529b\u7ef4\u5ea6\ntrain.dropout=0.3              # Dropout\u7387\ntrain.num_heads=8              # \u6ce8\u610f\u529b\u5934\u6570\ntrain.n_layers=8               # Transformer\u5c42\u6570\n\n# \u5e8f\u5217\u53c2\u6570\ntrain.max_seq_len=512          # \u6700\u5927\u5e8f\u5217\u957f\u5ea6\ntrain.num_item_embeddings=256  # \u7269\u54c1\u5d4c\u5165\u6570\u91cf\ntrain.num_user_embeddings=2000 # \u7528\u6237\u5d4c\u5165\u6570\u91cf\ntrain.sem_id_dim=3             # \u8bed\u4e49ID\u7ef4\u5ea6\n\n# \u9884\u8bad\u7ec3\u6a21\u578b\u8def\u5f84\ntrain.pretrained_rqvae_path=\"./out/rqvae/p5_amazon/beauty/checkpoint_299999.pt\"\n</code></pre>"},{"location":"zh/training/tiger/#_4","title":"\u5f00\u59cb\u8bad\u7ec3","text":""},{"location":"zh/training/tiger/#_5","title":"\u57fa\u672c\u8bad\u7ec3\u547d\u4ee4","text":"<pre><code>python generative_recommenders/trainers/tiger_trainer.py config/tiger/p5_amazon.gin\n</code></pre>"},{"location":"zh/training/tiger/#_6","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u591aGPU\u8bad\u7ec3\uff1a</p> <pre><code>accelerate config\naccelerate launch generative_recommenders/trainers/tiger_trainer.py config/tiger/p5_amazon.gin\n</code></pre>"},{"location":"zh/training/tiger/#_7","title":"\u8bad\u7ec3\u8fc7\u7a0b","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u770b\u5230\uff1a</p> <ol> <li>\u6570\u636e\u52a0\u8f7d: \u5e8f\u5217\u6570\u636e\u96c6\u52a0\u8f7d\u548c\u8bed\u4e49ID\u751f\u6210</li> <li>\u6a21\u578b\u521d\u59cb\u5316: Transformer\u6a21\u578b\u521d\u59cb\u5316</li> <li>\u8bad\u7ec3\u5faa\u73af: \u635f\u5931\u4e0b\u964d\u548c\u6307\u6807\u76d1\u63a7</li> <li>\u9a8c\u8bc1\u8bc4\u4f30: \u5468\u671f\u6027\u6027\u80fd\u8bc4\u4f30</li> </ol>"},{"location":"zh/training/tiger/#_8","title":"\u81ea\u5b9a\u4e49\u914d\u7f6e","text":""},{"location":"zh/training/tiger/#_9","title":"\u521b\u5efa\u81ea\u5b9a\u4e49\u914d\u7f6e","text":"<pre><code># my_tiger_config.gin\nimport generative_recommenders.data.p5_amazon\n\n# \u8c03\u6574\u6a21\u578b\u89c4\u6a21\ntrain.embedding_dim=256\ntrain.attn_dim=1024\ntrain.n_layers=12\ntrain.num_heads=16\n\n# \u8c03\u6574\u8bad\u7ec3\u53c2\u6570\ntrain.learning_rate=1e-4\ntrain.batch_size=128\ntrain.epochs=10000\n\n# \u81ea\u5b9a\u4e49\u8def\u5f84\ntrain.dataset_folder=\"my_dataset\"\ntrain.pretrained_rqvae_path=\"my_rqvae/checkpoint.pt\"\ntrain.save_dir_root=\"my_tiger_output/\"\n\n# \u5b9e\u9a8c\u8ddf\u8e2a\ntrain.wandb_logging=True\ntrain.wandb_project=\"my_tiger_experiment\"\n</code></pre>"},{"location":"zh/training/tiger/#_10","title":"\u6a21\u578b\u67b6\u6784\u89e3\u6790","text":""},{"location":"zh/training/tiger/#transformer","title":"Transformer \u7ed3\u6784","text":"<p>TIGER \u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff1a</p> <pre><code>class Tiger(nn.Module):\n    def __init__(self, config):\n        # \u7528\u6237\u548c\u7269\u54c1\u5d4c\u5165\n        self.user_embedding = UserIdEmbedding(...)\n        self.item_embedding = SemIdEmbedding(...)\n\n        # Transformer \u7f16\u7801\u5668-\u89e3\u7801\u5668\n        self.transformer = TransformerEncoderDecoder(...)\n\n        # \u8f93\u51fa\u6295\u5f71\n        self.output_projection = nn.Linear(...)\n</code></pre>"},{"location":"zh/training/tiger/#id","title":"\u8bed\u4e49ID\u6620\u5c04","text":"<p>TIGER \u5c06\u7269\u54c1\u8f6c\u6362\u4e3a\u8bed\u4e49ID\u5e8f\u5217\uff1a</p> <pre><code># \u7269\u54c1 -&gt; \u8bed\u4e49ID\u5e8f\u5217\nitem_id = 123\nsemantic_ids = rqvae.get_semantic_ids(item_features[item_id])\n# semantic_ids: [45, 67, 89]  # \u957f\u5ea6\u4e3a sem_id_dim\n</code></pre>"},{"location":"zh/training/tiger/#_11","title":"\u8bad\u7ec3\u76d1\u63a7","text":""},{"location":"zh/training/tiger/#_12","title":"\u5173\u952e\u6307\u6807","text":"<ul> <li>\u8bad\u7ec3\u635f\u5931: \u5e8f\u5217\u5efa\u6a21\u635f\u5931</li> <li>\u9a8c\u8bc1\u635f\u5931: \u9a8c\u8bc1\u96c6\u6027\u80fd</li> <li>Recall@K: Top-K \u53ec\u56de\u7387</li> <li>NDCG@K: \u5f52\u4e00\u5316\u6298\u6263\u7d2f\u79ef\u589e\u76ca</li> </ul>"},{"location":"zh/training/tiger/#weights-biases","title":"Weights &amp; Biases \u96c6\u6210","text":"<p>\u542f\u7528\u5b9e\u9a8c\u8ddf\u8e2a\uff1a</p> <pre><code>train.wandb_logging=True\ntrain.wandb_project=\"tiger_p5_amazon\"\ntrain.wandb_log_interval=100\n</code></pre> <p>\u67e5\u770b\u8bad\u7ec3\u66f2\u7ebf\uff1a - \u8bbf\u95ee wandb.ai - \u627e\u5230\u4f60\u7684\u9879\u76ee\u548c\u5b9e\u9a8c</p>"},{"location":"zh/training/tiger/#_13","title":"\u6a21\u578b\u8bc4\u4f30","text":""},{"location":"zh/training/tiger/#_14","title":"\u63a8\u8350\u8d28\u91cf\u8bc4\u4f30","text":"<pre><code>from generative_recommenders.models.tiger import Tiger\nfrom generative_recommenders.modules.metrics import TopKAccumulator\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = Tiger.load_from_checkpoint(\"out/tiger/checkpoint.pt\")\n\n# \u521b\u5efa\u8bc4\u4f30\u5668\nevaluator = TopKAccumulator(k=[5, 10, 20])\n\n# \u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\ntest_dataloader = DataLoader(test_dataset, batch_size=256)\nmetrics = evaluator.evaluate(model, test_dataloader)\n\nprint(f\"Recall@10: {metrics['recall@10']:.4f}\")\nprint(f\"NDCG@10: {metrics['ndcg@10']:.4f}\")\n</code></pre>"},{"location":"zh/training/tiger/#_15","title":"\u751f\u6210\u5f0f\u63a8\u8350","text":"<pre><code>def generate_recommendations(model, user_sequence, top_k=10):\n    \"\"\"\u4e3a\u7528\u6237\u751f\u6210\u63a8\u8350\"\"\"\n    model.eval()\n\n    with torch.no_grad():\n        # \u7f16\u7801\u7528\u6237\u5e8f\u5217\n        sequence_embedding = model.encode_sequence(user_sequence)\n\n        # \u751f\u6210\u63a8\u8350\n        logits = model.generate(sequence_embedding, max_length=top_k)\n\n        # \u83b7\u53d6Top-K\u7269\u54c1\n        recommendations = torch.topk(logits, top_k).indices\n\n    return recommendations.tolist()\n\n# \u4f7f\u7528\u793a\u4f8b\nuser_history = [item1_semantic_ids, item2_semantic_ids, ...]\nrecommendations = generate_recommendations(model, user_history, top_k=10)\n</code></pre>"},{"location":"zh/training/tiger/#_16","title":"\u9ad8\u7ea7\u529f\u80fd","text":""},{"location":"zh/training/tiger/#trie","title":"Trie\u7ea6\u675f\u751f\u6210","text":"<p>TIGER \u652f\u6301\u57fa\u4e8eTrie\u7684\u7ea6\u675f\u751f\u6210\uff1a</p> <pre><code>from generative_recommenders.models.tiger import build_trie\n\n# \u6784\u5efa\u6709\u6548\u7269\u54c1ID\u7684Trie\nvalid_items = torch.tensor([[1, 2, 3], [4, 5, 6], ...])  # \u8bed\u4e49ID\u5e8f\u5217\ntrie = build_trie(valid_items)\n\n# \u7ea6\u675f\u751f\u6210\nconstrained_output = model.generate_with_trie(\n    user_sequence, \n    trie=trie,\n    max_length=10\n)\n</code></pre>"},{"location":"zh/training/tiger/#_17","title":"\u5e8f\u5217\u589e\u5f3a","text":"<p>\u8bad\u7ec3\u65f6\u652f\u6301\u5e8f\u5217\u589e\u5f3a\uff1a</p> <pre><code>train.subsample=True  # \u52a8\u6001\u5b50\u91c7\u6837\ntrain.augmentation=True  # \u5e8f\u5217\u589e\u5f3a\n</code></pre>"},{"location":"zh/training/tiger/#_18","title":"\u6545\u969c\u6392\u9664","text":""},{"location":"zh/training/tiger/#_19","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>Q: RQVAE\u68c0\u67e5\u70b9\u627e\u4e0d\u5230\uff1f</p> <p>A: \u68c0\u67e5\u8def\u5f84\u662f\u5426\u6b63\u786e\uff1a <pre><code># \u786e\u8ba4\u6587\u4ef6\u5b58\u5728\nls -la out/rqvae/p5_amazon/beauty/checkpoint_299999.pt\n\n# \u66f4\u65b0\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u8def\u5f84\ntrain.pretrained_rqvae_path=\"\u5b9e\u9645\u7684\u68c0\u67e5\u70b9\u8def\u5f84\"\n</code></pre></p> <p>Q: \u8bad\u7ec3\u901f\u5ea6\u6162\uff1f</p> <p>A: \u4f18\u5316\u5efa\u8bae\uff1a - \u589e\u52a0\u6279\u91cf\u5927\u5c0f\uff1a<code>train.batch_size=512</code> - \u51cf\u5c11\u5e8f\u5217\u957f\u5ea6\uff1a<code>train.max_seq_len=256</code> - \u4f7f\u7528\u591aGPU\u8bad\u7ec3</p> <p>Q: \u63a8\u8350\u6548\u679c\u5dee\uff1f</p> <p>A: \u8c03\u4f18\u5efa\u8bae\uff1a - \u589e\u52a0\u6a21\u578b\u89c4\u6a21\uff1a<code>train.n_layers=12</code> - \u8c03\u6574\u5b66\u4e60\u7387\uff1a<code>train.learning_rate=1e-4</code> - \u589e\u52a0\u8bad\u7ec3\u8f6e\u6570\uff1a<code>train.epochs=10000</code></p>"},{"location":"zh/training/tiger/#_20","title":"\u8c03\u8bd5\u6280\u5de7","text":"<ol> <li> <p>\u68c0\u67e5\u8bed\u4e49ID\u751f\u6210\uff1a <pre><code># \u9a8c\u8bc1RQVAE\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\nrqvae = RqVae.load_from_checkpoint(pretrained_path)\nsample_item = dataset[0]\nsemantic_ids = rqvae.get_semantic_ids(sample_item)\nprint(f\"Semantic IDs: {semantic_ids}\")\n</code></pre></p> </li> <li> <p>\u76d1\u63a7\u6ce8\u610f\u529b\u6743\u91cd\uff1a <pre><code># \u68c0\u67e5\u6a21\u578b\u662f\u5426\u5b66\u5230\u6709\u610f\u4e49\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\nattention_weights = model.get_attention_weights(user_sequence)\nprint(f\"Attention shape: {attention_weights.shape}\")\n</code></pre></p> </li> </ol>"},{"location":"zh/training/tiger/#_21","title":"\u6027\u80fd\u4f18\u5316","text":""},{"location":"zh/training/tiger/#_22","title":"\u5185\u5b58\u4f18\u5316","text":"<pre><code># \u51cf\u5c11\u5185\u5b58\u4f7f\u7528\ntrain.gradient_accumulate_every=4  # \u68af\u5ea6\u7d2f\u79ef\ntrain.batch_size=64               # \u8f83\u5c0f\u6279\u91cf\ntrain.max_seq_len=256            # \u8f83\u77ed\u5e8f\u5217\n</code></pre>"},{"location":"zh/training/tiger/#_23","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"<pre><code>train.mixed_precision_type=\"fp16\"  # \u4f7f\u7528\u534a\u7cbe\u5ea6\n</code></pre>"},{"location":"zh/training/tiger/#_24","title":"\u5b9e\u9a8c\u5efa\u8bae","text":""},{"location":"zh/training/tiger/#_25","title":"\u8d85\u53c2\u6570\u7f51\u683c\u641c\u7d22","text":"<pre><code># \u5efa\u8bae\u7684\u8d85\u53c2\u6570\u8303\u56f4\nlearning_rates = [1e-4, 3e-4, 1e-3]\nbatch_sizes = [128, 256, 512]\nmodel_dims = [128, 256, 512]\nn_layers = [6, 8, 12]\n\nfor lr in learning_rates:\n    for bs in batch_sizes:\n        # \u521b\u5efa\u914d\u7f6e\u5e76\u8bad\u7ec3\n        config = create_config(lr=lr, batch_size=bs)\n        train_model(config)\n</code></pre>"},{"location":"zh/training/tiger/#ab","title":"A/B\u6d4b\u8bd5","text":"<p>\u6bd4\u8f83\u4e0d\u540c\u67b6\u6784\uff1a</p> <pre><code># \u5b9e\u9a8cA: \u6807\u51c6TIGER\ntrain.n_layers=8\ntrain.num_heads=8\n\n# \u5b9e\u9a8cB: \u66f4\u6df1\u7684\u6a21\u578b\ntrain.n_layers=12\ntrain.num_heads=16\n\n# \u5b9e\u9a8cC: \u66f4\u5bbd\u7684\u6a21\u578b\ntrain.embedding_dim=256\ntrain.attn_dim=1024\n</code></pre>"},{"location":"zh/training/tiger/#_26","title":"\u4e0b\u4e00\u6b65","text":"<p>\u8bad\u7ec3\u5b8c\u6210\u540e\uff1a</p> <ol> <li>\u8bc4\u4f30\u63a8\u8350\u6548\u679c</li> <li>\u90e8\u7f72\u5230\u751f\u4ea7\u73af\u5883</li> <li>\u5c1d\u8bd5\u5176\u4ed6\u6570\u636e\u96c6</li> </ol>"}]}